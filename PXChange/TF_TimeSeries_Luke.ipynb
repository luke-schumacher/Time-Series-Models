{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "tensorflow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Output library versions\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"tensorflow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Constants and Encoding Legend\n",
    "# ----------------------------\n",
    "START_TOKEN = 13\n",
    "END_TOKEN = 14\n",
    "\n",
    "ENCODING_LEGEND = {\n",
    "    'MRI_CCS_11': 1, 'MRI_EXU_95': 2, 'MRI_FRR_18': 3, 'MRI_FRR_257': 4,\n",
    "    'MRI_FRR_264': 5, 'MRI_FRR_3': 6, 'MRI_FRR_34': 7, 'MRI_MPT_1005': 8,\n",
    "    'MRI_MSR_100': 9, 'MRI_MSR_104': 10, 'MRI_MSR_21': 11, 'MRI_MSR_34': 12,\n",
    "    'START': START_TOKEN,\n",
    "    'END': END_TOKEN\n",
    "}\n",
    "# Build reverse mapping for decoding:\n",
    "reverse_encoding = {v: k for k, v in ENCODING_LEGEND.items()}\n",
    "\n",
    "CHAR_TO_INT = {\n",
    "    '0': 0,\n",
    "    '1': 1,\n",
    "    '2': 2,\n",
    "    '3': 3,\n",
    "    '4': 4,\n",
    "    '5': 5,\n",
    "    '6': 6,\n",
    "    '7': 7,\n",
    "    '8': 8,\n",
    "    '9': 9,\n",
    "    '10': 10,\n",
    "    '11': 11,\n",
    "    '12': 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV with columns: ['SeqOrder', 'sourceID', 'timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to']\n",
      "Found 186 sequences.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Data Loading and Sequence Splitting\n",
    "# ----------------------------\n",
    "data_file = \"encoded_182625.csv\"\n",
    "data = pd.read_csv(data_file)\n",
    "print(\"Loaded CSV with columns:\", data.columns.tolist())\n",
    "\n",
    "# Split sequences based on \"SeqOrder\" (reset to 0 indicates new sequence)\n",
    "sequences_tokens = []\n",
    "sequences_times = []\n",
    "\n",
    "current_tokens = []\n",
    "current_times = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    seq_order = row['SeqOrder']\n",
    "    s_id = float(row['sourceID'])\n",
    "    t_diff = float(row['timediff'])\n",
    "    if seq_order == 0 and current_tokens:\n",
    "        token_seq = [START_TOKEN] + [int(x) for x in current_tokens] + [END_TOKEN]\n",
    "        time_seq = [0.0] + current_times\n",
    "        if len(time_seq) < len(token_seq):\n",
    "            time_seq = time_seq + [time_seq[-1]]\n",
    "        sequences_tokens.append(token_seq)\n",
    "        sequences_times.append(time_seq)\n",
    "        current_tokens = []\n",
    "        current_times = []\n",
    "    current_tokens.append(s_id)\n",
    "    current_times.append(t_diff)\n",
    "if current_tokens:\n",
    "    token_seq = [START_TOKEN] + [int(x) for x in current_tokens] + [END_TOKEN]\n",
    "    time_seq = [0.0] + current_times\n",
    "    if len(time_seq) < len(token_seq):\n",
    "        time_seq = time_seq + [time_seq[-1]]\n",
    "    sequences_tokens.append(token_seq)\n",
    "    sequences_times.append(time_seq)\n",
    "\n",
    "print(f\"Found {len(sequences_tokens)} sequences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1, 8370)\n",
      "Y_cum_target shape: (1, 8370)\n",
      "Mask shape: (1, 8370)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Prepare Training Data and Masks\n",
    "# ----------------------------\n",
    "# For each sequence, use input tokens = tokens[:-1] and target times = times[1:]\n",
    "X_list, Y_list, masks_list = [], [], []\n",
    "for tokens, times in zip(sequences_tokens, sequences_times):\n",
    "    x_seq = tokens[:-1]    # input\n",
    "    y_seq = times[1:]      # target cumulative times\n",
    "    # Mask: valid tokens are those not equal to the pad value (we use END_TOKEN for padding)\n",
    "    mask_seq = [1 if t != END_TOKEN else 0 for t in x_seq]\n",
    "    X_list.append(x_seq)\n",
    "    Y_list.append(y_seq)\n",
    "    masks_list.append(mask_seq)\n",
    "\n",
    "max_len = max(len(x) for x in X_list)\n",
    "X_train = pad_sequences(X_list, maxlen=max_len, padding='post', value=END_TOKEN)\n",
    "Y_cum_target = pad_sequences(Y_list, maxlen=max_len, padding='post', value=0.0)\n",
    "mask_train = pad_sequences(masks_list, maxlen=max_len, padding='post', value=0)\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.int32)\n",
    "Y_cum_target = np.array(Y_cum_target, dtype=np.float32)\n",
    "mask_train = np.array(mask_train, dtype=np.float32)\n",
    "\n",
    "X_train = X_train.reshape(1, -1)\n",
    "Y_cum_target = Y_cum_target.reshape(1, -1)\n",
    "mask_train = mask_train.reshape(1, -1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_cum_target shape:\", Y_cum_target.shape)\n",
    "print(\"Mask shape:\", mask_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Target Computation Function\n",
    "# ----------------------------\n",
    "def compute_true_targets(cumulative_times):\n",
    "    \"\"\"\n",
    "    Given cumulative times (shape: [batch, L_target]), compute:\n",
    "      - true_proportions: increments normalized by total time.\n",
    "      - true_total: total time per sequence.\n",
    "    \"\"\"\n",
    "    diffs = cumulative_times[:, 0:1]\n",
    "    diffs = tf.concat([diffs, cumulative_times[:, 1:] - cumulative_times[:, :-1]], axis=1)\n",
    "    true_total = cumulative_times[:, -1:]\n",
    "    true_total_safe = tf.where(true_total == 0, tf.ones_like(true_total), true_total)\n",
    "    true_proportions = diffs / true_total_safe\n",
    "    return true_proportions, true_total\n",
    "\n",
    "true_prop, true_total = compute_true_targets(tf.convert_to_tensor(Y_cum_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Transformer Components (unchanged)\n",
    "# ----------------------------\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_len=16384, use_embedding=True):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_embedding = use_embedding\n",
    "        if self.use_embedding:\n",
    "            self.embedding = layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        else:\n",
    "            self.embedding = layers.Dense(d_model, activation=\"relu\")\n",
    "        self.max_len = max_len\n",
    "        self.pos_encoding = positional_encoding(self.max_len, d_model)\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        if self.use_embedding:\n",
    "            return self.embedding.compute_mask(*args, **kwargs)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x += self.pos_encoding[tf.newaxis, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model),\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, key=x, value=x, use_causal_mask=True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttentionFeedForwardLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = CausalSelfAttention(num_heads=num_heads, d_model=d_model, dropout_rate=dropout_rate)\n",
    "        self.ffn = FeedForward(d_model, dff, dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n",
    "        self.enc_layers = [SelfAttentionFeedForwardLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [SelfAttentionFeedForwardLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "    \n",
    "    def call(self, x, context):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TimeDiffTransformer Model (Dual-Target)\n",
    "# ----------------------------\n",
    "class TimeDiffTransformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    This model predicts:\n",
    "      1. A sequence of per-step proportions (via softmax, summing to 1 over valid tokens).\n",
    "      2. An overall total time (nonnegative).\n",
    "    Predicted increments = proportions * total time,\n",
    "    and cumulative sum of increments gives predicted cumulative times.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate)\n",
    "        self.proportion_head = layers.Dense(1)\n",
    "        self.total_time_head = layers.Dense(1, activation='relu')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        encoder_out = self.encoder(inputs)\n",
    "        decoder_out = self.decoder(inputs, encoder_out)\n",
    "        proportions_logits = self.proportion_head(decoder_out)  # (batch, seq_len, 1)\n",
    "        proportions_logits = tf.squeeze(proportions_logits, axis=-1)  # (batch, seq_len)\n",
    "        proportions = tf.nn.softmax(proportions_logits, axis=-1)\n",
    "        pooled_encoder = tf.reduce_mean(encoder_out, axis=1)\n",
    "        total_time = self.total_time_head(pooled_encoder)\n",
    "        return proportions, total_time\n",
    "    \n",
    "    def predict_time_differences(self, inputs):\n",
    "        proportions, total_time = self(inputs)\n",
    "        pred_increments = proportions * total_time\n",
    "        pred_cumulative = tf.math.cumsum(pred_increments, axis=1)\n",
    "        return proportions, pred_increments, pred_cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Loss Functions with Masking for Proportions\n",
    "# ----------------------------\n",
    "def masked_proportion_loss(y_true, y_pred, mask):\n",
    "    loss = tf.square(y_true - y_pred)  # now shape (batch, L)\n",
    "    loss *= tf.cast(mask, tf.float32)\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "def total_time_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.MeanSquaredError()(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_54' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_54' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_54' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_54' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_55' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_55' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_55' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_55' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_56' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_56' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_56' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_56' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_57' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_57' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_57' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_57' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_58' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_58' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_58' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_58' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_59' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_59' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_59' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_59' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"time_diff_transformer_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"time_diff_transformer_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ encoder_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Encoder</span>)             â”‚ ?                      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">126,624</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             â”‚ ?                      â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">126,624</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_138 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8370</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_139 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ encoder_9 (\u001b[38;5;33mEncoder\u001b[0m)             â”‚ ?                      â”‚       \u001b[38;5;34m126,624\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder_9 (\u001b[38;5;33mDecoder\u001b[0m)             â”‚ ?                      â”‚       \u001b[38;5;34m126,624\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_138 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8370\u001b[0m, \u001b[38;5;34m1\u001b[0m)           â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_139 (\u001b[38;5;33mDense\u001b[0m)               â”‚ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">253,314</span> (989.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m253,314\u001b[0m (989.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">253,314</span> (989.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m253,314\u001b[0m (989.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Model Instantiation, Compilation, and Training\n",
    "# ----------------------------\n",
    "vocab_size = max(ENCODING_LEGEND.values()) + 1\n",
    "max_seq_len = X_train.shape[1]\n",
    "model = TimeDiffTransformer(num_layers=3, d_model=32, num_heads=8, dff=128,\n",
    "                            input_vocab_size=vocab_size, dropout_rate=0.1, max_len=max_seq_len)\n",
    "\n",
    "_ = model(X_train)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 6104.2061 (Props: 6103.8882, Total: 0.3177)\n",
      "Epoch 2/20 - Loss: 6103.8823 (Props: 6103.8823, Total: 0.0000)\n",
      "Epoch 3/20 - Loss: 6103.8735 (Props: 6103.8735, Total: 0.0000)\n",
      "Epoch 4/20 - Loss: 6103.8584 (Props: 6103.8584, Total: 0.0000)\n",
      "Epoch 5/20 - Loss: 6103.8306 (Props: 6103.8306, Total: 0.0000)\n",
      "Epoch 6/20 - Loss: 6103.7617 (Props: 6103.7617, Total: 0.0000)\n",
      "Epoch 7/20 - Loss: 6103.5977 (Props: 6103.5977, Total: 0.0000)\n",
      "Epoch 8/20 - Loss: 6103.3677 (Props: 6103.3677, Total: 0.0000)\n",
      "Epoch 9/20 - Loss: 6103.2056 (Props: 6103.2056, Total: 0.0000)\n",
      "Epoch 10/20 - Loss: 6103.1455 (Props: 6103.1455, Total: 0.0000)\n",
      "Epoch 11/20 - Loss: 6103.1221 (Props: 6103.1221, Total: 0.0000)\n",
      "Epoch 12/20 - Loss: 6103.1123 (Props: 6103.1123, Total: 0.0000)\n",
      "Epoch 13/20 - Loss: 6103.1064 (Props: 6103.1064, Total: 0.0000)\n",
      "Epoch 14/20 - Loss: 6103.1021 (Props: 6103.1021, Total: 0.0000)\n",
      "Epoch 15/20 - Loss: 6103.0996 (Props: 6103.0996, Total: 0.0000)\n",
      "Epoch 16/20 - Loss: 6103.0981 (Props: 6103.0981, Total: 0.0000)\n",
      "Epoch 17/20 - Loss: 6103.0972 (Props: 6103.0972, Total: 0.0000)\n",
      "Epoch 18/20 - Loss: 6103.0957 (Props: 6103.0957, Total: 0.0000)\n",
      "Epoch 19/20 - Loss: 6103.0942 (Props: 6103.0942, Total: 0.0000)\n",
      "Epoch 20/20 - Loss: 6103.0928 (Props: 6103.0928, Total: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "# For training, we'll use a custom training loop to apply the mask for the proportions loss.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y_cum, mask):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_props, pred_total = model(x)\n",
    "        # Compute true targets.\n",
    "        true_props, true_total = compute_true_targets(y_cum)\n",
    "        loss_props = masked_proportion_loss(true_props, pred_props, mask)\n",
    "        loss_total = total_time_loss(true_total, pred_total)\n",
    "        total_loss = loss_props + loss_total\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return total_loss, loss_props, loss_total\n",
    "\n",
    "# Train for a few epochs.\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss_total_val, loss_props_val, loss_total_branch_val = train_step(X_train, Y_cum_target, mask_train)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss_total_val.numpy():.4f} (Props: {loss_props_val.numpy():.4f}, Total: {loss_total_branch_val.numpy():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_54' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_54' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_54' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_54' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_55' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_55' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_55' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_55' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_56' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_56' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_56' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_56' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_57' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_57' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_57' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_57' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_58' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_58' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_58' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_58' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Predictions saved to predictions_SplitModel_all_sequences.csv\n",
      "\n",
      "ğŸ“Œ Sample predictions:\n",
      "   Sequence  Step     SourceID  Predicted_Proportion  Predicted_Increment   \n",
      "0         0     1  MRI_MSR_104              0.000001                  0.0  \\\n",
      "1         0     2       UNK(0)              0.000001                  0.0   \n",
      "2         0     3  MRI_FRR_257              0.000001                  0.0   \n",
      "3         0     4  MRI_FRR_264              0.000002                  0.0   \n",
      "4         0     5  MRI_FRR_264              0.000002                  0.0   \n",
      "5         0     6   MRI_CCS_11              0.000002                  0.0   \n",
      "6         0     7   MRI_CCS_11              0.000002                  0.0   \n",
      "7         0     8  MRI_FRR_257              0.000001                  0.0   \n",
      "8         0     9  MRI_FRR_264              0.000002                  0.0   \n",
      "9         0    10  MRI_FRR_264              0.000002                  0.0   \n",
      "\n",
      "   Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \n",
      "0                   0.0                   40.0                    40.0  \n",
      "1                   0.0                    5.0                    45.0  \n",
      "2                   0.0                    7.0                    52.0  \n",
      "3                   0.0                   16.0                    68.0  \n",
      "4                   0.0                    9.0                    77.0  \n",
      "5                   0.0                    6.0                    83.0  \n",
      "6                   0.0                  130.0                   213.0  \n",
      "7                   0.0                    1.0                   214.0  \n",
      "8                   0.0                   10.0                   224.0  \n",
      "9                   0.0                    2.0                   226.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_59' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_59' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_59' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_59' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Inference and CSV Output (Fully Fixed for Individual Sequences)\n",
    "# ----------------------------\n",
    "\n",
    "# Get predictions for the entire batch\n",
    "proportions_pred, increments_pred, cumulative_pred = model.predict_time_differences(X_train)\n",
    "\n",
    "# Convert ground truth arrays to NumPy for convenience\n",
    "gt_increments = np.concatenate([Y_cum_target[:, 0:1], Y_cum_target[:, 1:] - Y_cum_target[:, :-1]], axis=1)\n",
    "gt_cumulative = Y_cum_target\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "# Loop over each sequence in the batch\n",
    "for seq_idx in range(X_train.shape[0]):\n",
    "    # Get predictions and corresponding mask for the sequence\n",
    "    props = proportions_pred[seq_idx]  # shape: (L,)\n",
    "    incs = increments_pred[seq_idx]  # shape: (L,)\n",
    "    cumuls = cumulative_pred[seq_idx]  # shape: (L,)\n",
    "    gt_incs = gt_increments[seq_idx]  # shape: (L,)\n",
    "    gt_cumuls = gt_cumulative[seq_idx]  # shape: (L,)\n",
    "\n",
    "    # Retrieve valid indices (not padding) from the mask\n",
    "    valid_indices = np.where(mask_train[seq_idx] == 1)[0]\n",
    "\n",
    "    # Ensure that valid_indices is properly converted to a TensorFlow tensor\n",
    "    valid_indices_tf = tf.convert_to_tensor(valid_indices, dtype=tf.int32)\n",
    "\n",
    "    # Initialize step tracker\n",
    "    filtered_indices = []\n",
    "    decoded_sourceIDs = []\n",
    "    step_counter = 1  # Reset step count for each sequence\n",
    "\n",
    "    # Identify individual sequences using SeqOrder resets (new sequence starts at 0)\n",
    "    for idx in valid_indices:\n",
    "        if idx >= X_train.shape[1]:  # Prevent indexing out of bounds\n",
    "            continue\n",
    "        token = int(X_train[seq_idx, idx])  # Ensure int32 conversion\n",
    "\n",
    "        if token == START_TOKEN:\n",
    "            continue  # Skip start token\n",
    "\n",
    "        filtered_indices.append(idx)\n",
    "\n",
    "        if token == END_TOKEN:\n",
    "            decoded_sourceIDs.append(\"END\")\n",
    "        else:\n",
    "            decoded_sourceIDs.append(reverse_encoding.get(token, f\"UNK({token})\"))\n",
    "\n",
    "    # Ensure filtered indices are valid\n",
    "    if len(filtered_indices) == 0:\n",
    "        continue  # Skip this sequence if no valid tokens remain\n",
    "\n",
    "    # Convert filtered indices to TensorFlow tensor\n",
    "    filtered_indices_tf = tf.convert_to_tensor(filtered_indices, dtype=tf.int32)\n",
    "\n",
    "    # Use tf.gather() to safely extract valid prediction values\n",
    "    pred_props_filtered = tf.gather(props, filtered_indices_tf).numpy()\n",
    "    pred_incs_filtered = tf.gather(incs, filtered_indices_tf).numpy()\n",
    "    pred_cumuls_filtered = tf.gather(cumuls, filtered_indices_tf).numpy()\n",
    "    gt_incs_filtered = tf.gather(gt_incs, filtered_indices_tf).numpy()\n",
    "    gt_cumuls_filtered = tf.gather(gt_cumuls, filtered_indices_tf).numpy()\n",
    "\n",
    "    # Create DataFrame for this sequence (reset step numbering for each sequence)\n",
    "    df = pd.DataFrame({\n",
    "        \"Sequence\": seq_idx,  # Sequence index\n",
    "        \"Step\": np.arange(1, len(decoded_sourceIDs) + 1),  # Step within this sequence\n",
    "        \"SourceID\": decoded_sourceIDs,\n",
    "        \"Predicted_Proportion\": pred_props_filtered,\n",
    "        \"Predicted_Increment\": pred_incs_filtered,\n",
    "        \"Predicted_Cumulative\": pred_cumuls_filtered,\n",
    "        \"GroundTruth_Increment\": gt_incs_filtered,\n",
    "        \"GroundTruth_Cumulative\": gt_cumuls_filtered\n",
    "    })\n",
    "\n",
    "    all_outputs.append(df)\n",
    "\n",
    "# Concatenate all sequences into one DataFrame and save as CSV\n",
    "if all_outputs:\n",
    "    output_df = pd.concat(all_outputs, ignore_index=True)\n",
    "    output_csv = \"predictions_SplitModel_all_sequences.csv\"\n",
    "    output_df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nâœ… Predictions saved to {output_csv}\")\n",
    "    print(\"\\nğŸ“Œ Sample predictions:\")\n",
    "    print(output_df.head(10))\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No valid sequences were found for output. Check data processing.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
