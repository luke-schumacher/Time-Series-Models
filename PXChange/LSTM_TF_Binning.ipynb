{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import AdamW # Using AdamW for weight decay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os # Added for file path handling\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Validation\n",
    "def load_and_validate_data(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Load and validate transformer predictions CSV file.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Validated dataframe.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file does not exist.\n",
    "        ValueError: If required columns are missing or have wrong types.\n",
    "        Exception: For other potential loading errors.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Error: Input file not found at {transformer_predictions_file}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(transformer_predictions_file)\n",
    "        print(f\"Successfully loaded CSV. Shape: {df.shape}\")\n",
    "        print(f\"Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "        # Basic validation checks\n",
    "        # Ensure these columns exist based on subsequent usage\n",
    "        required_columns = ['Sequence', 'Step', 'SourceID', 'Predicted_Proportion',\n",
    "                            'Predicted_Increment', 'Predicted_Cumulative',\n",
    "                            'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
    "\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in input CSV: {', '.join(missing_cols)}\")\n",
    "        print(\"All required columns are present.\")\n",
    "\n",
    "        # Check for essential numeric types\n",
    "        numeric_cols_to_check = ['Step', 'Predicted_Proportion', 'Predicted_Increment',\n",
    "                                 'Predicted_Cumulative', 'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
    "        for col in numeric_cols_to_check:\n",
    "             # Attempt to convert to numeric, coercing errors to NaN\n",
    "             df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "             if df[col].isnull().any():\n",
    "                 print(f\"Warning: Column '{col}' contains non-numeric values that were converted to NaN.\")\n",
    "                 # Optionally raise ValueError if NaNs are unacceptable here\n",
    "                 # raise ValueError(f\"Column '{col}' must be entirely numeric.\")\n",
    "\n",
    "        print(\"Data validation checks passed.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading error: {e}\")\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion Binning\n",
    "def bin_proportions(proportions, bin_size=0.05):\n",
    "    \"\"\"\n",
    "    Bin proportions and normalize if the sum deviates significantly from 1.0.\n",
    "\n",
    "    Args:\n",
    "        proportions (array-like): Original proportion values.\n",
    "        bin_size (float): Bin resolution.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binned and potentially normalized proportions.\n",
    "    \"\"\"\n",
    "    # Ensure input is a numpy array for calculations\n",
    "    proportions = np.asarray(proportions)\n",
    "    \n",
    "    # Avoid division by zero if bin_size is zero\n",
    "    if bin_size <= 0:\n",
    "        raise ValueError(\"bin_size must be positive.\")\n",
    "        \n",
    "    binned_props = np.round(proportions / bin_size) * bin_size\n",
    "    total = np.sum(binned_props)\n",
    "\n",
    "    # Normalize only if the sum is significantly different from 1.0\n",
    "    # Use a small tolerance (epsilon) for floating point comparison\n",
    "    epsilon = 1e-6\n",
    "    if abs(total - 1.0) > 0.1 and abs(total) > epsilon: # Avoid division by near-zero total\n",
    "        print(f\"Warning: Sum of binned proportions ({total:.4f}) deviates > 0.1 from 1.0. Normalizing.\")\n",
    "        binned_props = binned_props / total\n",
    "    elif abs(total) <= epsilon and len(binned_props) > 0:\n",
    "         print(\"Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\")\n",
    "         # Return original binned props or zeros, depending on desired behavior\n",
    "         # Returning zeros might be safer if normalization is expected\n",
    "         # return np.zeros_like(binned_props)\n",
    "\n",
    "    return binned_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing for LSTM (Minor logging added)\n",
    "def process_transformer_predictions(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process transformer predictions for LSTM model input.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed data dictionary containing padded sequences, masks, etc.\n",
    "    \"\"\"\n",
    "    df = load_and_validate_data(transformer_predictions_file)\n",
    "    sequences = df['Sequence'].unique()\n",
    "    print(f\"Found {len(sequences)} unique sequences in the data.\")\n",
    "\n",
    "    X_data, y_increments, y_cumulative, masks = [], [], [], []\n",
    "    prop_scaler, inc_scaler, cum_scaler = StandardScaler(), StandardScaler(), StandardScaler()\n",
    "    processed_sequence_ids = [] # Keep track of sequences successfully processed\n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_data = df[df['Sequence'] == seq_id].sort_values('Step')\n",
    "        seq_len = len(seq_data)\n",
    "        if seq_len == 0:\n",
    "            print(f\"Warning: Sequence {seq_id} has no data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            binned_proportions = bin_proportions(seq_data['Predicted_Proportion'].values)\n",
    "            scaled_props = prop_scaler.fit_transform(binned_proportions.reshape(-1, 1)).flatten()\n",
    "            # Handle potential NaNs introduced by coerce during loading before scaling\n",
    "            gt_inc_valid = seq_data['GroundTruth_Increment'].dropna()\n",
    "            gt_cum_valid = seq_data['GroundTruth_Cumulative'].dropna()\n",
    "            if gt_inc_valid.empty or gt_cum_valid.empty:\n",
    "                 print(f\"Warning: Sequence {seq_id} has only NaN ground truth values after coercion. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            scaled_gt_inc = inc_scaler.fit_transform(gt_inc_valid.values.reshape(-1, 1)).flatten()\n",
    "            scaled_gt_cum = cum_scaler.fit_transform(gt_cum_valid.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Ensure lengths match after potential dropna (this indicates an issue if they don't)\n",
    "            if len(scaled_gt_inc) != seq_len or len(scaled_gt_cum) != seq_len:\n",
    "                 print(f\"Warning: Length mismatch after handling NaNs in sequence {seq_id}. Check input data. Skipping.\")\n",
    "                 # This case needs careful handling - maybe impute NaNs instead of dropping?\n",
    "                 # For now, skipping the sequence if lengths don't match after dropna.\n",
    "                 continue\n",
    "\n",
    "\n",
    "            max_step = seq_data['Step'].max()\n",
    "            normalized_step = seq_data['Step'].values / max_step if max_step > 0 else np.zeros(seq_len)\n",
    "\n",
    "            features = np.column_stack([scaled_props, scaled_gt_inc, scaled_gt_cum, normalized_step])\n",
    "            increments = seq_data['GroundTruth_Increment'].values # Use original values with potential NaNs for target\n",
    "            cumulative = seq_data['GroundTruth_Cumulative'].values # Use original values with potential NaNs for target\n",
    "\n",
    "            X_data.append(features)\n",
    "            y_increments.append(increments)\n",
    "            y_cumulative.append(cumulative)\n",
    "            masks.append(np.ones(seq_len))\n",
    "            processed_sequence_ids.append(seq_id) # Add ID if processed successfully\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence {seq_id}: {e}. Skipping sequence.\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    if not X_data:\n",
    "         raise ValueError(\"No valid sequences found after processing. Cannot proceed.\")\n",
    "\n",
    "    num_processed_sequences = len(X_data)\n",
    "    print(f\"Successfully processed {num_processed_sequences} sequences.\")\n",
    "\n",
    "    max_length = max(len(x) for x in X_data)\n",
    "    num_features = X_data[0].shape[1]\n",
    "    print(f\"Padding sequences to max length: {max_length}\")\n",
    "\n",
    "    # Initialize padded arrays - use np.nan as fill value for targets might be better\n",
    "    X_padded = np.zeros((num_processed_sequences, max_length, num_features), dtype=np.float32)\n",
    "    y_increments_padded = np.full((num_processed_sequences, max_length), np.nan, dtype=np.float32)\n",
    "    y_cumulative_padded = np.full((num_processed_sequences, max_length), np.nan, dtype=np.float32)\n",
    "    masks_padded = np.zeros((num_processed_sequences, max_length), dtype=np.float32)\n",
    "\n",
    "    for i in range(num_processed_sequences):\n",
    "        seq_len = len(X_data[i])\n",
    "        X_padded[i, :seq_len, :] = X_data[i]\n",
    "        y_increments_padded[i, :seq_len] = y_increments[i]\n",
    "        y_cumulative_padded[i, :seq_len] = y_cumulative[i]\n",
    "        # Mask should be 0 where target is NaN, 1 otherwise\n",
    "        masks_padded[i, :seq_len] = (~np.isnan(y_increments[i])).astype(np.float32) # Example: mask based on increment NaNs\n",
    "\n",
    "    # Recalculate total times based on the potentially NaN-filled padded ground truth\n",
    "    y_total_times = []\n",
    "    for i in range(num_processed_sequences):\n",
    "        valid_indices = np.where(masks_padded[i] == 1)[0]\n",
    "        if len(valid_indices) > 0:\n",
    "            last_valid_index = valid_indices[-1]\n",
    "            y_total_times.append(y_cumulative_padded[i, last_valid_index])\n",
    "        else:\n",
    "            y_total_times.append(0) # Or np.nan if preferred\n",
    "    y_total_times = np.array(y_total_times, dtype=np.float32)\n",
    "\n",
    "    # Replace NaNs in target arrays with 0 for training (loss function needs numbers)\n",
    "    # The mask will handle ignoring these steps during loss calculation.\n",
    "    y_increments_padded = np.nan_to_num(y_increments_padded, nan=0.0)\n",
    "    y_cumulative_padded = np.nan_to_num(y_cumulative_padded, nan=0.0)\n",
    "    # y_total_times should already be numeric based on calculation above\n",
    "\n",
    "    print(\"Data processing and padding complete.\")\n",
    "    return {\n",
    "        'X': X_padded,\n",
    "        'y_increments': y_increments_padded,\n",
    "        'y_cumulative': y_cumulative_padded,\n",
    "        'y_total_times': y_total_times,\n",
    "        'masks': masks_padded, # Crucial: Mask reflects original NaNs\n",
    "        'sequences': processed_sequence_ids, # Use the list of successfully processed IDs\n",
    "        'df': df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Masked Loss Function (Updated mask handling logic)\n",
    "def custom_masked_huber_loss(mask_tensor, delta=1.0):\n",
    "    \"\"\"\n",
    "    Factory function to create a masked Huber loss.\n",
    "    Mask should be 1 for valid steps, 0 for padded/invalid steps.\n",
    "\n",
    "    Args:\n",
    "        mask_tensor (tf.Tensor): The mask indicating valid time steps (batch_size, seq_len).\n",
    "                                 Passed during training setup.\n",
    "        delta (float): Huber loss delta parameter.\n",
    "\n",
    "    Returns:\n",
    "        Callable: A loss function `masked_huber(y_true, y_pred)`.\n",
    "    \"\"\"\n",
    "    # Convert the persistent mask tensor to float32 once\n",
    "    mask_float32 = tf.cast(mask_tensor, tf.float32)\n",
    "\n",
    "    def masked_huber(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate Huber loss only for valid (masked == 1) time steps.\n",
    "\n",
    "        Args:\n",
    "            y_true (tf.Tensor): Ground truth tensor (batch_size, seq_len).\n",
    "            y_pred (tf.Tensor): Prediction tensor (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Scalar mean loss over the batch.\n",
    "        \"\"\"\n",
    "        # Slice the persistent mask for the current batch size dynamically\n",
    "        current_batch_size = tf.shape(y_true)[0]\n",
    "        # Assume mask_tensor corresponds row-wise to the full dataset\n",
    "        mask_batch = mask_float32[:current_batch_size]\n",
    "\n",
    "        # Ensure mask_batch has compatible shape (e.g., (batch_size, seq_len))\n",
    "        # This might require reshaping or broadcasting depending on exact shapes\n",
    "        # Assuming y_true, y_pred, mask_batch are all (batch_size, seq_len)\n",
    "\n",
    "        # Huber loss calculation\n",
    "        error = y_true - y_pred\n",
    "        abs_error = tf.abs(error)\n",
    "        quadratic = tf.minimum(abs_error, delta)\n",
    "        linear = abs_error - quadratic\n",
    "        huber_loss = 0.5 * quadratic**2 + delta * linear\n",
    "\n",
    "        # Apply the mask (element-wise multiplication)\n",
    "        masked_loss = huber_loss * mask_batch\n",
    "\n",
    "        # Calculate mean loss per sequence, avoiding division by zero\n",
    "        # Sum loss over sequence length dimension\n",
    "        sum_loss_per_sequence = tf.reduce_sum(masked_loss, axis=1)\n",
    "        # Sum mask over sequence length dimension to get count of valid tokens\n",
    "        valid_tokens_per_sequence = tf.reduce_sum(mask_batch, axis=1)\n",
    "\n",
    "        # Avoid division by zero for sequences with no valid tokens\n",
    "        # Replace zero counts with 1; the corresponding sum_loss will also be 0.\n",
    "        valid_tokens_safe = tf.where(valid_tokens_per_sequence == 0,\n",
    "                                     tf.ones_like(valid_tokens_per_sequence),\n",
    "                                     valid_tokens_per_sequence)\n",
    "\n",
    "        mean_loss_per_sequence = sum_loss_per_sequence / valid_tokens_safe\n",
    "\n",
    "        # Return the mean loss across the batch\n",
    "        return tf.reduce_mean(mean_loss_per_sequence)\n",
    "\n",
    "    # Assign a name for clarity in logs/history if possible\n",
    "    masked_huber.__name__ = f'masked_huber_delta_{delta}'\n",
    "    return masked_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved LSTM Model Definition\n",
    "class ImprovedTimeDiffLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    LSTM model with enhancements like LayerNorm, Attention, Residuals, and Regularization.\n",
    "    Predicts time differences, total time, and calculates cumulative times.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=64, dropout_rate=0.3, num_attention_heads=4, l2_reg=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the model layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_units (int): Number of units in the main LSTM layer.\n",
    "            dropout_rate (float): Dropout rate for LSTM and Attention layers.\n",
    "            num_attention_heads (int): Number of heads for MultiHeadAttention.\n",
    "            l2_reg (float): L2 regularization factor for the first LSTM kernel.\n",
    "        \"\"\"\n",
    "        super().__init__() # Use super().__init__() for Python 3 style\n",
    "\n",
    "        # Input normalization\n",
    "        self.input_normalization = layers.LayerNormalization(name=\"InputNorm\")\n",
    "\n",
    "        # --- LSTM Block ---\n",
    "        # First LSTM layer with regularization and return sequences\n",
    "        self.lstm_layer1 = layers.LSTM(\n",
    "            hidden_units,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate, # Be cautious with recurrent_dropout on GPU\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            name=\"LSTM1\"\n",
    "        )\n",
    "        # Dense layer for residual connection matching dimensions\n",
    "        # Ensure it outputs `hidden_units` to match lstm_layer1 output\n",
    "        self.residual_dense = layers.Dense(hidden_units, activation='relu', name=\"ResidualDense\")\n",
    "        self.add_layer1 = layers.Add(name=\"AddResidual1\") # Explicit Add layer\n",
    "        self.norm_layer1 = layers.LayerNormalization(name=\"Norm1\") # Normalize after residual\n",
    "\n",
    "        # Second LSTM layer\n",
    "        self.lstm_layer2 = layers.LSTM(\n",
    "            hidden_units // 2, # Reduce dimensionality\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate,\n",
    "            name=\"LSTM2\"\n",
    "        )\n",
    "\n",
    "        # --- Attention Block ---\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_attention_heads,\n",
    "            key_dim=hidden_units // 2, # Key dim often matches the query/value dim\n",
    "            dropout=dropout_rate,\n",
    "            name=\"MultiHeadAttention\"\n",
    "        )\n",
    "        self.add_layer2 = layers.Add(name=\"AddResidual2\") # Add attention output to LSTM output\n",
    "        self.norm_layer2 = layers.LayerNormalization(name=\"Norm2\") # Normalize after attention\n",
    "\n",
    "        # --- Output Heads ---\n",
    "        # 1. Time Difference Head (predicts increment for each step)\n",
    "        # Use 'relu' to enforce non-negativity for time differences\n",
    "        self.time_diff_head = layers.Dense(1, activation='relu', name='TimeDiffHead')\n",
    "\n",
    "        # 2. Total Time Head (predicts total duration for the sequence)\n",
    "        # Takes the aggregated sequence representation after attention\n",
    "        # Use 'softplus' or 'relu' for non-negativity\n",
    "        self.total_time_head = layers.Dense(1, activation='softplus', name='TotalTimeHead')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor (batch_size, seq_len, num_features).\n",
    "            training (bool): Whether the model is in training mode (for dropout).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (predicted_time_diffs, predicted_total_time, predicted_cumulative_times)\n",
    "        \"\"\"\n",
    "        # Input normalization\n",
    "        x = self.input_normalization(inputs)\n",
    "\n",
    "        # --- LSTM Block with Residual ---\n",
    "        lstm_out1 = self.lstm_layer1(x, training=training)\n",
    "        residual = self.residual_dense(x) # Project input for residual connection\n",
    "        lstm_out1 = self.add_layer1([lstm_out1, residual])\n",
    "        lstm_out1 = self.norm_layer1(lstm_out1) # Normalize after adding\n",
    "\n",
    "        # Second LSTM layer\n",
    "        lstm_out2 = self.lstm_layer2(lstm_out1, training=training)\n",
    "\n",
    "        # --- Attention Block with Residual ---\n",
    "        # Use lstm_out2 as query, key, and value for self-attention\n",
    "        attn_out = self.attention(query=lstm_out2, value=lstm_out2, key=lstm_out2, training=training)\n",
    "        # Add residual connection from before attention\n",
    "        attn_out = self.add_layer2([attn_out, lstm_out2])\n",
    "        attn_out = self.norm_layer2(attn_out) # Normalize after adding\n",
    "\n",
    "        # --- Output Generation ---\n",
    "        # 1. Time Differences (per step)\n",
    "        # Apply the dense head to the output of the attention block\n",
    "        time_diffs = self.time_diff_head(attn_out)\n",
    "        # Remove the last dimension (Dense adds a dim of 1)\n",
    "        time_diffs = tf.squeeze(time_diffs, axis=-1) # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # 2. Total Time (per sequence)\n",
    "        # Aggregate the sequence information (e.g., mean pooling over time)\n",
    "        sequence_encoding = tf.reduce_mean(attn_out, axis=1) # Shape: (batch_size, hidden_units//2)\n",
    "        total_time = self.total_time_head(sequence_encoding) # Shape: (batch_size, 1)\n",
    "\n",
    "        # 3. Cumulative Times (calculated from predicted differences)\n",
    "        # Ensure calculation happens correctly even if time_diffs has NaNs (shouldn't if using relu)\n",
    "        cumulative_times = tf.cumsum(time_diffs, axis=1) # Shape: (batch_size, seq_len)\n",
    "\n",
    "        return time_diffs, total_time, cumulative_times\n",
    "\n",
    "    # Optional: Build method to define input shape explicitly\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Function (Added build confirmation)\n",
    "def train_improved_lstm(transformer_predictions_file, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Preprocess data, build, compile, and train the improved LSTM model.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to the transformer predictions CSV.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Training batch size.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_model, training_history, processed_data)\n",
    "    \"\"\"\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Starting LSTM Model Training Process\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 1. Process Data\n",
    "    print(\"Processing data...\")\n",
    "    data = process_transformer_predictions(transformer_predictions_file)\n",
    "    X_train = data['X']\n",
    "    y_inc_train = data['y_increments']\n",
    "    y_cum_train = data['y_cumulative']\n",
    "    y_total_train = data['y_total_times']\n",
    "    masks_train = data['masks']\n",
    "\n",
    "    if X_train.shape[0] == 0:\n",
    "        print(\"Error: No data available for training after processing.\")\n",
    "        return None, None, data\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    print(f\"Input shape for LSTM: {input_shape}\")\n",
    "    print(f\"Number of sequences for training/validation: {X_train.shape[0]}\")\n",
    "    print(f\"Target shapes: Increments {y_inc_train.shape}, Cumulative {y_cum_train.shape}, Total {y_total_train.shape}\")\n",
    "    print(f\"Mask shape: {masks_train.shape}, Mask sum (total valid steps): {np.sum(masks_train)}\")\n",
    "\n",
    "    # 2. Build Model\n",
    "    print(\"Building model...\")\n",
    "    lstm_model = ImprovedTimeDiffLSTM()\n",
    "    # Explicitly build the model\n",
    "    lstm_model.build(input_shape=(None,) + input_shape)\n",
    "    # *** Add confirmation print ***\n",
    "    print(f\"Model built status: {lstm_model.built}\")\n",
    "    # Print summary *after* building\n",
    "    lstm_model.summary(line_length=100)\n",
    "\n",
    "    # 3. Define Loss Functions\n",
    "    print(\"Defining loss functions...\")\n",
    "    masked_huber_loss_fn = custom_masked_huber_loss(tf.constant(masks_train, dtype=tf.float32))\n",
    "    total_time_loss_fn = tf.keras.losses.MeanSquaredError(name='total_time_mse')\n",
    "    loss_functions = [masked_huber_loss_fn, total_time_loss_fn, masked_huber_loss_fn]\n",
    "    loss_weights = [0.4, 0.3, 0.3]\n",
    "    print(f\"Loss functions set. Weights: {loss_weights}\")\n",
    "\n",
    "    # 4. Define Optimizer\n",
    "    print(\"Configuring optimizer...\")\n",
    "    learning_rate_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    optimizer = AdamW(learning_rate=learning_rate_schedule, weight_decay=0.001)\n",
    "\n",
    "    # 5. Compile Model\n",
    "    print(\"Compiling model...\")\n",
    "    lstm_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_functions,\n",
    "        loss_weights=loss_weights,\n",
    "    )\n",
    "    print(\"Model compiled successfully.\")\n",
    "\n",
    "    # 6. Define Callbacks\n",
    "    print(\"Configuring callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 7. Train Model\n",
    "    print(f\"Starting training for {epochs} epochs with batch size {batch_size}...\")\n",
    "    history = lstm_model.fit(\n",
    "        X_train,\n",
    "        [y_inc_train, y_total_train, y_cum_train],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Training finished.\")\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history.get('val_loss', [np.nan])[-1]\n",
    "    print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return lstm_model, history, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Generation Function (Changed output filename)\n",
    "def generate_lstm_predictions_from_transformer_csv(lstm_model, data):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained LSTM model and compare with transformer.\n",
    "\n",
    "    Args:\n",
    "        lstm_model (tf.keras.Model): The trained LSTM model.\n",
    "        data (dict): The processed data dictionary from `process_transformer_predictions`.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe containing original data, transformer predictions,\n",
    "                          LSTM predictions, and comparison metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Generating LSTM predictions...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    X_predict = data['X']\n",
    "    original_df = data['df']\n",
    "    processed_sequences = data['sequences']\n",
    "    masks_predict = data['masks']\n",
    "\n",
    "    if X_predict.shape[0] == 0:\n",
    "        print(\"Error: No data available for prediction.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Predicting on {X_predict.shape[0]} sequences...\")\n",
    "    try:\n",
    "        time_diffs_pred, total_time_pred, cumulative_times_pred = lstm_model.predict(X_predict)\n",
    "        print(\"Model prediction successful.\")\n",
    "        print(f\"Predicted shapes: Diffs {time_diffs_pred.shape}, Total {total_time_pred.shape}, Cumul {cumulative_times_pred.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model prediction: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    results_list = []\n",
    "    print(f\"Processing predictions for {len(processed_sequences)} sequences...\")\n",
    "\n",
    "    for seq_idx, seq_id in enumerate(processed_sequences):\n",
    "        seq_original_data = original_df[original_df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        seq_len = int(np.sum(masks_predict[seq_idx]))\n",
    "\n",
    "        if seq_len == 0:\n",
    "            print(f\"Warning: Sequence {seq_id} (index {seq_idx}) has length 0 in mask during prediction. Skipping.\")\n",
    "            continue\n",
    "        if seq_len > time_diffs_pred.shape[1]:\n",
    "             print(f\"Warning: Sequence {seq_id} (index {seq_idx}) mask length ({seq_len}) exceeds prediction dimension ({time_diffs_pred.shape[1]}). Clamping length.\")\n",
    "             seq_len = time_diffs_pred.shape[1]\n",
    "\n",
    "        current_time_diffs = time_diffs_pred[seq_idx, :seq_len]\n",
    "        current_cumulative = cumulative_times_pred[seq_idx, :seq_len]\n",
    "        current_total = total_time_pred[seq_idx, 0]\n",
    "\n",
    "        if len(seq_original_data) == seq_len:\n",
    "            seq_original_data['LSTM_Predicted_TimeDiff'] = current_time_diffs\n",
    "            seq_original_data['LSTM_Predicted_Cumulative'] = current_cumulative\n",
    "            seq_original_data['LSTM_Predicted_TotalTime'] = current_total\n",
    "        elif len(seq_original_data) > seq_len:\n",
    "             print(f\"Warning: Original data length ({len(seq_original_data)}) > mask length ({seq_len}) for seq {seq_id}. Padding predictions.\")\n",
    "             padded_diffs = np.pad(current_time_diffs, (0, len(seq_original_data) - seq_len), constant_values=np.nan)\n",
    "             padded_cumul = np.pad(current_cumulative, (0, len(seq_original_data) - seq_len), constant_values=np.nan)\n",
    "             seq_original_data['LSTM_Predicted_TimeDiff'] = padded_diffs\n",
    "             seq_original_data['LSTM_Predicted_Cumulative'] = padded_cumul\n",
    "             seq_original_data['LSTM_Predicted_TotalTime'] = current_total\n",
    "        else:\n",
    "             print(f\"Error: Original data length ({len(seq_original_data)}) < mask length ({seq_len}) for seq {seq_id}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        required_orig_cols = ['GroundTruth_Increment', 'Predicted_Increment',\n",
    "                              'GroundTruth_Cumulative', 'Predicted_Cumulative']\n",
    "        if not all(col in seq_original_data for col in required_orig_cols):\n",
    "             print(f\"Warning: Missing required original prediction columns for sequence {seq_id}. Cannot calculate improvements.\")\n",
    "             seq_original_data['TimeDiff_Improvement_Abs'] = np.nan\n",
    "             seq_original_data['Cumulative_Improvement_Abs'] = np.nan\n",
    "        else:\n",
    "            transformer_diff_error = abs(seq_original_data['GroundTruth_Increment'] - seq_original_data['Predicted_Increment'])\n",
    "            lstm_diff_error = abs(seq_original_data['GroundTruth_Increment'] - seq_original_data['LSTM_Predicted_TimeDiff'])\n",
    "            transformer_cum_error = abs(seq_original_data['GroundTruth_Cumulative'] - seq_original_data['Predicted_Cumulative'])\n",
    "            lstm_cum_error = abs(seq_original_data['GroundTruth_Cumulative'] - seq_original_data['LSTM_Predicted_Cumulative'])\n",
    "            seq_original_data['TimeDiff_Improvement_Abs'] = transformer_diff_error - lstm_diff_error\n",
    "            seq_original_data['Cumulative_Improvement_Abs'] = transformer_cum_error - lstm_cum_error\n",
    "\n",
    "        results_list.append(seq_original_data)\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"Error: No results generated after processing predictions.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    results_df = pd.concat(results_list, ignore_index=True)\n",
    "    print(f\"Finished processing predictions. Combined results shape: {results_df.shape}\")\n",
    "    print(\"Combined results head:\\n\", results_df.head())\n",
    "    print(\"Combined results tail:\\n\", results_df.tail())\n",
    "    print(\"Checking for NaNs in LSTM predictions:\")\n",
    "    print(results_df[['LSTM_Predicted_TimeDiff', 'LSTM_Predicted_Cumulative', 'LSTM_Predicted_TotalTime']].isnull().sum())\n",
    "\n",
    "    # *** Changed output filename ***\n",
    "    output_filename = 'predictions_lstm_refined_run2.csv'\n",
    "    output_path = os.path.abspath(output_filename)\n",
    "    try:\n",
    "        results_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Combined predictions successfully saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to CSV ({output_path}): {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Overall Model Performance Comparison (Mean Absolute Error)\")\n",
    "    print(\"-\" * 30)\n",
    "    comparison_cols = required_orig_cols + ['LSTM_Predicted_TimeDiff', 'LSTM_Predicted_Cumulative']\n",
    "    valid_results = results_df.dropna(subset=comparison_cols)\n",
    "    print(f\"Calculating MAE based on {len(valid_results)} valid rows (after dropping NaNs in comparison columns).\")\n",
    "\n",
    "    if valid_results.empty:\n",
    "        print(\"Warning: No valid data points found for calculating overall MAE.\")\n",
    "    else:\n",
    "        try:\n",
    "            transformer_time_diff_mae = np.mean(abs(valid_results['GroundTruth_Increment'] - valid_results['Predicted_Increment']))\n",
    "            lstm_time_diff_mae = np.mean(abs(valid_results['GroundTruth_Increment'] - valid_results['LSTM_Predicted_TimeDiff']))\n",
    "            transformer_cumulative_mae = np.mean(abs(valid_results['GroundTruth_Cumulative'] - valid_results['Predicted_Cumulative']))\n",
    "            lstm_cumulative_mae = np.mean(abs(valid_results['GroundTruth_Cumulative'] - valid_results['LSTM_Predicted_Cumulative']))\n",
    "\n",
    "            time_diff_improvement_pct = (1 - lstm_time_diff_mae / transformer_time_diff_mae) * 100 if transformer_time_diff_mae != 0 else float('inf') if lstm_time_diff_mae < transformer_time_diff_mae else 0\n",
    "            cumulative_improvement_pct = (1 - lstm_cumulative_mae / transformer_cumulative_mae) * 100 if transformer_cumulative_mae != 0 else float('inf') if lstm_cumulative_mae < transformer_cumulative_mae else 0\n",
    "\n",
    "            print(f\"Time Differences MAE:\")\n",
    "            print(f\"  Transformer: {transformer_time_diff_mae:.4f}\")\n",
    "            print(f\"  LSTM:        {lstm_time_diff_mae:.4f}\")\n",
    "            print(f\"  Improvement: {time_diff_improvement_pct:.2f}%\")\n",
    "            print(f\"\\nCumulative Times MAE:\")\n",
    "            print(f\"  Transformer: {transformer_cumulative_mae:.4f}\")\n",
    "            print(f\"  LSTM:        {lstm_cumulative_mae:.4f}\")\n",
    "            print(f\"  Improvement: {cumulative_improvement_pct:.2f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating overall MAE: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM model training, prediction, and visualization.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\" LSTM Model Training and Evaluation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # --- Configuration ---\n",
    "    transformer_predictions_file = \"predictions_transformer.csv\"\n",
    "    training_epochs = 100\n",
    "    training_batch_size = 32\n",
    "    output_suffix = \"_run2\" # Suffix for output files\n",
    "\n",
    "    lstm_model = None\n",
    "    history = None\n",
    "    data = None\n",
    "    results_df = None\n",
    "\n",
    "    try:\n",
    "        # --- Step 1: Train LSTM Model ---\n",
    "        print(\"\\n--- Step 1: Training LSTM Model ---\")\n",
    "        lstm_model, history, data = train_improved_lstm(\n",
    "            transformer_predictions_file,\n",
    "            epochs=training_epochs,\n",
    "            batch_size=training_batch_size\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Generate Predictions ---\n",
    "        print(\"\\n--- Step 2: Generating LSTM Predictions ---\")\n",
    "        if lstm_model is None or data is None:\n",
    "             print(\"Model training or data processing failed. Cannot generate predictions.\")\n",
    "        else:\n",
    "            # Pass suffix to prediction function if needed, or handle filenames inside main\n",
    "            results_df = generate_lstm_predictions_from_transformer_csv(lstm_model, data) # Filename handled inside\n",
    "\n",
    "        # --- Step 3: Display Sample Results ---\n",
    "        print(\"\\n--- Step 3: Displaying Sample Results ---\")\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            print(\"\\nSample Combined Predictions (Head):\")\n",
    "            display_cols = ['Sequence', 'Step', 'SourceID',\n",
    "                            'GroundTruth_Increment', 'Predicted_Increment', 'LSTM_Predicted_TimeDiff',\n",
    "                            'GroundTruth_Cumulative','Predicted_Cumulative','LSTM_Predicted_Cumulative',\n",
    "                            'TimeDiff_Improvement_Abs', 'Cumulative_Improvement_Abs']\n",
    "            display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "            print(results_df[display_cols].head(10).to_string())\n",
    "        else:\n",
    "            print(\"No results dataframe generated or it is empty.\")\n",
    "\n",
    "        # --- Step 4: Generate Visualizations ---\n",
    "        print(\"\\n--- Step 4: Generating Visualizations ---\")\n",
    "        # Pass suffix to visualization function if needed, or handle filenames inside main\n",
    "        visualize_results(results_df, history, num_samples=3) # Filename handled inside\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" Pipeline Execution Completed\")\n",
    "        # Provide paths to output files with the new suffix\n",
    "        print(f\"  - Combined predictions CSV saved to: {os.path.abspath(f'predictions_lstm_refined{output_suffix}.csv')}\")\n",
    "        print(f\"  - Training performance plot saved to: {os.path.abspath(f'training_performance{output_suffix}.png')}\")\n",
    "        print(f\"  - Sequence comparison plots saved as: sequence_<ID>_comparison{output_suffix}.png\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"\\nFatal Error: {fnf_error}\")\n",
    "        print(\"Please ensure the input CSV file exists and the path is correct.\")\n",
    "    except ValueError as val_error:\n",
    "        print(f\"\\nFatal Error: {val_error}\")\n",
    "        print(\"Please check the data format, required columns, and content in the input file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected fatal error occurred in main execution: {e}\")\n",
    "        print(\"\\n--- Traceback ---\")\n",
    "        traceback.print_exc()\n",
    "        print(\"--- End Traceback ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM model training, prediction, and visualization.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\" LSTM Model Training and Evaluation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # --- Configuration ---\n",
    "    transformer_predictions_file = \"predictions_transformer.csv\" # Ensure this file exists!\n",
    "    training_epochs = 100 # Use the original epoch count\n",
    "    training_batch_size = 32\n",
    "\n",
    "    lstm_model = None\n",
    "    history = None\n",
    "    data = None\n",
    "    results_df = None\n",
    "\n",
    "    try:\n",
    "        # --- Step 1: Train LSTM Model ---\n",
    "        print(\"\\n--- Step 1: Training LSTM Model ---\")\n",
    "        lstm_model, history, data = train_improved_lstm(\n",
    "            transformer_predictions_file,\n",
    "            epochs=training_epochs,\n",
    "            batch_size=training_batch_size\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Generate Predictions ---\n",
    "        print(\"\\n--- Step 2: Generating LSTM Predictions ---\")\n",
    "        if lstm_model is None or data is None:\n",
    "             print(\"Model training or data processing failed. Cannot generate predictions.\")\n",
    "             # Optionally: try loading a pre-trained model if available\n",
    "        else:\n",
    "            results_df = generate_lstm_predictions_from_transformer_csv(lstm_model, data)\n",
    "\n",
    "        # --- Step 3: Display Sample Results ---\n",
    "        print(\"\\n--- Step 3: Displaying Sample Results ---\")\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            print(\"\\nSample Combined Predictions (Head):\")\n",
    "            display_cols = ['Sequence', 'Step', 'SourceID',\n",
    "                            'GroundTruth_Increment', 'Predicted_Increment', 'LSTM_Predicted_TimeDiff',\n",
    "                            'GroundTruth_Cumulative','Predicted_Cumulative','LSTM_Predicted_Cumulative',\n",
    "                            'TimeDiff_Improvement_Abs', 'Cumulative_Improvement_Abs']\n",
    "            display_cols = [col for col in display_cols if col in results_df.columns] # Filter existing columns\n",
    "            print(results_df[display_cols].head(10).to_string())\n",
    "        else:\n",
    "            print(\"No results dataframe generated or it is empty.\")\n",
    "\n",
    "        # --- Step 4: Generate Visualizations ---\n",
    "        print(\"\\n--- Step 4: Generating Visualizations ---\")\n",
    "        # Pass history and results_df even if they are None/empty, the function handles it\n",
    "        visualize_results(results_df, history, num_samples=3)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" Pipeline Execution Completed\")\n",
    "        # Provide paths to output files\n",
    "        print(f\"  - Combined predictions CSV saved to: {os.path.abspath('predictions_lstm_refined_1.csv')}\")\n",
    "        print(f\"  - Training performance plot saved to: {os.path.abspath('training_performance.png')}\")\n",
    "        print(\"  - Sequence comparison plots saved as: sequence_<ID>_comparison.png\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"\\nFatal Error: {fnf_error}\")\n",
    "        print(\"Please ensure the input CSV file exists and the path is correct.\")\n",
    "    except ValueError as val_error:\n",
    "        print(f\"\\nFatal Error: {val_error}\")\n",
    "        print(\"Please check the data format, required columns, and content in the input file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected fatal error occurred in main execution: {e}\")\n",
    "        print(\"\\n--- Traceback ---\")\n",
    "        traceback.print_exc()\n",
    "        print(\"--- End Traceback ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " LSTM Model Training and Evaluation Pipeline\n",
      "==================================================\n",
      "\n",
      "--- Step 1: Training LSTM Model ---\n",
      "------------------------------\n",
      "Starting LSTM Model Training Process\n",
      "------------------------------\n",
      "Processing data...\n",
      "Attempting to load data from: predictions_transformer.csv\n",
      "Successfully loaded CSV. Shape: (3986, 8)\n",
      "Columns found: ['Sequence', 'Step', 'SourceID', 'Predicted_Proportion', 'Predicted_Increment', 'Predicted_Cumulative', 'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
      "All required columns are present.\n",
      "Data validation checks passed.\n",
      "Found 186 unique sequences in the data.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.9000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.9000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Successfully processed 186 sequences.\n",
      "Padding sequences to max length: 45\n",
      "Data processing and padding complete.\n",
      "Input shape for LSTM: (45, 4)\n",
      "Number of sequences for training/validation: 186\n",
      "Target shapes: Increments (186, 45), Cumulative (186, 45), Total (186,)\n",
      "Mask shape: (186, 45), Mask sum (total valid steps): 3986.0\n",
      "Building model...\n",
      "Model built status: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"improved_time_diff_lstm_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"improved_time_diff_lstm_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                               </span><span style=\"font-weight: bold\"> Output Shape                    </span><span style=\"font-weight: bold\">           Param # </span>\n",
       "\n",
       " InputNorm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " LSTM1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                                ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " ResidualDense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " AddResidual1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                          ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " Norm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                  ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " LSTM2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                                ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " MultiHeadAttention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)     ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " AddResidual2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                          ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " Norm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                  ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " TimeDiffHead (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " TotalTimeHead (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                              \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " InputNorm (\u001b[38;5;33mLayerNormalization\u001b[0m)              ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " LSTM1 (\u001b[38;5;33mLSTM\u001b[0m)                                ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " ResidualDense (\u001b[38;5;33mDense\u001b[0m)                       ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " AddResidual1 (\u001b[38;5;33mAdd\u001b[0m)                          ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " Norm1 (\u001b[38;5;33mLayerNormalization\u001b[0m)                  ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " LSTM2 (\u001b[38;5;33mLSTM\u001b[0m)                                ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " MultiHeadAttention (\u001b[38;5;33mMultiHeadAttention\u001b[0m)     ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " AddResidual2 (\u001b[38;5;33mAdd\u001b[0m)                          ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " Norm2 (\u001b[38;5;33mLayerNormalization\u001b[0m)                  ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " TimeDiffHead (\u001b[38;5;33mDense\u001b[0m)                        ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " TotalTimeHead (\u001b[38;5;33mDense\u001b[0m)                       ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining loss functions...\n",
      "Loss functions set. Weights: [0.4, 0.3, 0.3]\n",
      "Configuring optimizer...\n",
      "Compiling model...\n",
      "Model compiled successfully.\n",
      "Configuring callbacks...\n",
      "Starting training for 100 epochs with batch size 32...\n",
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 167ms/step - loss: 59.3927 - masked_huber_delta_10_loss: 49.7009 - total_time_mse_loss: 0.1403 - val_loss: 83.6995 - val_masked_huber_delta_10_loss: 81.8474 - val_total_time_mse_loss: 0.0023\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 53.1226 - masked_huber_delta_10_loss: 43.9613 - total_time_mse_loss: 0.0213 - val_loss: 80.5205 - val_masked_huber_delta_10_loss: 78.0529 - val_total_time_mse_loss: 0.0043\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 49.2959 - masked_huber_delta_10_loss: 40.2677 - total_time_mse_loss: 0.0185 - val_loss: 79.5217 - val_masked_huber_delta_10_loss: 76.7552 - val_total_time_mse_loss: 0.0068\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 47.8683 - masked_huber_delta_10_loss: 38.8354 - total_time_mse_loss: 0.0182 - val_loss: 78.9928 - val_masked_huber_delta_10_loss: 76.1036 - val_total_time_mse_loss: 0.0075\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 47.2588 - masked_huber_delta_10_loss: 38.2006 - total_time_mse_loss: 0.0156 - val_loss: 78.4691 - val_masked_huber_delta_10_loss: 75.5391 - val_total_time_mse_loss: 0.0067\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 46.6238 - masked_huber_delta_10_loss: 37.5846 - total_time_mse_loss: 0.0142 - val_loss: 77.7574 - val_masked_huber_delta_10_loss: 74.9534 - val_total_time_mse_loss: 0.0049\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 46.0475 - masked_huber_delta_10_loss: 37.0444 - total_time_mse_loss: 0.0124 - val_loss: 76.6446 - val_masked_huber_delta_10_loss: 74.1762 - val_total_time_mse_loss: 0.0028\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 45.0692 - masked_huber_delta_10_loss: 36.1230 - total_time_mse_loss: 0.0113 - val_loss: 75.7227 - val_masked_huber_delta_10_loss: 73.4389 - val_total_time_mse_loss: 0.0022\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 44.6748 - masked_huber_delta_10_loss: 35.7337 - total_time_mse_loss: 0.0126 - val_loss: 74.9995 - val_masked_huber_delta_10_loss: 72.7989 - val_total_time_mse_loss: 0.0022\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 44.1018 - masked_huber_delta_10_loss: 35.1564 - total_time_mse_loss: 0.0114 - val_loss: 74.5155 - val_masked_huber_delta_10_loss: 72.2076 - val_total_time_mse_loss: 0.0025\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 43.5197 - masked_huber_delta_10_loss: 34.5590 - total_time_mse_loss: 0.0107 - val_loss: 74.0264 - val_masked_huber_delta_10_loss: 71.5814 - val_total_time_mse_loss: 0.0030\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 43.2605 - masked_huber_delta_10_loss: 34.2683 - total_time_mse_loss: 0.0122 - val_loss: 73.6098 - val_masked_huber_delta_10_loss: 71.0776 - val_total_time_mse_loss: 0.0034\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 42.8235 - masked_huber_delta_10_loss: 33.8147 - total_time_mse_loss: 0.0116 - val_loss: 73.2743 - val_masked_huber_delta_10_loss: 70.7534 - val_total_time_mse_loss: 0.0035\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 42.5565 - masked_huber_delta_10_loss: 33.5216 - total_time_mse_loss: 0.0118 - val_loss: 73.0187 - val_masked_huber_delta_10_loss: 70.5509 - val_total_time_mse_loss: 0.0036\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 42.2648 - masked_huber_delta_10_loss: 33.2154 - total_time_mse_loss: 0.0119 - val_loss: 72.7061 - val_masked_huber_delta_10_loss: 70.1868 - val_total_time_mse_loss: 0.0037\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 42.0473 - masked_huber_delta_10_loss: 32.9730 - total_time_mse_loss: 0.0116 - val_loss: 72.3842 - val_masked_huber_delta_10_loss: 69.7879 - val_total_time_mse_loss: 0.0036\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 41.8297 - masked_huber_delta_10_loss: 32.7400 - total_time_mse_loss: 0.0088 - val_loss: 72.1022 - val_masked_huber_delta_10_loss: 69.4537 - val_total_time_mse_loss: 0.0032\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 41.6381 - masked_huber_delta_10_loss: 32.5286 - total_time_mse_loss: 0.0084 - val_loss: 71.8601 - val_masked_huber_delta_10_loss: 69.1892 - val_total_time_mse_loss: 0.0031\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 41.4725 - masked_huber_delta_10_loss: 32.3551 - total_time_mse_loss: 0.0083 - val_loss: 71.6195 - val_masked_huber_delta_10_loss: 68.9061 - val_total_time_mse_loss: 0.0030\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 41.3038 - masked_huber_delta_10_loss: 32.1569 - total_time_mse_loss: 0.0077 - val_loss: 71.4158 - val_masked_huber_delta_10_loss: 68.6850 - val_total_time_mse_loss: 0.0029\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 41.1370 - masked_huber_delta_10_loss: 31.9791 - total_time_mse_loss: 0.0078 - val_loss: 71.1958 - val_masked_huber_delta_10_loss: 68.4069 - val_total_time_mse_loss: 0.0026\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 41.1189 - masked_huber_delta_10_loss: 31.9482 - total_time_mse_loss: 0.0061 - val_loss: 71.0240 - val_masked_huber_delta_10_loss: 68.2359 - val_total_time_mse_loss: 0.0023\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.9279 - masked_huber_delta_10_loss: 31.7463 - total_time_mse_loss: 0.0057 - val_loss: 70.7832 - val_masked_huber_delta_10_loss: 67.8932 - val_total_time_mse_loss: 0.0020\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.8515 - masked_huber_delta_10_loss: 31.6492 - total_time_mse_loss: 0.0049 - val_loss: 70.5917 - val_masked_huber_delta_10_loss: 67.6422 - val_total_time_mse_loss: 0.0018\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.5900 - masked_huber_delta_10_loss: 31.4002 - total_time_mse_loss: 0.0054 - val_loss: 70.4584 - val_masked_huber_delta_10_loss: 67.5132 - val_total_time_mse_loss: 0.0018\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.7270 - masked_huber_delta_10_loss: 31.5293 - total_time_mse_loss: 0.0047 - val_loss: 70.3423 - val_masked_huber_delta_10_loss: 67.4450 - val_total_time_mse_loss: 0.0015\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.4680 - masked_huber_delta_10_loss: 31.2407 - total_time_mse_loss: 0.0035 - val_loss: 70.1587 - val_masked_huber_delta_10_loss: 67.1789 - val_total_time_mse_loss: 0.0013\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.4282 - masked_huber_delta_10_loss: 31.2010 - total_time_mse_loss: 0.0044 - val_loss: 69.9769 - val_masked_huber_delta_10_loss: 66.8431 - val_total_time_mse_loss: 0.0011\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.6932 - masked_huber_delta_10_loss: 31.4443 - total_time_mse_loss: 0.0037 - val_loss: 69.9392 - val_masked_huber_delta_10_loss: 66.8417 - val_total_time_mse_loss: 8.9351e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.5658 - masked_huber_delta_10_loss: 31.3102 - total_time_mse_loss: 0.0027 - val_loss: 69.9344 - val_masked_huber_delta_10_loss: 66.9440 - val_total_time_mse_loss: 8.1260e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 40.4566 - masked_huber_delta_10_loss: 31.1843 - total_time_mse_loss: 0.0029 - val_loss: 69.8758 - val_masked_huber_delta_10_loss: 66.8906 - val_total_time_mse_loss: 7.4108e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.3667 - masked_huber_delta_10_loss: 31.0770 - total_time_mse_loss: 0.0034 - val_loss: 69.7744 - val_masked_huber_delta_10_loss: 66.6987 - val_total_time_mse_loss: 6.9154e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.3380 - masked_huber_delta_10_loss: 31.0569 - total_time_mse_loss: 0.0024 - val_loss: 69.7238 - val_masked_huber_delta_10_loss: 66.6955 - val_total_time_mse_loss: 7.7346e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 40.4346 - masked_huber_delta_10_loss: 31.1371 - total_time_mse_loss: 0.0021 - val_loss: 69.6558 - val_masked_huber_delta_10_loss: 66.6390 - val_total_time_mse_loss: 7.6767e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.3075 - masked_huber_delta_10_loss: 31.0125 - total_time_mse_loss: 0.0023 - val_loss: 69.5355 - val_masked_huber_delta_10_loss: 66.4509 - val_total_time_mse_loss: 7.0149e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.3148 - masked_huber_delta_10_loss: 31.0050 - total_time_mse_loss: 0.0018 - val_loss: 69.4921 - val_masked_huber_delta_10_loss: 66.3968 - val_total_time_mse_loss: 6.1183e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.3266 - masked_huber_delta_10_loss: 31.0269 - total_time_mse_loss: 0.0020 - val_loss: 69.5026 - val_masked_huber_delta_10_loss: 66.4712 - val_total_time_mse_loss: 5.6182e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.2495 - masked_huber_delta_10_loss: 30.9302 - total_time_mse_loss: 0.0016 - val_loss: 69.4505 - val_masked_huber_delta_10_loss: 66.3575 - val_total_time_mse_loss: 5.0789e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40.3768 - masked_huber_delta_10_loss: 31.0387 - total_time_mse_loss: 0.0017 - val_loss: 69.3484 - val_masked_huber_delta_10_loss: 66.1276 - val_total_time_mse_loss: 4.6569e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 40.2666 - masked_huber_delta_10_loss: 30.9038 - total_time_mse_loss: 0.0020 - val_loss: 69.2619 - val_masked_huber_delta_10_loss: 65.9659 - val_total_time_mse_loss: 4.1015e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40.3236 - masked_huber_delta_10_loss: 30.9741 - total_time_mse_loss: 0.0013 - val_loss: 69.2415 - val_masked_huber_delta_10_loss: 66.0023 - val_total_time_mse_loss: 3.3885e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40.2639 - masked_huber_delta_10_loss: 30.9008 - total_time_mse_loss: 0.0010 - val_loss: 69.2510 - val_masked_huber_delta_10_loss: 66.0918 - val_total_time_mse_loss: 2.8541e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 40.2575 - masked_huber_delta_10_loss: 30.9253 - total_time_mse_loss: 9.3062e-04 - val_loss: 69.2311 - val_masked_huber_delta_10_loss: 66.1062 - val_total_time_mse_loss: 2.4285e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.1794 - masked_huber_delta_10_loss: 30.8114 - total_time_mse_loss: 7.3132e-04 - val_loss: 69.1539 - val_masked_huber_delta_10_loss: 65.9688 - val_total_time_mse_loss: 2.1241e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.0710 - masked_huber_delta_10_loss: 30.7498 - total_time_mse_loss: 7.2918e-04 - val_loss: 69.0711 - val_masked_huber_delta_10_loss: 65.8460 - val_total_time_mse_loss: 1.9204e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40.2110 - masked_huber_delta_10_loss: 30.8650 - total_time_mse_loss: 5.8886e-04 - val_loss: 68.9511 - val_masked_huber_delta_10_loss: 65.6277 - val_total_time_mse_loss: 1.6334e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 40.0872 - masked_huber_delta_10_loss: 30.7599 - total_time_mse_loss: 3.5382e-04 - val_loss: 68.8265 - val_masked_huber_delta_10_loss: 65.3444 - val_total_time_mse_loss: 1.4733e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.0618 - masked_huber_delta_10_loss: 30.7149 - total_time_mse_loss: 4.8489e-04 - val_loss: 68.7584 - val_masked_huber_delta_10_loss: 65.2673 - val_total_time_mse_loss: 1.2152e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.9083 - masked_huber_delta_10_loss: 30.5625 - total_time_mse_loss: 3.6195e-04 - val_loss: 68.7991 - val_masked_huber_delta_10_loss: 65.4097 - val_total_time_mse_loss: 1.0529e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.0607 - masked_huber_delta_10_loss: 30.7184 - total_time_mse_loss: 4.9937e-04 - val_loss: 68.8428 - val_masked_huber_delta_10_loss: 65.5462 - val_total_time_mse_loss: 8.9173e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40.0638 - masked_huber_delta_10_loss: 30.7299 - total_time_mse_loss: 3.4720e-04 - val_loss: 68.9077 - val_masked_huber_delta_10_loss: 65.7153 - val_total_time_mse_loss: 1.0334e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40.2295 - masked_huber_delta_10_loss: 30.8971 - total_time_mse_loss: 6.0779e-04 - val_loss: 68.8977 - val_masked_huber_delta_10_loss: 65.6783 - val_total_time_mse_loss: 1.6936e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40.1473 - masked_huber_delta_10_loss: 30.8091 - total_time_mse_loss: 6.3571e-04 - val_loss: 68.8655 - val_masked_huber_delta_10_loss: 65.5878 - val_total_time_mse_loss: 2.0970e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 40.1360 - masked_huber_delta_10_loss: 30.8036 - total_time_mse_loss: 6.1206e-04 - val_loss: 68.8939 - val_masked_huber_delta_10_loss: 65.6863 - val_total_time_mse_loss: 1.9068e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40.0801 - masked_huber_delta_10_loss: 30.7557 - total_time_mse_loss: 5.0449e-04 - val_loss: 68.8716 - val_masked_huber_delta_10_loss: 65.6778 - val_total_time_mse_loss: 1.4917e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39.8039 - masked_huber_delta_10_loss: 30.5167 - total_time_mse_loss: 4.3378e-04 - val_loss: 68.7247 - val_masked_huber_delta_10_loss: 65.3529 - val_total_time_mse_loss: 1.3475e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39.8010 - masked_huber_delta_10_loss: 30.4765 - total_time_mse_loss: 4.8192e-04 - val_loss: 68.5177 - val_masked_huber_delta_10_loss: 64.8349 - val_total_time_mse_loss: 2.3549e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.9679 - masked_huber_delta_10_loss: 30.6195 - total_time_mse_loss: 5.9967e-04 - val_loss: 68.5075 - val_masked_huber_delta_10_loss: 64.8640 - val_total_time_mse_loss: 2.3987e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.7624 - masked_huber_delta_10_loss: 30.4408 - total_time_mse_loss: 5.7307e-04 - val_loss: 68.5906 - val_masked_huber_delta_10_loss: 65.1014 - val_total_time_mse_loss: 2.3983e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 39.8133 - masked_huber_delta_10_loss: 30.5268 - total_time_mse_loss: 4.8891e-04 - val_loss: 68.6532 - val_masked_huber_delta_10_loss: 65.1846 - val_total_time_mse_loss: 2.5958e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.6342 - masked_huber_delta_10_loss: 30.3437 - total_time_mse_loss: 7.0622e-04 - val_loss: 68.6313 - val_masked_huber_delta_10_loss: 65.1157 - val_total_time_mse_loss: 2.7779e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.4644 - masked_huber_delta_10_loss: 30.1915 - total_time_mse_loss: 8.2571e-04 - val_loss: 68.5303 - val_masked_huber_delta_10_loss: 64.8649 - val_total_time_mse_loss: 3.0282e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39.5809 - masked_huber_delta_10_loss: 30.2740 - total_time_mse_loss: 9.8218e-04 - val_loss: 68.3592 - val_masked_huber_delta_10_loss: 64.5389 - val_total_time_mse_loss: 3.8730e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.5804 - masked_huber_delta_10_loss: 30.2963 - total_time_mse_loss: 9.7841e-04 - val_loss: 68.5145 - val_masked_huber_delta_10_loss: 64.8598 - val_total_time_mse_loss: 3.1522e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.5427 - masked_huber_delta_10_loss: 30.2767 - total_time_mse_loss: 9.2304e-04 - val_loss: 68.4474 - val_masked_huber_delta_10_loss: 64.8303 - val_total_time_mse_loss: 3.4664e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 39.4966 - masked_huber_delta_10_loss: 30.1854 - total_time_mse_loss: 0.0012 - val_loss: 68.4206 - val_masked_huber_delta_10_loss: 64.8368 - val_total_time_mse_loss: 3.6607e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39.9870 - masked_huber_delta_10_loss: 30.6936 - total_time_mse_loss: 8.1761e-04 - val_loss: 68.5240 - val_masked_huber_delta_10_loss: 65.1749 - val_total_time_mse_loss: 2.9408e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.7222 - masked_huber_delta_10_loss: 30.4382 - total_time_mse_loss: 6.6931e-04 - val_loss: 68.6781 - val_masked_huber_delta_10_loss: 65.4649 - val_total_time_mse_loss: 2.0595e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.4720 - masked_huber_delta_10_loss: 30.2575 - total_time_mse_loss: 4.0462e-04 - val_loss: 68.4076 - val_masked_huber_delta_10_loss: 64.9848 - val_total_time_mse_loss: 2.0030e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 39.2701 - masked_huber_delta_10_loss: 30.0246 - total_time_mse_loss: 4.5393e-04 - val_loss: 68.1889 - val_masked_huber_delta_10_loss: 64.5448 - val_total_time_mse_loss: 2.4700e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39.3663 - masked_huber_delta_10_loss: 30.1367 - total_time_mse_loss: 6.0198e-04 - val_loss: 68.0578 - val_masked_huber_delta_10_loss: 64.3138 - val_total_time_mse_loss: 3.2275e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39.3264 - masked_huber_delta_10_loss: 30.0886 - total_time_mse_loss: 6.5420e-04 - val_loss: 68.0792 - val_masked_huber_delta_10_loss: 64.4575 - val_total_time_mse_loss: 3.5275e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.1489 - masked_huber_delta_10_loss: 29.8797 - total_time_mse_loss: 9.7964e-04 - val_loss: 68.2062 - val_masked_huber_delta_10_loss: 64.7939 - val_total_time_mse_loss: 3.4785e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.8387 - masked_huber_delta_10_loss: 29.6780 - total_time_mse_loss: 7.8692e-04 - val_loss: 68.0550 - val_masked_huber_delta_10_loss: 64.5443 - val_total_time_mse_loss: 3.6970e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.0736 - masked_huber_delta_10_loss: 29.8358 - total_time_mse_loss: 8.2289e-04 - val_loss: 68.0611 - val_masked_huber_delta_10_loss: 64.4413 - val_total_time_mse_loss: 4.1490e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.4015 - masked_huber_delta_10_loss: 30.1137 - total_time_mse_loss: 9.0837e-04 - val_loss: 68.1027 - val_masked_huber_delta_10_loss: 64.7517 - val_total_time_mse_loss: 3.2873e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 39.2169 - masked_huber_delta_10_loss: 30.0328 - total_time_mse_loss: 6.3940e-04 - val_loss: 68.2320 - val_masked_huber_delta_10_loss: 64.9969 - val_total_time_mse_loss: 2.8758e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 38.6989 - masked_huber_delta_10_loss: 29.5339 - total_time_mse_loss: 6.9742e-04 - val_loss: 67.9075 - val_masked_huber_delta_10_loss: 64.3920 - val_total_time_mse_loss: 2.9666e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 38.9969 - masked_huber_delta_10_loss: 29.7483 - total_time_mse_loss: 6.5320e-04 - val_loss: 67.8502 - val_masked_huber_delta_10_loss: 64.2789 - val_total_time_mse_loss: 2.6773e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.9212 - masked_huber_delta_10_loss: 29.7708 - total_time_mse_loss: 6.5969e-04 - val_loss: 67.6407 - val_masked_huber_delta_10_loss: 63.9339 - val_total_time_mse_loss: 2.4477e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.7097 - masked_huber_delta_10_loss: 29.5578 - total_time_mse_loss: 4.6867e-04 - val_loss: 67.5512 - val_masked_huber_delta_10_loss: 63.7707 - val_total_time_mse_loss: 2.8767e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.7048 - masked_huber_delta_10_loss: 29.5117 - total_time_mse_loss: 6.9639e-04 - val_loss: 67.7265 - val_masked_huber_delta_10_loss: 64.3523 - val_total_time_mse_loss: 2.8057e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 38.3454 - masked_huber_delta_10_loss: 29.2758 - total_time_mse_loss: 4.8812e-04 - val_loss: 67.6923 - val_masked_huber_delta_10_loss: 64.3971 - val_total_time_mse_loss: 3.1864e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.7459 - masked_huber_delta_10_loss: 29.6228 - total_time_mse_loss: 8.4605e-04 - val_loss: 67.5735 - val_masked_huber_delta_10_loss: 64.2216 - val_total_time_mse_loss: 3.9148e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.8272 - masked_huber_delta_10_loss: 29.6807 - total_time_mse_loss: 7.1340e-04 - val_loss: 67.4565 - val_masked_huber_delta_10_loss: 64.0411 - val_total_time_mse_loss: 4.2268e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.5163 - masked_huber_delta_10_loss: 29.4239 - total_time_mse_loss: 9.7464e-04 - val_loss: 67.4714 - val_masked_huber_delta_10_loss: 63.9316 - val_total_time_mse_loss: 4.6467e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.2418 - masked_huber_delta_10_loss: 29.1937 - total_time_mse_loss: 9.1636e-04 - val_loss: 67.3747 - val_masked_huber_delta_10_loss: 63.8093 - val_total_time_mse_loss: 6.3040e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.1023 - masked_huber_delta_10_loss: 29.0432 - total_time_mse_loss: 0.0010 - val_loss: 67.0680 - val_masked_huber_delta_10_loss: 63.2514 - val_total_time_mse_loss: 8.1157e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 38.3451 - masked_huber_delta_10_loss: 29.2024 - total_time_mse_loss: 0.0011 - val_loss: 67.1159 - val_masked_huber_delta_10_loss: 63.2853 - val_total_time_mse_loss: 7.4087e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37.9258 - masked_huber_delta_10_loss: 28.8948 - total_time_mse_loss: 0.0013 - val_loss: 66.8503 - val_masked_huber_delta_10_loss: 62.9056 - val_total_time_mse_loss: 7.4585e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 38.1414 - masked_huber_delta_10_loss: 29.0599 - total_time_mse_loss: 0.0013 - val_loss: 66.7507 - val_masked_huber_delta_10_loss: 62.7920 - val_total_time_mse_loss: 6.8154e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 37.8233 - masked_huber_delta_10_loss: 28.8203 - total_time_mse_loss: 9.5831e-04 - val_loss: 66.9405 - val_masked_huber_delta_10_loss: 63.3098 - val_total_time_mse_loss: 5.5824e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 38.4248 - masked_huber_delta_10_loss: 29.4058 - total_time_mse_loss: 8.0623e-04 - val_loss: 66.5725 - val_masked_huber_delta_10_loss: 62.6458 - val_total_time_mse_loss: 5.4124e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37.6370 - masked_huber_delta_10_loss: 28.6483 - total_time_mse_loss: 7.6045e-04 - val_loss: 66.4171 - val_masked_huber_delta_10_loss: 62.4018 - val_total_time_mse_loss: 4.9791e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 38.3040 - masked_huber_delta_10_loss: 29.3474 - total_time_mse_loss: 9.0159e-04 - val_loss: 66.4763 - val_masked_huber_delta_10_loss: 62.6833 - val_total_time_mse_loss: 4.5851e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37.5912 - masked_huber_delta_10_loss: 28.6263 - total_time_mse_loss: 8.8711e-04 - val_loss: 66.2753 - val_masked_huber_delta_10_loss: 62.3273 - val_total_time_mse_loss: 4.8944e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37.4621 - masked_huber_delta_10_loss: 28.4972 - total_time_mse_loss: 9.1173e-04 - val_loss: 66.0588 - val_masked_huber_delta_10_loss: 62.0310 - val_total_time_mse_loss: 5.1387e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 37.5013 - masked_huber_delta_10_loss: 28.5449 - total_time_mse_loss: 7.9691e-04 - val_loss: 66.1756 - val_masked_huber_delta_10_loss: 62.2884 - val_total_time_mse_loss: 4.8526e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37.5708 - masked_huber_delta_10_loss: 28.5742 - total_time_mse_loss: 7.9161e-04 - val_loss: 66.3419 - val_masked_huber_delta_10_loss: 62.4993 - val_total_time_mse_loss: 5.1938e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 37.6191 - masked_huber_delta_10_loss: 28.6004 - total_time_mse_loss: 6.7685e-04 - val_loss: 66.1665 - val_masked_huber_delta_10_loss: 62.1300 - val_total_time_mse_loss: 6.1385e-04\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "Training finished.\n",
      "Final Training Loss: 36.3873\n",
      "Final Validation Loss: 66.1665\n",
      "------------------------------\n",
      "\n",
      "--- Step 2: Generating LSTM Predictions ---\n",
      "\n",
      "------------------------------\n",
      "Generating LSTM predictions...\n",
      "------------------------------\n",
      "Predicting on 186 sequences...\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "Model prediction successful.\n",
      "Predicted shapes: Diffs (186, 45), Total (186, 1), Cumul (186, 45)\n",
      "Processing predictions for 186 sequences...\n",
      "Finished processing predictions. Combined results shape: (3986, 13)\n",
      "Combined results head:\n",
      "    Sequence  Step SourceID  Predicted_Proportion  Predicted_Increment  \\\n",
      "0         0     1     10.0              0.049973            15.341682   \n",
      "1         0     2      0.0              0.050101            15.380990   \n",
      "2         0     3      4.0              0.051360            15.767509   \n",
      "3         0     4      5.0              0.049961            15.337996   \n",
      "4         0     5      5.0              0.049819            15.294573   \n",
      "\n",
      "   Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \\\n",
      "0             15.341682                    0.0                     0.0   \n",
      "1             30.722672                   40.0                    40.0   \n",
      "2             46.490180                    5.0                    45.0   \n",
      "3             61.828180                    7.0                    52.0   \n",
      "4             77.122750                   16.0                    68.0   \n",
      "\n",
      "   LSTM_Predicted_TimeDiff  LSTM_Predicted_Cumulative  \\\n",
      "0                 0.000000                   0.000000   \n",
      "1                18.214544                  18.214544   \n",
      "2                17.863008                  36.077553   \n",
      "3                18.075102                  54.152657   \n",
      "4                18.243958                  72.396614   \n",
      "\n",
      "   LSTM_Predicted_TotalTime  TimeDiff_Improvement_Abs  \\\n",
      "0                  0.051432                 15.341682   \n",
      "1                  0.051432                  2.833554   \n",
      "2                  0.051432                 -2.095499   \n",
      "3                  0.051432                 -2.737106   \n",
      "4                  0.051432                 -1.538531   \n",
      "\n",
      "   Cumulative_Improvement_Abs  \n",
      "0                   15.341682  \n",
      "1                  -12.508128  \n",
      "2                   -7.432267  \n",
      "3                    7.675523  \n",
      "4                    4.726136  \n",
      "Combined results tail:\n",
      "       Sequence  Step    SourceID  Predicted_Proportion  Predicted_Increment  \\\n",
      "3981       185    18         5.0              0.044701            55.562750   \n",
      "3982       185    19         6.0              0.044441            55.239826   \n",
      "3983       185    20         2.0              0.045120            56.083973   \n",
      "3984       185    21         9.0              0.045455            56.500710   \n",
      "3985       185    22  Unknown_21              0.045643            56.734410   \n",
      "\n",
      "      Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \\\n",
      "3981            1018.44104                    5.0                  1215.0   \n",
      "3982            1073.68090                    9.0                  1224.0   \n",
      "3983            1129.76490                   19.0                  1243.0   \n",
      "3984            1186.26560                    0.0                  1243.0   \n",
      "3985            1243.00000                -1243.0                     0.0   \n",
      "\n",
      "      LSTM_Predicted_TimeDiff  LSTM_Predicted_Cumulative  \\\n",
      "3981                      0.0                 219.834641   \n",
      "3982                      0.0                 219.834641   \n",
      "3983                      0.0                 219.834641   \n",
      "3984                      0.0                 219.834641   \n",
      "3985                      0.0                 219.834641   \n",
      "\n",
      "      LSTM_Predicted_TotalTime  TimeDiff_Improvement_Abs  \\\n",
      "3981                   0.03882                 45.562750   \n",
      "3982                   0.03882                 37.239826   \n",
      "3983                   0.03882                 18.083973   \n",
      "3984                   0.03882                 56.500710   \n",
      "3985                   0.03882                 56.734410   \n",
      "\n",
      "      Cumulative_Improvement_Abs  \n",
      "3981                 -798.606399  \n",
      "3982                 -853.846259  \n",
      "3983                 -909.930259  \n",
      "3984                 -966.430959  \n",
      "3985                 1023.165359  \n",
      "Checking for NaNs in LSTM predictions:\n",
      "LSTM_Predicted_TimeDiff      0\n",
      "LSTM_Predicted_Cumulative    0\n",
      "LSTM_Predicted_TotalTime     0\n",
      "dtype: int64\n",
      "Combined predictions successfully saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\predictions_lstm_refined_run2.csv\n",
      "\n",
      "------------------------------\n",
      "Overall Model Performance Comparison (Mean Absolute Error)\n",
      "------------------------------\n",
      "Calculating MAE based on 3986 valid rows (after dropping NaNs in comparison columns).\n",
      "Time Differences MAE:\n",
      "  Transformer: 38.9862\n",
      "  LSTM:        33.5007\n",
      "  Improvement: 14.07%\n",
      "\n",
      "Cumulative Times MAE:\n",
      "  Transformer: 80.9812\n",
      "  LSTM:        119.8682\n",
      "  Improvement: -48.02%\n",
      "------------------------------\n",
      "\n",
      "--- Step 3: Displaying Sample Results ---\n",
      "\n",
      "Sample Combined Predictions (Head):\n",
      "   Sequence  Step SourceID  GroundTruth_Increment  Predicted_Increment  LSTM_Predicted_TimeDiff  GroundTruth_Cumulative  Predicted_Cumulative  LSTM_Predicted_Cumulative  TimeDiff_Improvement_Abs  Cumulative_Improvement_Abs\n",
      "0         0     1     10.0                    0.0            15.341682                 0.000000                     0.0             15.341682                   0.000000                 15.341682                   15.341682\n",
      "1         0     2      0.0                   40.0            15.380990                18.214544                    40.0             30.722672                  18.214544                  2.833554                  -12.508128\n",
      "2         0     3      4.0                    5.0            15.767509                17.863008                    45.0             46.490180                  36.077553                 -2.095499                   -7.432267\n",
      "3         0     4      5.0                    7.0            15.337996                18.075102                    52.0             61.828180                  54.152657                 -2.737106                    7.675523\n",
      "4         0     5      5.0                   16.0            15.294573                18.243958                    68.0             77.122750                  72.396614                 -1.538531                    4.726136\n",
      "5         0     6      1.0                    9.0            15.310900                18.237688                    77.0             92.433650                  90.634300                 -2.926788                    1.799350\n",
      "6         0     7      1.0                    6.0            15.092329                18.239960                    83.0            107.525980                 108.874260                 -3.147631                   -1.348280\n",
      "7         0     8      4.0                  130.0            15.130347                18.213608                   213.0            122.656330                 127.087868                  3.083261                    4.431538\n",
      "8         0     9      5.0                    1.0            15.492865                18.036757                   214.0            138.149190                 145.124619                 -2.543892                    6.975429\n",
      "9         0    10      5.0                   10.0            15.434329                17.962812                   224.0            153.583510                 163.087433                 -2.528483                    9.503923\n",
      "\n",
      "--- Step 4: Generating Visualizations ---\n",
      "\n",
      "------------------------------\n",
      "Generating visualizations...\n",
      "------------------------------\n",
      "Attempting to save training performance plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\training_performance.png\n",
      "Successfully saved training performance plot.\n",
      "Attempting to plot prediction comparisons for 3 sample sequences: [113 164 169]\n",
      "\n",
      "Processing plot for Sequence ID: 113\n",
      "  Data shape for sequence 113: (25, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_113_comparison.png\n",
      "  Successfully saved comparison plot for sequence 113.\n",
      "\n",
      "Processing plot for Sequence ID: 164\n",
      "  Data shape for sequence 164: (23, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_164_comparison.png\n",
      "  Successfully saved comparison plot for sequence 164.\n",
      "\n",
      "Processing plot for Sequence ID: 169\n",
      "  Data shape for sequence 169: (21, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_169_comparison.png\n",
      "  Successfully saved comparison plot for sequence 169.\n",
      "------------------------------\n",
      "Visualization generation finished.\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      " Pipeline Execution Completed\n",
      "  - Combined predictions CSV saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\predictions_lstm_refined_1.csv\n",
      "  - Training performance plot saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\training_performance.png\n",
      "  - Sequence comparison plots saved as: sequence_<ID>_comparison.png\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Script Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    # import matplotlib\n",
    "    # matplotlib.use('Agg') # Uncomment if running in a non-GUI environment\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
