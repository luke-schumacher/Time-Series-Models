{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_data(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Load and validate transformer predictions CSV file\n",
    "    \n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Validated dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(transformer_predictions_file)\n",
    "        \n",
    "        # Basic validation checks\n",
    "        required_columns = ['Sequence', 'Step', 'Predicted_Proportion', \n",
    "                            'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Missing required column: {col}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Data loading error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_proportions(proportions, bin_size=0.05):\n",
    "    \"\"\"\n",
    "    Bin proportions and normalize\n",
    "    \n",
    "    Args:\n",
    "        proportions (array-like): Original proportion values\n",
    "        bin_size (float): Bin resolution\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Binned and normalized proportions\n",
    "    \"\"\"\n",
    "    binned_props = np.round(proportions / bin_size) * bin_size\n",
    "    total = np.sum(binned_props)\n",
    "    \n",
    "    if abs(total - 1.0) > 0.1:\n",
    "        binned_props = binned_props / total\n",
    "    \n",
    "    return binned_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transformer_predictions(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process transformer predictions for LSTM model\n",
    "    \n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to CSV file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Processed data for LSTM training\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = load_and_validate_data(transformer_predictions_file)\n",
    "    \n",
    "    # Extract unique sequences\n",
    "    sequences = df['Sequence'].unique()\n",
    "    \n",
    "    # Initialize lists for data storage\n",
    "    X_data = []        # Input features\n",
    "    y_increments = []  # Ground truth increments\n",
    "    y_cumulative = []  # Ground truth cumulative times\n",
    "    masks = []         # Masks for valid data\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Process each sequence\n",
    "    for seq in sequences:\n",
    "        seq_data = df[df['Sequence'] == seq].sort_values('Step')\n",
    "        \n",
    "        # Bin proportions\n",
    "        binned_proportions = bin_proportions(seq_data['Predicted_Proportion'].values)\n",
    "        \n",
    "        # Feature extraction with scaling\n",
    "        features = np.column_stack([\n",
    "            scaler.fit_transform(binned_proportions.reshape(-1, 1)).flatten(),\n",
    "            scaler.fit_transform(seq_data['GroundTruth_Increment'].values.reshape(-1, 1)).flatten(),\n",
    "            scaler.fit_transform(seq_data['GroundTruth_Cumulative'].values.reshape(-1, 1)).flatten(),\n",
    "            seq_data['Step'].values / max(seq_data['Step'].values)\n",
    "        ])\n",
    "        \n",
    "        # Ground truth values\n",
    "        increments = seq_data['GroundTruth_Increment'].values\n",
    "        cumulative = seq_data['GroundTruth_Cumulative'].values\n",
    "        \n",
    "        # Add to lists\n",
    "        X_data.append(features)\n",
    "        y_increments.append(increments)\n",
    "        y_cumulative.append(cumulative)\n",
    "        masks.append(np.ones(len(features)))\n",
    "    \n",
    "    # Pad sequences to uniform length\n",
    "    max_length = max(len(x) for x in X_data)\n",
    "    \n",
    "    X_padded = np.zeros((len(X_data), max_length, X_data[0].shape[1]))\n",
    "    y_increments_padded = np.zeros((len(y_increments), max_length))\n",
    "    y_cumulative_padded = np.zeros((len(y_cumulative), max_length))\n",
    "    masks_padded = np.zeros((len(masks), max_length))\n",
    "    \n",
    "    for i in range(len(X_data)):\n",
    "        seq_len = len(X_data[i])\n",
    "        X_padded[i, :seq_len, :] = X_data[i]\n",
    "        y_increments_padded[i, :seq_len] = y_increments[i]\n",
    "        y_cumulative_padded[i, :seq_len] = y_cumulative[i]\n",
    "        masks_padded[i, :seq_len] = masks[i]\n",
    "    \n",
    "    # Extract total times\n",
    "    y_total_times = np.array([seq[-1] if len(seq) > 0 else 0 for seq in y_cumulative])\n",
    "    \n",
    "    return {\n",
    "        'X': X_padded,\n",
    "        'y_increments': y_increments_padded,\n",
    "        'y_cumulative': y_cumulative_padded,\n",
    "        'y_total_times': y_total_times,\n",
    "        'masks': masks_padded,\n",
    "        'sequences': sequences,\n",
    "        'df': df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_masked_huber_loss(mask, delta=1.0):\n",
    "    \"\"\"\n",
    "    Custom masked Huber loss function\n",
    "    \n",
    "    Args:\n",
    "        mask: Tensor mask for valid tokens\n",
    "        delta: Huber loss threshold\n",
    "    \n",
    "    Returns:\n",
    "        Callable loss function\n",
    "    \"\"\"\n",
    "    def masked_huber(y_true, y_pred):\n",
    "        mask_float = tf.cast(mask, tf.float32)\n",
    "        current_batch_size = tf.shape(y_true)[0]\n",
    "        mask_batch = mask_float[:current_batch_size]\n",
    "        \n",
    "        # Compute Huber loss\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) <= delta\n",
    "        squared_loss = 0.5 * tf.square(error)\n",
    "        linear_loss = delta * (tf.abs(error) - 0.5 * delta)\n",
    "        \n",
    "        huber_loss = tf.where(is_small_error, squared_loss, linear_loss)\n",
    "        masked_loss = huber_loss * mask_batch\n",
    "        \n",
    "        valid_tokens = tf.reduce_sum(mask_batch, axis=1, keepdims=True)\n",
    "        valid_tokens = tf.where(valid_tokens == 0, tf.ones_like(valid_tokens), valid_tokens)\n",
    "        \n",
    "        return tf.reduce_mean(tf.reduce_sum(masked_loss, axis=1) / valid_tokens)\n",
    "    \n",
    "    return masked_huber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTimeDiffLSTM(tf.keras.Model):\n",
    "    def __init__(self, hidden_units=64, dropout_rate=0.3):\n",
    "        super(ImprovedTimeDiffLSTM, self).__init__()\n",
    "        \n",
    "        # Input normalization\n",
    "        self.input_normalization = layers.LayerNormalization()\n",
    "        \n",
    "        # LSTM layers with regularization and residual connections\n",
    "        self.lstm_layer1 = layers.LSTM(\n",
    "            hidden_units, \n",
    "            return_sequences=True, \n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate,\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "        )\n",
    "        \n",
    "        self.lstm_layer2 = layers.LSTM(\n",
    "            hidden_units // 2, \n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=4, \n",
    "            key_dim=hidden_units,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Output layers with non-negativity constraints\n",
    "        self.time_diff_head = layers.Dense(1, activation='relu')\n",
    "        self.total_time_head = layers.Dense(1, activation='softplus')\n",
    "        \n",
    "        # Residual connection dense layer\n",
    "        self.residual_dense = layers.Dense(hidden_units, activation='relu')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Normalize inputs\n",
    "        x = self.input_normalization(inputs)\n",
    "        \n",
    "        # First LSTM layer with residual connection\n",
    "        lstm_out1 = self.lstm_layer1(x)\n",
    "        residual = self.residual_dense(x)\n",
    "        lstm_out1 += residual\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        lstm_out2 = self.lstm_layer2(lstm_out1)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_out = self.attention(lstm_out2, lstm_out2, lstm_out2)\n",
    "        \n",
    "        # Time difference prediction\n",
    "        time_diffs = self.time_diff_head(attn_out)\n",
    "        time_diffs = tf.squeeze(time_diffs, axis=-1)\n",
    "        \n",
    "        # Total time prediction\n",
    "        sequence_encoding = tf.reduce_mean(attn_out, axis=1)\n",
    "        total_time = self.total_time_head(sequence_encoding)\n",
    "        \n",
    "        # Cumulative times calculation\n",
    "        cumulative_times = tf.cumsum(time_diffs, axis=1)\n",
    "        \n",
    "        return time_diffs, total_time, cumulative_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lstm_predictions_from_transformer_csv(lstm_model, data):\n",
    "    \"\"\"\n",
    "    Generate predictions using the LSTM model trained on transformer predictions\n",
    "    \"\"\"\n",
    "    # Get LSTM predictions\n",
    "    time_diffs_pred, total_time_pred, cumulative_times_pred = lstm_model(data['X'])\n",
    "    \n",
    "    # Convert to numpy for further processing\n",
    "    time_diffs_pred = time_diffs_pred.numpy()\n",
    "    total_time_pred = total_time_pred.numpy()\n",
    "    cumulative_times_pred = cumulative_times_pred.numpy()\n",
    "    \n",
    "    # Create results dataframe with both transformer and LSTM predictions\n",
    "    results_list = []\n",
    "    \n",
    "    # Original dataframe with transformer predictions\n",
    "    original_df = data['df']\n",
    "    \n",
    "    # Process each sequence\n",
    "    for seq_idx, seq in enumerate(data['sequences']):\n",
    "        seq_data = original_df[original_df['Sequence'] == seq].sort_values('Step')\n",
    "        \n",
    "        # Get number of steps in this sequence\n",
    "        seq_steps = len(seq_data)\n",
    "        \n",
    "        # Add LSTM predictions\n",
    "        seq_data = seq_data.copy()\n",
    "        seq_data['LSTM_Predicted_TimeDiff'] = time_diffs_pred[seq_idx, :seq_steps]\n",
    "        seq_data['LSTM_Predicted_Cumulative'] = cumulative_times_pred[seq_idx, :seq_steps]\n",
    "        seq_data['LSTM_Predicted_TotalTime'] = total_time_pred[seq_idx, 0]\n",
    "        \n",
    "        # Calculate improvement metrics\n",
    "        seq_data['TimeDiff_Improvement_Pct'] = (\n",
    "            (abs(seq_data['GroundTruth_Increment'] - seq_data['Predicted_Increment']) - \n",
    "             abs(seq_data['GroundTruth_Increment'] - seq_data['LSTM_Predicted_TimeDiff'])) / \n",
    "             abs(seq_data['GroundTruth_Increment'] - seq_data['Predicted_Increment']) * 100\n",
    "        ).fillna(0)\n",
    "        \n",
    "        seq_data['Cumulative_Improvement_Pct'] = (\n",
    "            (abs(seq_data['GroundTruth_Cumulative'] - seq_data['Predicted_Cumulative']) - \n",
    "             abs(seq_data['GroundTruth_Cumulative'] - seq_data['LSTM_Predicted_Cumulative'])) / \n",
    "             abs(seq_data['GroundTruth_Cumulative'] - seq_data['Predicted_Cumulative']) * 100\n",
    "        ).fillna(0)\n",
    "        \n",
    "        results_list.append(seq_data)\n",
    "    \n",
    "    # Combine all results\n",
    "    results_df = pd.concat(results_list, ignore_index=True)\n",
    "    results_df.to_csv('predictions_lstm_refined.csv', index=False)\n",
    "    print(\"Combined predictions saved to predictions_lstm_refined.csv\")\n",
    "    \n",
    "    # Calculate overall improvement statistics\n",
    "    transformer_time_diff_mae = np.mean(abs(results_df['GroundTruth_Increment'] - results_df['Predicted_Increment']))\n",
    "    lstm_time_diff_mae = np.mean(abs(results_df['GroundTruth_Increment'] - results_df['LSTM_Predicted_TimeDiff']))\n",
    "    \n",
    "    transformer_cumulative_mae = np.mean(abs(results_df['GroundTruth_Cumulative'] - results_df['Predicted_Cumulative']))\n",
    "    lstm_cumulative_mae = np.mean(abs(results_df['GroundTruth_Cumulative'] - results_df['LSTM_Predicted_Cumulative']))\n",
    "    \n",
    "    print(\"\\nModel Performance Comparison (Mean Absolute Error)\")\n",
    "    print(f\"Time Differences: Transformer {transformer_time_diff_mae:.4f}, LSTM {lstm_time_diff_mae:.4f}, \" \n",
    "          f\"Improvement: {(1 - lstm_time_diff_mae/transformer_time_diff_mae)*100:.2f}%\")\n",
    "    \n",
    "    print(f\"Cumulative Times: Transformer {transformer_cumulative_mae:.4f}, LSTM {lstm_cumulative_mae:.4f}, \"\n",
    "          f\"Improvement: {(1 - lstm_cumulative_mae/transformer_cumulative_mae)*100:.2f}%\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Train the improved LSTM model\n",
    "    \n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to transformer predictions CSV\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Trained model, training history, processed data\n",
    "    \"\"\"\n",
    "    # Process data\n",
    "    data = process_transformer_predictions(transformer_predictions_file)\n",
    "    \n",
    "    # Create improved model\n",
    "    lstm_model = ImprovedTimeDiffLSTM()\n",
    "    \n",
    "    # Robust Huber loss function\n",
    "    masked_huber_loss = custom_masked_huber_loss(data['masks'])\n",
    "    \n",
    "    # Advanced optimizer with learning rate decay\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=0.001,\n",
    "            decay_steps=100,\n",
    "            decay_rate=0.96\n",
    "        ),\n",
    "        weight_decay=0.001\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    lstm_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=[masked_huber_loss, 'mse', masked_huber_loss],\n",
    "        loss_weights=[0.4, 0.3, 0.3]\n",
    "    )\n",
    "    \n",
    "    # Advanced callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train with advanced strategy\n",
    "    history = lstm_model.fit(\n",
    "        data['X'],\n",
    "        [data['y_increments'], data['y_total_times'], data['y_cumulative']],\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return lstm_model, history, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM model using transformer predictions and visualize results\n",
    "    \n",
    "    Enhanced with robust error handling and NaN filtering\n",
    "    \"\"\"\n",
    "    try:\n",
    "        transformer_predictions_file = \"predictions_transformer.csv\"\n",
    "        \n",
    "        # Train LSTM model using transformer predictions\n",
    "        print(\"Training LSTM model using transformer predictions...\")\n",
    "        lstm_model, history, data = train_lstm_from_transformer_predictions(\n",
    "            transformer_predictions_file, epochs=50, batch_size=32\n",
    "        )\n",
    "        \n",
    "        # Generate and save predictions\n",
    "        print(\"Generating LSTM predictions...\")\n",
    "        results_df = generate_lstm_predictions_from_transformer_csv(lstm_model, data)\n",
    "        \n",
    "        # Display sample results\n",
    "        print(\"\\nSample Combined Predictions:\")\n",
    "        display_cols = ['Sequence', 'Step', 'SourceID', 'Predicted_Increment', \n",
    "                         'LSTM_Predicted_TimeDiff', 'GroundTruth_Increment', \n",
    "                         'TimeDiff_Improvement_Pct']\n",
    "        print(results_df[display_cols].head(10))\n",
    "        \n",
    "        # Generate visualizations\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        visualize_results(results_df, data, lstm_model)\n",
    "        print(\"Visualization completed. Check the generated PNG files.\")\n",
    "        \n",
    "        # Plot training history\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss During Training')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Robust error calculation and filtering\n",
    "        def filter_valid_errors(predictions, ground_truth):\n",
    "            \"\"\"\n",
    "            Filter out NaN and infinite values from predictions and ground truth\n",
    "            \n",
    "            Args:\n",
    "                predictions (np.ndarray): Model predictions\n",
    "                ground_truth (np.ndarray): Ground truth values\n",
    "            \n",
    "            Returns:\n",
    "                np.ndarray: Mean absolute errors for valid predictions\n",
    "            \"\"\"\n",
    "            # Ensure predictions and ground truth are numpy arrays\n",
    "            predictions = np.asarray(predictions)\n",
    "            ground_truth = np.asarray(ground_truth)\n",
    "            \n",
    "            # Calculate absolute errors\n",
    "            abs_errors = np.abs(predictions - ground_truth)\n",
    "            \n",
    "            # Filter out NaN and infinite values\n",
    "            valid_mask = np.isfinite(abs_errors)\n",
    "            filtered_errors = abs_errors[valid_mask]\n",
    "            \n",
    "            return filtered_errors\n",
    "        \n",
    "        # Predict on training and validation data\n",
    "        try:\n",
    "            # Ensure only using training portion of the data\n",
    "            train_size = int(0.8 * len(data['X']))\n",
    "            \n",
    "            # Predict and calculate errors for training data\n",
    "            train_pred = lstm_model.predict(data['X'][:train_size])\n",
    "            train_errors = filter_valid_errors(train_pred[0], data['y_increments'][:train_size])\n",
    "            \n",
    "            # Predict and calculate errors for validation data\n",
    "            val_pred = lstm_model.predict(data['X'][train_size:])\n",
    "            val_errors = filter_valid_errors(val_pred[0], data['y_increments'][train_size:])\n",
    "            \n",
    "            # Plot error distribution\n",
    "            plt.subplot(1, 2, 2)\n",
    "            \n",
    "            # Only plot if we have valid errors\n",
    "            if len(train_errors) > 0 and len(val_errors) > 0:\n",
    "                plt.hist(train_errors, bins=30, alpha=0.5, label='Training Errors')\n",
    "                plt.hist(val_errors, bins=30, alpha=0.5, label='Validation Errors')\n",
    "                plt.title('Error Distribution')\n",
    "                plt.xlabel('Mean Absolute Error')\n",
    "                plt.ylabel('Frequency')\n",
    "                plt.legend()\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No valid errors to display', \n",
    "                         horizontalalignment='center', \n",
    "                         verticalalignment='center')\n",
    "                plt.title('Error Distribution')\n",
    "        \n",
    "        except Exception as pred_error:\n",
    "            print(f\"Error in prediction or error calculation: {pred_error}\")\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.text(0.5, 0.5, f'Error in prediction: {pred_error}', \n",
    "                     horizontalalignment='center', \n",
    "                     verticalalignment='center')\n",
    "            plt.title('Error Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_performance.png')\n",
    "        print(\"Saved training performance visualization\")\n",
    "        \n",
    "        return lstm_model, results_df, history, data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_improved_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Train the improved LSTM model with a more flexible learning rate approach\n",
    "    \n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to transformer predictions CSV\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Trained model, training history, processed data\n",
    "    \"\"\"\n",
    "    # Enhanced data processing\n",
    "    data = process_transformer_predictions(transformer_predictions_file)\n",
    "    \n",
    "    # Create improved model\n",
    "    lstm_model = ImprovedTimeDiffLSTM()\n",
    "    \n",
    "    # Robust loss functions\n",
    "    masked_huber_loss = custom_masked_huber_loss(data['masks'])\n",
    "    \n",
    "    # Create a learning rate schedule\n",
    "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.001,  # Starting learning rate\n",
    "        decay_steps=100,               # Decay every 100 steps\n",
    "        decay_rate=0.96                # Reduce to 96% of previous rate\n",
    "    )\n",
    "    \n",
    "    # Advanced optimizer with dynamic learning rate\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate_schedule,  # Use the schedule directly\n",
    "        weight_decay=0.001\n",
    "    )\n",
    "    \n",
    "    # Compile model with robust loss functions\n",
    "    lstm_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=[masked_huber_loss, 'mse', masked_huber_loss],\n",
    "        loss_weights=[0.4, 0.3, 0.3]\n",
    "    )\n",
    "    \n",
    "    # Advanced callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train with advanced strategy\n",
    "    history = lstm_model.fit(\n",
    "        data['X'],\n",
    "        [data['y_increments'], data['y_total_times'], data['y_cumulative']],\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return lstm_model, history, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train and demonstrate the LSTM model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Replace with your actual transformer predictions file path\n",
    "        transformer_predictions_file = \"predictions_transformer.csv\"\n",
    "        \n",
    "        # Train improved LSTM model\n",
    "        lstm_model, history, data = train_improved_lstm(transformer_predictions_file)\n",
    "        \n",
    "        # Optional: Add visualization or further analysis here\n",
    "        print(\"Model training completed successfully!\")\n",
    "        \n",
    "        return lstm_model, history, data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_37224\\891154112.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  binned_props = binned_props / total\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - loss: nan - masked_huber_loss: nan - mse_loss: nan - val_loss: nan - val_masked_huber_loss: nan - val_mse_loss: nan - learning_rate: 9.9796e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: nan - masked_huber_loss: nan - mse_loss: nan - val_loss: nan - val_masked_huber_loss: nan - val_mse_loss: nan - learning_rate: 9.9593e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: nan - masked_huber_loss: nan - mse_loss: nan - val_loss: nan - val_masked_huber_loss: nan - val_mse_loss: nan - learning_rate: 9.9390e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: nan - masked_huber_loss: nan - mse_loss: nan - val_loss: nan - val_masked_huber_loss: nan - val_mse_loss: nan - learning_rate: 9.9187e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: nan - masked_huber_loss: nan - mse_loss: nanError in main function: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_37224\\830203811.py\", line 10, in main\n",
      "    lstm_model, history, data = train_improved_lstm(transformer_predictions_file)\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_37224\\2435613863.py\", line 55, in train_improved_lstm\n",
      "    history = lstm_model.fit(\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 597, in learning_rate\n",
      "    raise TypeError(\n",
      "TypeError: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
