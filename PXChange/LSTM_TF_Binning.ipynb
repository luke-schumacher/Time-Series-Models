{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import AdamW # Using AdamW for weight decay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os # Added for file path handling\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Validation\n",
    "def load_and_validate_data(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Load and validate transformer predictions CSV file.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Validated dataframe.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file does not exist.\n",
    "        ValueError: If required columns are missing or have wrong types.\n",
    "        Exception: For other potential loading errors.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Error: Input file not found at {transformer_predictions_file}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(transformer_predictions_file)\n",
    "        print(f\"Successfully loaded CSV. Shape: {df.shape}\")\n",
    "        print(f\"Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "        # Basic validation checks\n",
    "        # Ensure these columns exist based on subsequent usage\n",
    "        required_columns = ['Sequence', 'Step', 'SourceID', 'Predicted_Proportion',\n",
    "                            'Predicted_Increment', 'Predicted_Cumulative',\n",
    "                            'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
    "\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in input CSV: {', '.join(missing_cols)}\")\n",
    "        print(\"All required columns are present.\")\n",
    "\n",
    "        # Check for essential numeric types\n",
    "        numeric_cols_to_check = ['Step', 'Predicted_Proportion', 'Predicted_Increment',\n",
    "                                 'Predicted_Cumulative', 'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
    "        for col in numeric_cols_to_check:\n",
    "             # Attempt to convert to numeric, coercing errors to NaN\n",
    "             df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "             if df[col].isnull().any():\n",
    "                 print(f\"Warning: Column '{col}' contains non-numeric values that were converted to NaN.\")\n",
    "                 # Optionally raise ValueError if NaNs are unacceptable here\n",
    "                 # raise ValueError(f\"Column '{col}' must be entirely numeric.\")\n",
    "\n",
    "        print(\"Data validation checks passed.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading error: {e}\")\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion Binning\n",
    "def bin_proportions(proportions, bin_size=0.05):\n",
    "    \"\"\"\n",
    "    Bin proportions and normalize if the sum deviates significantly from 1.0.\n",
    "\n",
    "    Args:\n",
    "        proportions (array-like): Original proportion values.\n",
    "        bin_size (float): Bin resolution.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binned and potentially normalized proportions.\n",
    "    \"\"\"\n",
    "    # Ensure input is a numpy array for calculations\n",
    "    proportions = np.asarray(proportions)\n",
    "    \n",
    "    # Avoid division by zero if bin_size is zero\n",
    "    if bin_size <= 0:\n",
    "        raise ValueError(\"bin_size must be positive.\")\n",
    "        \n",
    "    binned_props = np.round(proportions / bin_size) * bin_size\n",
    "    total = np.sum(binned_props)\n",
    "\n",
    "    # Normalize only if the sum is significantly different from 1.0\n",
    "    # Use a small tolerance (epsilon) for floating point comparison\n",
    "    epsilon = 1e-6\n",
    "    if abs(total - 1.0) > 0.1 and abs(total) > epsilon: # Avoid division by near-zero total\n",
    "        print(f\"Warning: Sum of binned proportions ({total:.4f}) deviates > 0.1 from 1.0. Normalizing.\")\n",
    "        binned_props = binned_props / total\n",
    "    elif abs(total) <= epsilon and len(binned_props) > 0:\n",
    "         print(\"Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\")\n",
    "         # Return original binned props or zeros, depending on desired behavior\n",
    "         # Returning zeros might be safer if normalization is expected\n",
    "         # return np.zeros_like(binned_props)\n",
    "\n",
    "    return binned_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing for LSTM (Minor logging added)\n",
    "def process_transformer_predictions(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process transformer predictions for LSTM model input.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed data dictionary containing padded sequences, masks, etc.\n",
    "    \"\"\"\n",
    "    df = load_and_validate_data(transformer_predictions_file)\n",
    "    sequences = df['Sequence'].unique()\n",
    "    print(f\"Found {len(sequences)} unique sequences in the data.\")\n",
    "\n",
    "    X_data, y_increments, y_cumulative, masks = [], [], [], []\n",
    "    prop_scaler, inc_scaler, cum_scaler = StandardScaler(), StandardScaler(), StandardScaler()\n",
    "    processed_sequence_ids = [] # Keep track of sequences successfully processed\n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_data = df[df['Sequence'] == seq_id].sort_values('Step')\n",
    "        seq_len = len(seq_data)\n",
    "        if seq_len == 0:\n",
    "            print(f\"Warning: Sequence {seq_id} has no data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            binned_proportions = bin_proportions(seq_data['Predicted_Proportion'].values)\n",
    "            scaled_props = prop_scaler.fit_transform(binned_proportions.reshape(-1, 1)).flatten()\n",
    "            # Handle potential NaNs introduced by coerce during loading before scaling\n",
    "            gt_inc_valid = seq_data['GroundTruth_Increment'].dropna()\n",
    "            gt_cum_valid = seq_data['GroundTruth_Cumulative'].dropna()\n",
    "            if gt_inc_valid.empty or gt_cum_valid.empty:\n",
    "                 print(f\"Warning: Sequence {seq_id} has only NaN ground truth values after coercion. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            scaled_gt_inc = inc_scaler.fit_transform(gt_inc_valid.values.reshape(-1, 1)).flatten()\n",
    "            scaled_gt_cum = cum_scaler.fit_transform(gt_cum_valid.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Ensure lengths match after potential dropna (this indicates an issue if they don't)\n",
    "            if len(scaled_gt_inc) != seq_len or len(scaled_gt_cum) != seq_len:\n",
    "                 print(f\"Warning: Length mismatch after handling NaNs in sequence {seq_id}. Check input data. Skipping.\")\n",
    "                 # This case needs careful handling - maybe impute NaNs instead of dropping?\n",
    "                 # For now, skipping the sequence if lengths don't match after dropna.\n",
    "                 continue\n",
    "\n",
    "\n",
    "            max_step = seq_data['Step'].max()\n",
    "            normalized_step = seq_data['Step'].values / max_step if max_step > 0 else np.zeros(seq_len)\n",
    "\n",
    "            features = np.column_stack([scaled_props, scaled_gt_inc, scaled_gt_cum, normalized_step])\n",
    "            increments = seq_data['GroundTruth_Increment'].values # Use original values with potential NaNs for target\n",
    "            cumulative = seq_data['GroundTruth_Cumulative'].values # Use original values with potential NaNs for target\n",
    "\n",
    "            X_data.append(features)\n",
    "            y_increments.append(increments)\n",
    "            y_cumulative.append(cumulative)\n",
    "            masks.append(np.ones(seq_len))\n",
    "            processed_sequence_ids.append(seq_id) # Add ID if processed successfully\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence {seq_id}: {e}. Skipping sequence.\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    if not X_data:\n",
    "         raise ValueError(\"No valid sequences found after processing. Cannot proceed.\")\n",
    "\n",
    "    num_processed_sequences = len(X_data)\n",
    "    print(f\"Successfully processed {num_processed_sequences} sequences.\")\n",
    "\n",
    "    max_length = max(len(x) for x in X_data)\n",
    "    num_features = X_data[0].shape[1]\n",
    "    print(f\"Padding sequences to max length: {max_length}\")\n",
    "\n",
    "    # Initialize padded arrays - use np.nan as fill value for targets might be better\n",
    "    X_padded = np.zeros((num_processed_sequences, max_length, num_features), dtype=np.float32)\n",
    "    y_increments_padded = np.full((num_processed_sequences, max_length), np.nan, dtype=np.float32)\n",
    "    y_cumulative_padded = np.full((num_processed_sequences, max_length), np.nan, dtype=np.float32)\n",
    "    masks_padded = np.zeros((num_processed_sequences, max_length), dtype=np.float32)\n",
    "\n",
    "    for i in range(num_processed_sequences):\n",
    "        seq_len = len(X_data[i])\n",
    "        X_padded[i, :seq_len, :] = X_data[i]\n",
    "        y_increments_padded[i, :seq_len] = y_increments[i]\n",
    "        y_cumulative_padded[i, :seq_len] = y_cumulative[i]\n",
    "        # Mask should be 0 where target is NaN, 1 otherwise\n",
    "        masks_padded[i, :seq_len] = (~np.isnan(y_increments[i])).astype(np.float32) # Example: mask based on increment NaNs\n",
    "\n",
    "    # Recalculate total times based on the potentially NaN-filled padded ground truth\n",
    "    y_total_times = []\n",
    "    for i in range(num_processed_sequences):\n",
    "        valid_indices = np.where(masks_padded[i] == 1)[0]\n",
    "        if len(valid_indices) > 0:\n",
    "            last_valid_index = valid_indices[-1]\n",
    "            y_total_times.append(y_cumulative_padded[i, last_valid_index])\n",
    "        else:\n",
    "            y_total_times.append(0) # Or np.nan if preferred\n",
    "    y_total_times = np.array(y_total_times, dtype=np.float32)\n",
    "\n",
    "    # Replace NaNs in target arrays with 0 for training (loss function needs numbers)\n",
    "    # The mask will handle ignoring these steps during loss calculation.\n",
    "    y_increments_padded = np.nan_to_num(y_increments_padded, nan=0.0)\n",
    "    y_cumulative_padded = np.nan_to_num(y_cumulative_padded, nan=0.0)\n",
    "    # y_total_times should already be numeric based on calculation above\n",
    "\n",
    "    print(\"Data processing and padding complete.\")\n",
    "    return {\n",
    "        'X': X_padded,\n",
    "        'y_increments': y_increments_padded,\n",
    "        'y_cumulative': y_cumulative_padded,\n",
    "        'y_total_times': y_total_times,\n",
    "        'masks': masks_padded, # Crucial: Mask reflects original NaNs\n",
    "        'sequences': processed_sequence_ids, # Use the list of successfully processed IDs\n",
    "        'df': df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Masked Loss Function (Updated mask handling logic)\n",
    "def custom_masked_huber_loss(mask_tensor, delta=1.0):\n",
    "    \"\"\"\n",
    "    Factory function to create a masked Huber loss.\n",
    "    Mask should be 1 for valid steps, 0 for padded/invalid steps.\n",
    "\n",
    "    Args:\n",
    "        mask_tensor (tf.Tensor): The mask indicating valid time steps (batch_size, seq_len).\n",
    "                                 Passed during training setup.\n",
    "        delta (float): Huber loss delta parameter.\n",
    "\n",
    "    Returns:\n",
    "        Callable: A loss function `masked_huber(y_true, y_pred)`.\n",
    "    \"\"\"\n",
    "    # Convert the persistent mask tensor to float32 once\n",
    "    mask_float32 = tf.cast(mask_tensor, tf.float32)\n",
    "\n",
    "    def masked_huber(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate Huber loss only for valid (masked == 1) time steps.\n",
    "\n",
    "        Args:\n",
    "            y_true (tf.Tensor): Ground truth tensor (batch_size, seq_len).\n",
    "            y_pred (tf.Tensor): Prediction tensor (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Scalar mean loss over the batch.\n",
    "        \"\"\"\n",
    "        # Slice the persistent mask for the current batch size dynamically\n",
    "        current_batch_size = tf.shape(y_true)[0]\n",
    "        # Assume mask_tensor corresponds row-wise to the full dataset\n",
    "        mask_batch = mask_float32[:current_batch_size]\n",
    "\n",
    "        # Ensure mask_batch has compatible shape (e.g., (batch_size, seq_len))\n",
    "        # This might require reshaping or broadcasting depending on exact shapes\n",
    "        # Assuming y_true, y_pred, mask_batch are all (batch_size, seq_len)\n",
    "\n",
    "        # Huber loss calculation\n",
    "        error = y_true - y_pred\n",
    "        abs_error = tf.abs(error)\n",
    "        quadratic = tf.minimum(abs_error, delta)\n",
    "        linear = abs_error - quadratic\n",
    "        huber_loss = 0.5 * quadratic**2 + delta * linear\n",
    "\n",
    "        # Apply the mask (element-wise multiplication)\n",
    "        masked_loss = huber_loss * mask_batch\n",
    "\n",
    "        # Calculate mean loss per sequence, avoiding division by zero\n",
    "        # Sum loss over sequence length dimension\n",
    "        sum_loss_per_sequence = tf.reduce_sum(masked_loss, axis=1)\n",
    "        # Sum mask over sequence length dimension to get count of valid tokens\n",
    "        valid_tokens_per_sequence = tf.reduce_sum(mask_batch, axis=1)\n",
    "\n",
    "        # Avoid division by zero for sequences with no valid tokens\n",
    "        # Replace zero counts with 1; the corresponding sum_loss will also be 0.\n",
    "        valid_tokens_safe = tf.where(valid_tokens_per_sequence == 0,\n",
    "                                     tf.ones_like(valid_tokens_per_sequence),\n",
    "                                     valid_tokens_per_sequence)\n",
    "\n",
    "        mean_loss_per_sequence = sum_loss_per_sequence / valid_tokens_safe\n",
    "\n",
    "        # Return the mean loss across the batch\n",
    "        return tf.reduce_mean(mean_loss_per_sequence)\n",
    "\n",
    "    # Assign a name for clarity in logs/history if possible\n",
    "    masked_huber.__name__ = f'masked_huber_delta_{delta}'\n",
    "    return masked_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved LSTM Model Definition\n",
    "class ImprovedTimeDiffLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    LSTM model with enhancements like LayerNorm, Attention, Residuals, and Regularization.\n",
    "    Predicts time differences, total time, and calculates cumulative times.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=64, dropout_rate=0.3, num_attention_heads=4, l2_reg=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the model layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_units (int): Number of units in the main LSTM layer.\n",
    "            dropout_rate (float): Dropout rate for LSTM and Attention layers.\n",
    "            num_attention_heads (int): Number of heads for MultiHeadAttention.\n",
    "            l2_reg (float): L2 regularization factor for the first LSTM kernel.\n",
    "        \"\"\"\n",
    "        super().__init__() # Use super().__init__() for Python 3 style\n",
    "\n",
    "        # Input normalization\n",
    "        self.input_normalization = layers.LayerNormalization(name=\"InputNorm\")\n",
    "\n",
    "        # --- LSTM Block ---\n",
    "        # First LSTM layer with regularization and return sequences\n",
    "        self.lstm_layer1 = layers.LSTM(\n",
    "            hidden_units,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate, # Be cautious with recurrent_dropout on GPU\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            name=\"LSTM1\"\n",
    "        )\n",
    "        # Dense layer for residual connection matching dimensions\n",
    "        # Ensure it outputs `hidden_units` to match lstm_layer1 output\n",
    "        self.residual_dense = layers.Dense(hidden_units, activation='relu', name=\"ResidualDense\")\n",
    "        self.add_layer1 = layers.Add(name=\"AddResidual1\") # Explicit Add layer\n",
    "        self.norm_layer1 = layers.LayerNormalization(name=\"Norm1\") # Normalize after residual\n",
    "\n",
    "        # Second LSTM layer\n",
    "        self.lstm_layer2 = layers.LSTM(\n",
    "            hidden_units // 2, # Reduce dimensionality\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate,\n",
    "            name=\"LSTM2\"\n",
    "        )\n",
    "\n",
    "        # --- Attention Block ---\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_attention_heads,\n",
    "            key_dim=hidden_units // 2, # Key dim often matches the query/value dim\n",
    "            dropout=dropout_rate,\n",
    "            name=\"MultiHeadAttention\"\n",
    "        )\n",
    "        self.add_layer2 = layers.Add(name=\"AddResidual2\") # Add attention output to LSTM output\n",
    "        self.norm_layer2 = layers.LayerNormalization(name=\"Norm2\") # Normalize after attention\n",
    "\n",
    "        # --- Output Heads ---\n",
    "        # 1. Time Difference Head (predicts increment for each step)\n",
    "        # Use 'relu' to enforce non-negativity for time differences\n",
    "        self.time_diff_head = layers.Dense(1, activation='relu', name='TimeDiffHead')\n",
    "\n",
    "        # 2. Total Time Head (predicts total duration for the sequence)\n",
    "        # Takes the aggregated sequence representation after attention\n",
    "        # Use 'softplus' or 'relu' for non-negativity\n",
    "        self.total_time_head = layers.Dense(1, activation='softplus', name='TotalTimeHead')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor (batch_size, seq_len, num_features).\n",
    "            training (bool): Whether the model is in training mode (for dropout).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (predicted_time_diffs, predicted_total_time, predicted_cumulative_times)\n",
    "        \"\"\"\n",
    "        # Input normalization\n",
    "        x = self.input_normalization(inputs)\n",
    "\n",
    "        # --- LSTM Block with Residual ---\n",
    "        lstm_out1 = self.lstm_layer1(x, training=training)\n",
    "        residual = self.residual_dense(x) # Project input for residual connection\n",
    "        lstm_out1 = self.add_layer1([lstm_out1, residual])\n",
    "        lstm_out1 = self.norm_layer1(lstm_out1) # Normalize after adding\n",
    "\n",
    "        # Second LSTM layer\n",
    "        lstm_out2 = self.lstm_layer2(lstm_out1, training=training)\n",
    "\n",
    "        # --- Attention Block with Residual ---\n",
    "        # Use lstm_out2 as query, key, and value for self-attention\n",
    "        attn_out = self.attention(query=lstm_out2, value=lstm_out2, key=lstm_out2, training=training)\n",
    "        # Add residual connection from before attention\n",
    "        attn_out = self.add_layer2([attn_out, lstm_out2])\n",
    "        attn_out = self.norm_layer2(attn_out) # Normalize after adding\n",
    "\n",
    "        # --- Output Generation ---\n",
    "        # 1. Time Differences (per step)\n",
    "        # Apply the dense head to the output of the attention block\n",
    "        time_diffs = self.time_diff_head(attn_out)\n",
    "        # Remove the last dimension (Dense adds a dim of 1)\n",
    "        time_diffs = tf.squeeze(time_diffs, axis=-1) # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # 2. Total Time (per sequence)\n",
    "        # Aggregate the sequence information (e.g., mean pooling over time)\n",
    "        sequence_encoding = tf.reduce_mean(attn_out, axis=1) # Shape: (batch_size, hidden_units//2)\n",
    "        total_time = self.total_time_head(sequence_encoding) # Shape: (batch_size, 1)\n",
    "\n",
    "        # 3. Cumulative Times (calculated from predicted differences)\n",
    "        # Ensure calculation happens correctly even if time_diffs has NaNs (shouldn't if using relu)\n",
    "        cumulative_times = tf.cumsum(time_diffs, axis=1) # Shape: (batch_size, seq_len)\n",
    "\n",
    "        return time_diffs, total_time, cumulative_times\n",
    "\n",
    "    # Optional: Build method to define input shape explicitly\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Function (Added build confirmation)\n",
    "def train_improved_lstm(transformer_predictions_file, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Preprocess data, build, compile, and train the improved LSTM model.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to the transformer predictions CSV.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Training batch size.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_model, training_history, processed_data)\n",
    "    \"\"\"\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Starting LSTM Model Training Process\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 1. Process Data\n",
    "    print(\"Processing data...\")\n",
    "    data = process_transformer_predictions(transformer_predictions_file)\n",
    "    X_train = data['X']\n",
    "    y_inc_train = data['y_increments']\n",
    "    y_cum_train = data['y_cumulative']\n",
    "    y_total_train = data['y_total_times']\n",
    "    masks_train = data['masks']\n",
    "\n",
    "    if X_train.shape[0] == 0:\n",
    "        print(\"Error: No data available for training after processing.\")\n",
    "        return None, None, data\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    print(f\"Input shape for LSTM: {input_shape}\")\n",
    "    print(f\"Number of sequences for training/validation: {X_train.shape[0]}\")\n",
    "    print(f\"Target shapes: Increments {y_inc_train.shape}, Cumulative {y_cum_train.shape}, Total {y_total_train.shape}\")\n",
    "    print(f\"Mask shape: {masks_train.shape}, Mask sum (total valid steps): {np.sum(masks_train)}\")\n",
    "\n",
    "    # 2. Build Model\n",
    "    print(\"Building model...\")\n",
    "    lstm_model = ImprovedTimeDiffLSTM()\n",
    "    # Explicitly build the model\n",
    "    lstm_model.build(input_shape=(None,) + input_shape)\n",
    "    # *** Add confirmation print ***\n",
    "    print(f\"Model built status: {lstm_model.built}\")\n",
    "    # Print summary *after* building\n",
    "    lstm_model.summary(line_length=100)\n",
    "\n",
    "    # 3. Define Loss Functions\n",
    "    print(\"Defining loss functions...\")\n",
    "    masked_huber_loss_fn = custom_masked_huber_loss(tf.constant(masks_train, dtype=tf.float32))\n",
    "    total_time_loss_fn = tf.keras.losses.MeanSquaredError(name='total_time_mse')\n",
    "    loss_functions = [masked_huber_loss_fn, total_time_loss_fn, masked_huber_loss_fn]\n",
    "    loss_weights = [0.4, 0.3, 0.3]\n",
    "    print(f\"Loss functions set. Weights: {loss_weights}\")\n",
    "\n",
    "    # 4. Define Optimizer\n",
    "    print(\"Configuring optimizer...\")\n",
    "    learning_rate_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    optimizer = AdamW(learning_rate=learning_rate_schedule, weight_decay=0.001)\n",
    "\n",
    "    # 5. Compile Model\n",
    "    print(\"Compiling model...\")\n",
    "    lstm_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_functions,\n",
    "        loss_weights=loss_weights,\n",
    "    )\n",
    "    print(\"Model compiled successfully.\")\n",
    "\n",
    "    # 6. Define Callbacks\n",
    "    print(\"Configuring callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 7. Train Model\n",
    "    print(f\"Starting training for {epochs} epochs with batch size {batch_size}...\")\n",
    "    history = lstm_model.fit(\n",
    "        X_train,\n",
    "        [y_inc_train, y_total_train, y_cum_train],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Training finished.\")\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history.get('val_loss', [np.nan])[-1]\n",
    "    print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return lstm_model, history, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Generation Function (Changed output filename)\n",
    "def generate_lstm_predictions_from_transformer_csv(lstm_model, data):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained LSTM model and compare with transformer.\n",
    "\n",
    "    Args:\n",
    "        lstm_model (tf.keras.Model): The trained LSTM model.\n",
    "        data (dict): The processed data dictionary from `process_transformer_predictions`.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe containing original data, transformer predictions,\n",
    "                          LSTM predictions, and comparison metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Generating LSTM predictions...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    X_predict = data['X']\n",
    "    original_df = data['df']\n",
    "    processed_sequences = data['sequences']\n",
    "    masks_predict = data['masks']\n",
    "\n",
    "    if X_predict.shape[0] == 0:\n",
    "        print(\"Error: No data available for prediction.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Predicting on {X_predict.shape[0]} sequences...\")\n",
    "    try:\n",
    "        time_diffs_pred, total_time_pred, cumulative_times_pred = lstm_model.predict(X_predict)\n",
    "        print(\"Model prediction successful.\")\n",
    "        print(f\"Predicted shapes: Diffs {time_diffs_pred.shape}, Total {total_time_pred.shape}, Cumul {cumulative_times_pred.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model prediction: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    results_list = []\n",
    "    print(f\"Processing predictions for {len(processed_sequences)} sequences...\")\n",
    "\n",
    "    for seq_idx, seq_id in enumerate(processed_sequences):\n",
    "        seq_original_data = original_df[original_df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        seq_len = int(np.sum(masks_predict[seq_idx]))\n",
    "\n",
    "        if seq_len == 0:\n",
    "            print(f\"Warning: Sequence {seq_id} (index {seq_idx}) has length 0 in mask during prediction. Skipping.\")\n",
    "            continue\n",
    "        if seq_len > time_diffs_pred.shape[1]:\n",
    "             print(f\"Warning: Sequence {seq_id} (index {seq_idx}) mask length ({seq_len}) exceeds prediction dimension ({time_diffs_pred.shape[1]}). Clamping length.\")\n",
    "             seq_len = time_diffs_pred.shape[1]\n",
    "\n",
    "        current_time_diffs = time_diffs_pred[seq_idx, :seq_len]\n",
    "        current_cumulative = cumulative_times_pred[seq_idx, :seq_len]\n",
    "        current_total = total_time_pred[seq_idx, 0]\n",
    "\n",
    "        if len(seq_original_data) == seq_len:\n",
    "            seq_original_data['LSTM_Predicted_TimeDiff'] = current_time_diffs\n",
    "            seq_original_data['LSTM_Predicted_Cumulative'] = current_cumulative\n",
    "            seq_original_data['LSTM_Predicted_TotalTime'] = current_total\n",
    "        elif len(seq_original_data) > seq_len:\n",
    "             print(f\"Warning: Original data length ({len(seq_original_data)}) > mask length ({seq_len}) for seq {seq_id}. Padding predictions.\")\n",
    "             padded_diffs = np.pad(current_time_diffs, (0, len(seq_original_data) - seq_len), constant_values=np.nan)\n",
    "             padded_cumul = np.pad(current_cumulative, (0, len(seq_original_data) - seq_len), constant_values=np.nan)\n",
    "             seq_original_data['LSTM_Predicted_TimeDiff'] = padded_diffs\n",
    "             seq_original_data['LSTM_Predicted_Cumulative'] = padded_cumul\n",
    "             seq_original_data['LSTM_Predicted_TotalTime'] = current_total\n",
    "        else:\n",
    "             print(f\"Error: Original data length ({len(seq_original_data)}) < mask length ({seq_len}) for seq {seq_id}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        required_orig_cols = ['GroundTruth_Increment', 'Predicted_Increment',\n",
    "                              'GroundTruth_Cumulative', 'Predicted_Cumulative']\n",
    "        if not all(col in seq_original_data for col in required_orig_cols):\n",
    "             print(f\"Warning: Missing required original prediction columns for sequence {seq_id}. Cannot calculate improvements.\")\n",
    "             seq_original_data['TimeDiff_Improvement_Abs'] = np.nan\n",
    "             seq_original_data['Cumulative_Improvement_Abs'] = np.nan\n",
    "        else:\n",
    "            transformer_diff_error = abs(seq_original_data['GroundTruth_Increment'] - seq_original_data['Predicted_Increment'])\n",
    "            lstm_diff_error = abs(seq_original_data['GroundTruth_Increment'] - seq_original_data['LSTM_Predicted_TimeDiff'])\n",
    "            transformer_cum_error = abs(seq_original_data['GroundTruth_Cumulative'] - seq_original_data['Predicted_Cumulative'])\n",
    "            lstm_cum_error = abs(seq_original_data['GroundTruth_Cumulative'] - seq_original_data['LSTM_Predicted_Cumulative'])\n",
    "            seq_original_data['TimeDiff_Improvement_Abs'] = transformer_diff_error - lstm_diff_error\n",
    "            seq_original_data['Cumulative_Improvement_Abs'] = transformer_cum_error - lstm_cum_error\n",
    "\n",
    "        results_list.append(seq_original_data)\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"Error: No results generated after processing predictions.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    results_df = pd.concat(results_list, ignore_index=True)\n",
    "    print(f\"Finished processing predictions. Combined results shape: {results_df.shape}\")\n",
    "    print(\"Combined results head:\\n\", results_df.head())\n",
    "    print(\"Combined results tail:\\n\", results_df.tail())\n",
    "    print(\"Checking for NaNs in LSTM predictions:\")\n",
    "    print(results_df[['LSTM_Predicted_TimeDiff', 'LSTM_Predicted_Cumulative', 'LSTM_Predicted_TotalTime']].isnull().sum())\n",
    "\n",
    "    # *** Changed output filename ***\n",
    "    output_filename = 'predictions_lstm_refined_175974.csv'\n",
    "    output_path = os.path.abspath(output_filename)\n",
    "    try:\n",
    "        results_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Combined predictions successfully saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to CSV ({output_path}): {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Overall Model Performance Comparison (Mean Absolute Error)\")\n",
    "    print(\"-\" * 30)\n",
    "    comparison_cols = required_orig_cols + ['LSTM_Predicted_TimeDiff', 'LSTM_Predicted_Cumulative']\n",
    "    valid_results = results_df.dropna(subset=comparison_cols)\n",
    "    print(f\"Calculating MAE based on {len(valid_results)} valid rows (after dropping NaNs in comparison columns).\")\n",
    "\n",
    "    if valid_results.empty:\n",
    "        print(\"Warning: No valid data points found for calculating overall MAE.\")\n",
    "    else:\n",
    "        try:\n",
    "            transformer_time_diff_mae = np.mean(abs(valid_results['GroundTruth_Increment'] - valid_results['Predicted_Increment']))\n",
    "            lstm_time_diff_mae = np.mean(abs(valid_results['GroundTruth_Increment'] - valid_results['LSTM_Predicted_TimeDiff']))\n",
    "            transformer_cumulative_mae = np.mean(abs(valid_results['GroundTruth_Cumulative'] - valid_results['Predicted_Cumulative']))\n",
    "            lstm_cumulative_mae = np.mean(abs(valid_results['GroundTruth_Cumulative'] - valid_results['LSTM_Predicted_Cumulative']))\n",
    "\n",
    "            time_diff_improvement_pct = (1 - lstm_time_diff_mae / transformer_time_diff_mae) * 100 if transformer_time_diff_mae != 0 else float('inf') if lstm_time_diff_mae < transformer_time_diff_mae else 0\n",
    "            cumulative_improvement_pct = (1 - lstm_cumulative_mae / transformer_cumulative_mae) * 100 if transformer_cumulative_mae != 0 else float('inf') if lstm_cumulative_mae < transformer_cumulative_mae else 0\n",
    "\n",
    "            print(f\"Time Differences MAE:\")\n",
    "            print(f\"  Transformer: {transformer_time_diff_mae:.4f}\")\n",
    "            print(f\"  LSTM:        {lstm_time_diff_mae:.4f}\")\n",
    "            print(f\"  Improvement: {time_diff_improvement_pct:.2f}%\")\n",
    "            print(f\"\\nCumulative Times MAE:\")\n",
    "            print(f\"  Transformer: {transformer_cumulative_mae:.4f}\")\n",
    "            print(f\"  LSTM:        {lstm_cumulative_mae:.4f}\")\n",
    "            print(f\"  Improvement: {cumulative_improvement_pct:.2f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating overall MAE: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM model training, prediction, and visualization.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\" LSTM Model Training and Evaluation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # --- Configuration ---\n",
    "    transformer_predictions_file = \"predictions_transformer.csv\"\n",
    "    training_epochs = 100\n",
    "    training_batch_size = 32\n",
    "    output_suffix = \"_runFinal182625\" # Suffix for output files\n",
    "\n",
    "    lstm_model = None\n",
    "    history = None\n",
    "    data = None\n",
    "    results_df = None\n",
    "\n",
    "    try:\n",
    "        # --- Step 1: Train LSTM Model ---\n",
    "        print(\"\\n--- Step 1: Training LSTM Model ---\")\n",
    "        lstm_model, history, data = train_improved_lstm(\n",
    "            transformer_predictions_file,\n",
    "            epochs=training_epochs,\n",
    "            batch_size=training_batch_size\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Generate Predictions ---\n",
    "        print(\"\\n--- Step 2: Generating LSTM Predictions ---\")\n",
    "        if lstm_model is None or data is None:\n",
    "             print(\"Model training or data processing failed. Cannot generate predictions.\")\n",
    "        else:\n",
    "            # Pass suffix to prediction function if needed, or handle filenames inside main\n",
    "            results_df = generate_lstm_predictions_from_transformer_csv(lstm_model, data) # Filename handled inside\n",
    "\n",
    "        # --- Step 3: Display Sample Results ---\n",
    "        print(\"\\n--- Step 3: Displaying Sample Results ---\")\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            print(\"\\nSample Combined Predictions (Head):\")\n",
    "            display_cols = ['Sequence', 'Step', 'SourceID',\n",
    "                            'GroundTruth_Increment', 'Predicted_Increment', 'LSTM_Predicted_TimeDiff',\n",
    "                            'GroundTruth_Cumulative','Predicted_Cumulative','LSTM_Predicted_Cumulative',\n",
    "                            'TimeDiff_Improvement_Abs', 'Cumulative_Improvement_Abs']\n",
    "            display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "            print(results_df[display_cols].head(10).to_string())\n",
    "        else:\n",
    "            print(\"No results dataframe generated or it is empty.\")\n",
    "\n",
    "        # --- Step 4: Generate Visualizations ---\n",
    "        print(\"\\n--- Step 4: Generating Visualizations ---\")\n",
    "        # Pass suffix to visualization function if needed, or handle filenames inside main\n",
    "        visualize_results(results_df, history, num_samples=3) # Filename handled inside\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" Pipeline Execution Completed\")\n",
    "        # Provide paths to output files with the new suffix\n",
    "        print(f\"  - Combined predictions CSV saved to: {os.path.abspath(f'predictions_lstm_refined{output_suffix}.csv')}\")\n",
    "        print(f\"  - Training performance plot saved to: {os.path.abspath(f'training_performance{output_suffix}.png')}\")\n",
    "        print(f\"  - Sequence comparison plots saved as: sequence_<ID>_comparison{output_suffix}.png\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"\\nFatal Error: {fnf_error}\")\n",
    "        print(\"Please ensure the input CSV file exists and the path is correct.\")\n",
    "    except ValueError as val_error:\n",
    "        print(f\"\\nFatal Error: {val_error}\")\n",
    "        print(\"Please check the data format, required columns, and content in the input file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected fatal error occurred in main execution: {e}\")\n",
    "        print(\"\\n--- Traceback ---\")\n",
    "        traceback.print_exc()\n",
    "        print(\"--- End Traceback ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Function (Changed output filenames)\n",
    "def visualize_results(results_df, history, num_samples=3):\n",
    "    \"\"\"\n",
    "    Generate visualizations for training history and prediction comparisons.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): Dataframe with combined predictions.\n",
    "        history (tf.keras.callbacks.History): Training history object.\n",
    "        num_samples (int): Number of sample sequences to plot.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Generating visualizations...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- Plot Suffix ---\n",
    "    file_suffix = \"_run2\" # Suffix for plot filenames\n",
    "\n",
    "    if history is None:\n",
    "        print(\"Warning: Training history object is None. Skipping history plots.\")\n",
    "    else:\n",
    "        # --- 1. Plot Training History ---\n",
    "        try:\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['loss'], label='Training Loss')\n",
    "            if 'val_loss' in history.history:\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            else:\n",
    "                 print(\"Warning: 'val_loss' not found in history.\")\n",
    "            plt.title('Model Loss During Training')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss (Weighted Sum)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plotted_specific_loss = False\n",
    "            loss_keys = [k for k in history.history.keys() if 'loss' in k and k != 'loss' and k != 'val_loss']\n",
    "            if loss_keys:\n",
    "                 for key in loss_keys[:1]:\n",
    "                     val_key = 'val_' + key\n",
    "                     plt.plot(history.history[key], label=f'Train {key}')\n",
    "                     if val_key in history.history:\n",
    "                         plt.plot(history.history[val_key], label=f'Val {key}')\n",
    "                     plt.title(f'{key.replace(\"_\",\" \").title()}')\n",
    "                     plotted_specific_loss = True\n",
    "                 plt.xlabel('Epoch')\n",
    "                 plt.ylabel('Loss')\n",
    "                 plt.legend()\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No specific loss keys found', horizontalalignment='center', verticalalignment='center')\n",
    "                plt.title('Additional Training Metrics')\n",
    "                plt.xlabel('Epoch')\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            # *** Changed output filename ***\n",
    "            output_filename = f'training_performance{file_suffix}.png'\n",
    "            output_path = os.path.abspath(output_filename)\n",
    "            print(f\"Attempting to save training performance plot to: {output_path}\")\n",
    "            plt.savefig(output_filename)\n",
    "            print(f\"Successfully saved training performance plot.\")\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating or saving training history plot: {e}\")\n",
    "            traceback.print_exc()\n",
    "            plt.close()\n",
    "\n",
    "    # --- 2. Plot Prediction Comparison for Sample Sequences ---\n",
    "    if results_df is None or results_df.empty:\n",
    "        print(\"Warning: Results dataframe is empty or None. Skipping prediction comparison plots.\")\n",
    "        return\n",
    "\n",
    "    required_lstm_cols = ['LSTM_Predicted_TimeDiff', 'LSTM_Predicted_Cumulative']\n",
    "    if not all(col in results_df.columns for col in required_lstm_cols):\n",
    "        print(f\"Warning: Missing required LSTM prediction columns ({required_lstm_cols}) in results_df. Skipping comparison plots.\")\n",
    "        return\n",
    "    if results_df[required_lstm_cols].isnull().all().all():\n",
    "         print(f\"Warning: LSTM prediction columns ({required_lstm_cols}) contain only NaN values. Skipping comparison plots.\")\n",
    "         return\n",
    "\n",
    "    sequences_to_plot = results_df['Sequence'].unique()\n",
    "    if len(sequences_to_plot) == 0:\n",
    "        print(\"No unique sequences found in results_df to plot.\")\n",
    "        return\n",
    "\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(sequences_to_plot)\n",
    "    sample_sequences = sequences_to_plot[:min(num_samples, len(sequences_to_plot))]\n",
    "    print(f\"Attempting to plot prediction comparisons for {len(sample_sequences)} sample sequences: {sample_sequences}\")\n",
    "\n",
    "    plot_cols_increments = ['GroundTruth_Increment', 'Predicted_Increment', 'LSTM_Predicted_TimeDiff']\n",
    "    plot_cols_cumulative = ['GroundTruth_Cumulative', 'Predicted_Cumulative', 'LSTM_Predicted_Cumulative']\n",
    "    plot_labels = ['Ground Truth', 'Transformer Pred.', 'LSTM Pred.']\n",
    "    plot_styles = ['o-', 's--', '^-']\n",
    "\n",
    "    for seq_id in sample_sequences:\n",
    "        print(f\"\\nProcessing plot for Sequence ID: {seq_id}\")\n",
    "        seq_results = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "        if seq_results.empty:\n",
    "            print(f\"  Skipping plot for sequence {seq_id} as no data found in results_df.\")\n",
    "            continue\n",
    "        print(f\"  Data shape for sequence {seq_id}: {seq_results.shape}\")\n",
    "\n",
    "        try:\n",
    "            plt.figure(figsize=(15, 7))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            for i, col in enumerate(plot_cols_increments):\n",
    "                if col in seq_results.columns and not seq_results[col].isnull().all():\n",
    "                    plt.plot(seq_results['Step'], seq_results[col], plot_styles[i], label=plot_labels[i], alpha=0.8)\n",
    "                    print(f\"  Plotted '{col}' for increments.\")\n",
    "                else:\n",
    "                    print(f\"  Skipping plot for '{col}' (increments) - column missing or all NaN.\")\n",
    "            plt.title(f'Sequence {seq_id} - Time Increments')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Time Increment')\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            for i, col in enumerate(plot_cols_cumulative):\n",
    "                 if col in seq_results.columns and not seq_results[col].isnull().all():\n",
    "                    plt.plot(seq_results['Step'], seq_results[col], plot_styles[i], label=plot_labels[i], alpha=0.8)\n",
    "                    print(f\"  Plotted '{col}' for cumulative.\")\n",
    "                 else:\n",
    "                    print(f\"  Skipping plot for '{col}' (cumulative) - column missing or all NaN.\")\n",
    "            plt.title(f'Sequence {seq_id} - Cumulative Time')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Cumulative Time')\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            # *** Changed output filename ***\n",
    "            output_filename = f'sequence_{seq_id}_comparison{file_suffix}.png'\n",
    "            output_path = os.path.abspath(output_filename)\n",
    "            print(f\"  Attempting to save comparison plot to: {output_path}\")\n",
    "            plt.savefig(output_filename)\n",
    "            print(f\"  Successfully saved comparison plot for sequence {seq_id}.\")\n",
    "            plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating or saving comparison plot for sequence {seq_id}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            plt.close()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Visualization generation finished.\")\n",
    "    print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM model training, prediction, and visualization.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\" LSTM Model Training and Evaluation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # --- Configuration ---\n",
    "    transformer_predictions_file = \"predictions_transformer_175974.csv\" # Ensure this file exists!\n",
    "    training_epochs = 100 # Use the original epoch count\n",
    "    training_batch_size = 32\n",
    "\n",
    "    lstm_model = None\n",
    "    history = None\n",
    "    data = None\n",
    "    results_df = None\n",
    "\n",
    "    try:\n",
    "        # --- Step 1: Train LSTM Model ---\n",
    "        print(\"\\n--- Step 1: Training LSTM Model ---\")\n",
    "        lstm_model, history, data = train_improved_lstm(\n",
    "            transformer_predictions_file,\n",
    "            epochs=training_epochs,\n",
    "            batch_size=training_batch_size\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Generate Predictions ---\n",
    "        print(\"\\n--- Step 2: Generating LSTM Predictions ---\")\n",
    "        if lstm_model is None or data is None:\n",
    "             print(\"Model training or data processing failed. Cannot generate predictions.\")\n",
    "             # Optionally: try loading a pre-trained model if available\n",
    "        else:\n",
    "            results_df = generate_lstm_predictions_from_transformer_csv(lstm_model, data)\n",
    "\n",
    "        # --- Step 3: Display Sample Results ---\n",
    "        print(\"\\n--- Step 3: Displaying Sample Results ---\")\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            print(\"\\nSample Combined Predictions (Head):\")\n",
    "            display_cols = ['Sequence', 'Step', 'SourceID',\n",
    "                            'GroundTruth_Increment', 'Predicted_Increment', 'LSTM_Predicted_TimeDiff',\n",
    "                            'GroundTruth_Cumulative','Predicted_Cumulative','LSTM_Predicted_Cumulative',\n",
    "                            'TimeDiff_Improvement_Abs', 'Cumulative_Improvement_Abs']\n",
    "            display_cols = [col for col in display_cols if col in results_df.columns] # Filter existing columns\n",
    "            print(results_df[display_cols].head(10).to_string())\n",
    "        else:\n",
    "            print(\"No results dataframe generated or it is empty.\")\n",
    "\n",
    "        # --- Step 4: Generate Visualizations ---\n",
    "        print(\"\\n--- Step 4: Generating Visualizations ---\")\n",
    "        # Pass history and results_df even if they are None/empty, the function handles it\n",
    "        visualize_results(results_df, history, num_samples=3)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" Pipeline Execution Completed\")\n",
    "        # Provide paths to output files\n",
    "        print(f\"  - Combined predictions CSV saved to: {os.path.abspath('predictions_lstm_refined_1.csv')}\")\n",
    "        print(f\"  - Training performance plot saved to: {os.path.abspath('training_performance.png')}\")\n",
    "        print(\"  - Sequence comparison plots saved as: sequence_<ID>_comparison.png\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"\\nFatal Error: {fnf_error}\")\n",
    "        print(\"Please ensure the input CSV file exists and the path is correct.\")\n",
    "    except ValueError as val_error:\n",
    "        print(f\"\\nFatal Error: {val_error}\")\n",
    "        print(\"Please check the data format, required columns, and content in the input file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected fatal error occurred in main execution: {e}\")\n",
    "        print(\"\\n--- Traceback ---\")\n",
    "        traceback.print_exc()\n",
    "        print(\"--- End Traceback ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " LSTM Model Training and Evaluation Pipeline\n",
      "==================================================\n",
      "\n",
      "--- Step 1: Training LSTM Model ---\n",
      "------------------------------\n",
      "Starting LSTM Model Training Process\n",
      "------------------------------\n",
      "Processing data...\n",
      "Attempting to load data from: predictions_transformer_175974.csv\n",
      "Successfully loaded CSV. Shape: (3349, 8)\n",
      "Columns found: ['Sequence', 'Step', 'SourceID', 'Predicted_Proportion', 'Predicted_Increment', 'Predicted_Cumulative', 'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
      "All required columns are present.\n",
      "Data validation checks passed.\n",
      "Found 137 unique sequences in the data.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.7500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Successfully processed 137 sequences.\n",
      "Padding sequences to max length: 170\n",
      "Data processing and padding complete.\n",
      "Input shape for LSTM: (170, 4)\n",
      "Number of sequences for training/validation: 137\n",
      "Target shapes: Increments (137, 170), Cumulative (137, 170), Total (137,)\n",
      "Mask shape: (137, 170), Mask sum (total valid steps): 3349.0\n",
      "Building model...\n",
      "Model built status: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"improved_time_diff_lstm_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"improved_time_diff_lstm_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                               </span><span style=\"font-weight: bold\"> Output Shape                    </span><span style=\"font-weight: bold\">           Param # </span>\n",
       "\n",
       " InputNorm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " LSTM1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                                ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " ResidualDense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " AddResidual1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                          ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " Norm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                  ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " LSTM2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                                ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " MultiHeadAttention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)     ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " AddResidual2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                          ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " Norm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                  ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " TimeDiffHead (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " TotalTimeHead (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                              \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " InputNorm (\u001b[38;5;33mLayerNormalization\u001b[0m)              ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " LSTM1 (\u001b[38;5;33mLSTM\u001b[0m)                                ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " ResidualDense (\u001b[38;5;33mDense\u001b[0m)                       ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " AddResidual1 (\u001b[38;5;33mAdd\u001b[0m)                          ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " Norm1 (\u001b[38;5;33mLayerNormalization\u001b[0m)                  ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " LSTM2 (\u001b[38;5;33mLSTM\u001b[0m)                                ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " MultiHeadAttention (\u001b[38;5;33mMultiHeadAttention\u001b[0m)     ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " AddResidual2 (\u001b[38;5;33mAdd\u001b[0m)                          ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " Norm2 (\u001b[38;5;33mLayerNormalization\u001b[0m)                  ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " TimeDiffHead (\u001b[38;5;33mDense\u001b[0m)                        ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " TotalTimeHead (\u001b[38;5;33mDense\u001b[0m)                       ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining loss functions...\n",
      "Loss functions set. Weights: [0.4, 0.3, 0.3]\n",
      "Configuring optimizer...\n",
      "Compiling model...\n",
      "Model compiled successfully.\n",
      "Configuring callbacks...\n",
      "Starting training for 100 epochs with batch size 32...\n",
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 272ms/step - loss: 95.3831 - masked_huber_delta_10_loss: 50.2491 - total_time_mse_loss: 0.0736 - val_loss: 27.4389 - val_masked_huber_delta_10_loss: 16.1503 - val_total_time_mse_loss: 3.1342e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 90.6827 - masked_huber_delta_10_loss: 45.9130 - total_time_mse_loss: 0.0014 - val_loss: 26.2535 - val_masked_huber_delta_10_loss: 14.8058 - val_total_time_mse_loss: 9.8587e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 87.9340 - masked_huber_delta_10_loss: 43.2589 - total_time_mse_loss: 0.0028 - val_loss: 26.6464 - val_masked_huber_delta_10_loss: 15.0640 - val_total_time_mse_loss: 0.0017\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 86.7527 - masked_huber_delta_10_loss: 42.0728 - total_time_mse_loss: 0.0046 - val_loss: 26.3408 - val_masked_huber_delta_10_loss: 14.7731 - val_total_time_mse_loss: 0.0029\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 86.0532 - masked_huber_delta_10_loss: 41.4196 - total_time_mse_loss: 0.0077 - val_loss: 25.7223 - val_masked_huber_delta_10_loss: 14.1941 - val_total_time_mse_loss: 0.0055\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 85.4764 - masked_huber_delta_10_loss: 40.8736 - total_time_mse_loss: 0.0118 - val_loss: 25.4260 - val_masked_huber_delta_10_loss: 13.8967 - val_total_time_mse_loss: 0.0083\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 85.3409 - masked_huber_delta_10_loss: 40.7698 - total_time_mse_loss: 0.0133 - val_loss: 25.3682 - val_masked_huber_delta_10_loss: 13.8229 - val_total_time_mse_loss: 0.0069\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 84.9768 - masked_huber_delta_10_loss: 40.3982 - total_time_mse_loss: 0.0117 - val_loss: 25.3947 - val_masked_huber_delta_10_loss: 13.8309 - val_total_time_mse_loss: 0.0044\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 84.6063 - masked_huber_delta_10_loss: 40.0352 - total_time_mse_loss: 0.0071 - val_loss: 25.5435 - val_masked_huber_delta_10_loss: 13.9439 - val_total_time_mse_loss: 0.0027\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 84.4075 - masked_huber_delta_10_loss: 39.8040 - total_time_mse_loss: 0.0053 - val_loss: 25.7109 - val_masked_huber_delta_10_loss: 14.0762 - val_total_time_mse_loss: 0.0022\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 84.0737 - masked_huber_delta_10_loss: 39.4659 - total_time_mse_loss: 0.0043 - val_loss: 25.7815 - val_masked_huber_delta_10_loss: 14.1286 - val_total_time_mse_loss: 0.0023\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 84.1172 - masked_huber_delta_10_loss: 39.4992 - total_time_mse_loss: 0.0038 - val_loss: 25.8557 - val_masked_huber_delta_10_loss: 14.1860 - val_total_time_mse_loss: 0.0019\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 83.9801 - masked_huber_delta_10_loss: 39.3335 - total_time_mse_loss: 0.0039 - val_loss: 25.8266 - val_masked_huber_delta_10_loss: 14.1562 - val_total_time_mse_loss: 0.0019\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 83.8037 - masked_huber_delta_10_loss: 39.1593 - total_time_mse_loss: 0.0033 - val_loss: 25.6970 - val_masked_huber_delta_10_loss: 14.0480 - val_total_time_mse_loss: 0.0020\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 83.7188 - masked_huber_delta_10_loss: 39.1591 - total_time_mse_loss: 0.0035 - val_loss: 25.6512 - val_masked_huber_delta_10_loss: 14.0074 - val_total_time_mse_loss: 0.0019\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 83.5158 - masked_huber_delta_10_loss: 38.9325 - total_time_mse_loss: 0.0042 - val_loss: 25.7275 - val_masked_huber_delta_10_loss: 14.0707 - val_total_time_mse_loss: 0.0016\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 83.2840 - masked_huber_delta_10_loss: 38.7032 - total_time_mse_loss: 0.0032 - val_loss: 25.7739 - val_masked_huber_delta_10_loss: 14.1104 - val_total_time_mse_loss: 0.0014\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 83.3088 - masked_huber_delta_10_loss: 38.7586 - total_time_mse_loss: 0.0033 - val_loss: 25.6576 - val_masked_huber_delta_10_loss: 14.0169 - val_total_time_mse_loss: 0.0013\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 83.2238 - masked_huber_delta_10_loss: 38.6952 - total_time_mse_loss: 0.0023 - val_loss: 25.6627 - val_masked_huber_delta_10_loss: 14.0250 - val_total_time_mse_loss: 0.0011\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 83.1753 - masked_huber_delta_10_loss: 38.5906 - total_time_mse_loss: 0.0027 - val_loss: 25.7051 - val_masked_huber_delta_10_loss: 14.0644 - val_total_time_mse_loss: 8.9222e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 83.3060 - masked_huber_delta_10_loss: 38.7406 - total_time_mse_loss: 0.0018 - val_loss: 25.6787 - val_masked_huber_delta_10_loss: 14.0471 - val_total_time_mse_loss: 7.7192e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 82.9764 - masked_huber_delta_10_loss: 38.4743 - total_time_mse_loss: 0.0014 - val_loss: 25.6131 - val_masked_huber_delta_10_loss: 13.9989 - val_total_time_mse_loss: 7.1640e-04\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Training finished.\n",
      "Final Training Loss: 82.2296\n",
      "Final Validation Loss: 25.6131\n",
      "------------------------------\n",
      "\n",
      "--- Step 2: Generating LSTM Predictions ---\n",
      "\n",
      "------------------------------\n",
      "Generating LSTM predictions...\n",
      "------------------------------\n",
      "Predicting on 137 sequences...\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step\n",
      "Model prediction successful.\n",
      "Predicted shapes: Diffs (137, 170), Total (137, 1), Cumul (137, 170)\n",
      "Processing predictions for 137 sequences...\n",
      "Finished processing predictions. Combined results shape: (3349, 13)\n",
      "Combined results head:\n",
      "    Sequence  Step  SourceID  Predicted_Proportion  Predicted_Increment  \\\n",
      "0         0     1      11.0              0.058614            15.181008   \n",
      "1         0     2       4.0              0.058658            15.192348   \n",
      "2         0     3       5.0              0.058133            15.056521   \n",
      "3         0     4       5.0              0.058406            15.127127   \n",
      "4         0     5       1.0              0.059546            15.422354   \n",
      "\n",
      "   Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \\\n",
      "0             30.692078                   42.0                    42.0   \n",
      "1             45.884426                   10.0                    52.0   \n",
      "2             60.940950                   10.0                    62.0   \n",
      "3             76.068080                    2.0                    64.0   \n",
      "4             91.490430                   93.0                   157.0   \n",
      "\n",
      "   LSTM_Predicted_TimeDiff  LSTM_Predicted_Cumulative  \\\n",
      "0                 7.983111                   7.983111   \n",
      "1                 8.144967                  16.128078   \n",
      "2                 8.166656                  24.294735   \n",
      "3                 8.165462                  32.460197   \n",
      "4                 7.985576                  40.445774   \n",
      "\n",
      "   LSTM_Predicted_TotalTime  TimeDiff_Improvement_Abs  \\\n",
      "0                  0.145988                 -7.197897   \n",
      "1                  0.145988                  3.337316   \n",
      "2                  0.145988                  3.223177   \n",
      "3                  0.145988                  6.961665   \n",
      "4                  0.145988                 -7.436778   \n",
      "\n",
      "   Cumulative_Improvement_Abs  \n",
      "0                  -22.708967  \n",
      "1                  -29.756348  \n",
      "2                  -36.646215  \n",
      "3                  -19.471723  \n",
      "4                  -51.044656  \n",
      "Combined results tail:\n",
      "       Sequence  Step  SourceID  Predicted_Proportion  Predicted_Increment  \\\n",
      "3344       135    20       9.0              0.040730            18.898499   \n",
      "3345       135    21       4.0              0.041213            19.122887   \n",
      "3346       135    22       5.0              0.041096            19.068563   \n",
      "3347       135    23       2.0              0.041484            19.248802   \n",
      "3348       136     1      10.0              0.496606           230.425130   \n",
      "\n",
      "      Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \\\n",
      "3344             406.55975                    6.0                   446.0   \n",
      "3345             425.68265                    2.0                   448.0   \n",
      "3346             444.75122                   16.0                   464.0   \n",
      "3347             464.00003                 -464.0                     0.0   \n",
      "3348             463.99997                 -464.0                     0.0   \n",
      "\n",
      "      LSTM_Predicted_TimeDiff  LSTM_Predicted_Cumulative  \\\n",
      "3344                 6.504357                 158.824829   \n",
      "3345                 4.268525                 163.093353   \n",
      "3346                 1.593228                 164.686584   \n",
      "3347                 0.787370                 165.473953   \n",
      "3348                 7.234004                   7.234004   \n",
      "\n",
      "      LSTM_Predicted_TotalTime  TimeDiff_Improvement_Abs  \\\n",
      "3344                  0.152977                 12.394142   \n",
      "3345                  0.152977                 14.854362   \n",
      "3346                  0.152977                -11.338209   \n",
      "3347                  0.152977                 18.461432   \n",
      "3348                  0.125598                223.191126   \n",
      "\n",
      "      Cumulative_Improvement_Abs  \n",
      "3344                 -247.734921  \n",
      "3345                 -262.589297  \n",
      "3346                 -280.064636  \n",
      "3347                  298.526077  \n",
      "3348                  456.765966  \n",
      "Checking for NaNs in LSTM predictions:\n",
      "LSTM_Predicted_TimeDiff      0\n",
      "LSTM_Predicted_Cumulative    0\n",
      "LSTM_Predicted_TotalTime     0\n",
      "dtype: int64\n",
      "Combined predictions successfully saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\predictions_lstm_refined_175974.csv\n",
      "\n",
      "------------------------------\n",
      "Overall Model Performance Comparison (Mean Absolute Error)\n",
      "------------------------------\n",
      "Calculating MAE based on 3349 valid rows (after dropping NaNs in comparison columns).\n",
      "Time Differences MAE:\n",
      "  Transformer: 55.1303\n",
      "  LSTM:        55.0545\n",
      "  Improvement: 0.14%\n",
      "\n",
      "Cumulative Times MAE:\n",
      "  Transformer: 69.8888\n",
      "  LSTM:        475.4004\n",
      "  Improvement: -580.22%\n",
      "------------------------------\n",
      "\n",
      "--- Step 3: Displaying Sample Results ---\n",
      "\n",
      "Sample Combined Predictions (Head):\n",
      "   Sequence  Step  SourceID  GroundTruth_Increment  Predicted_Increment  LSTM_Predicted_TimeDiff  GroundTruth_Cumulative  Predicted_Cumulative  LSTM_Predicted_Cumulative  TimeDiff_Improvement_Abs  Cumulative_Improvement_Abs\n",
      "0         0     1      11.0                   42.0            15.181008                 7.983111                    42.0             30.692078                   7.983111                 -7.197897                  -22.708967\n",
      "1         0     2       4.0                   10.0            15.192348                 8.144967                    52.0             45.884426                  16.128078                  3.337316                  -29.756348\n",
      "2         0     3       5.0                   10.0            15.056521                 8.166656                    62.0             60.940950                  24.294735                  3.223177                  -36.646215\n",
      "3         0     4       5.0                    2.0            15.127127                 8.165462                    64.0             76.068080                  32.460197                  6.961665                  -19.471723\n",
      "4         0     5       1.0                   93.0            15.422354                 7.985576                   157.0             91.490430                  40.445774                 -7.436778                  -51.044656\n",
      "5         0     6       1.0                    9.0            15.524850                 7.965937                   166.0            107.015280                  48.411713                  5.490787                  -58.603567\n",
      "6         0     7       5.0                    1.0            15.350183                 7.739552                   167.0            122.365460                  56.151264                  7.610632                  -66.214196\n",
      "7         0     8       4.0                    2.0            15.546829                 7.231389                   169.0            137.912300                  63.382652                  8.315440                  -74.529648\n",
      "8         0     9       5.0                    2.0            15.372622                 6.279351                   171.0            153.284910                  69.662003                  9.093271                  -83.622907\n",
      "9         0    10       4.0                    2.0            15.473594                 4.477065                   173.0            168.758500                  74.139069                 10.996529                  -94.619431\n",
      "\n",
      "--- Step 4: Generating Visualizations ---\n",
      "\n",
      "------------------------------\n",
      "Generating visualizations...\n",
      "------------------------------\n",
      "Attempting to save training performance plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\training_performance_run2.png\n",
      "Successfully saved training performance plot.\n",
      "Attempting to plot prediction comparisons for 3 sample sequences: [105 104  12]\n",
      "\n",
      "Processing plot for Sequence ID: 105\n",
      "  Data shape for sequence 105: (22, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_105_comparison_run2.png\n",
      "  Successfully saved comparison plot for sequence 105.\n",
      "\n",
      "Processing plot for Sequence ID: 104\n",
      "  Data shape for sequence 104: (20, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_104_comparison_run2.png\n",
      "  Successfully saved comparison plot for sequence 104.\n",
      "\n",
      "Processing plot for Sequence ID: 12\n",
      "  Data shape for sequence 12: (28, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_12_comparison_run2.png\n",
      "  Successfully saved comparison plot for sequence 12.\n",
      "------------------------------\n",
      "Visualization generation finished.\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      " Pipeline Execution Completed\n",
      "  - Combined predictions CSV saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\predictions_lstm_refined_1.csv\n",
      "  - Training performance plot saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\training_performance.png\n",
      "  - Sequence comparison plots saved as: sequence_<ID>_comparison.png\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Script Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    # import matplotlib\n",
    "    # matplotlib.use('Agg') # Uncomment if running in a non-GUI environment\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
