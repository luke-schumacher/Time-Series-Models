{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import AdamW # Using AdamW for weight decay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os # Added for file path handling\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Validation\n",
    "def load_and_validate_data(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Load and validate transformer predictions CSV file.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Validated dataframe.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file does not exist.\n",
    "        ValueError: If required columns are missing or have wrong types.\n",
    "        Exception: For other potential loading errors.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Error: Input file not found at {transformer_predictions_file}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(transformer_predictions_file)\n",
    "        print(f\"Successfully loaded CSV. Shape: {df.shape}\")\n",
    "        print(f\"Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "        # Basic validation checks\n",
    "        # Ensure these columns exist based on subsequent usage\n",
    "        required_columns = ['Sequence', 'Step', 'SourceID', 'Predicted_Proportion',\n",
    "                            'Predicted_Increment', 'Predicted_Cumulative',\n",
    "                            'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
    "\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns in input CSV: {', '.join(missing_cols)}\")\n",
    "        print(\"All required columns are present.\")\n",
    "\n",
    "        # Check for essential numeric types\n",
    "        numeric_cols_to_check = ['Step', 'Predicted_Proportion', 'Predicted_Increment',\n",
    "                                 'Predicted_Cumulative', 'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
    "        for col in numeric_cols_to_check:\n",
    "             # Attempt to convert to numeric, coercing errors to NaN\n",
    "             df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "             if df[col].isnull().any():\n",
    "                 print(f\"Warning: Column '{col}' contains non-numeric values that were converted to NaN.\")\n",
    "                 # Optionally raise ValueError if NaNs are unacceptable here\n",
    "                 # raise ValueError(f\"Column '{col}' must be entirely numeric.\")\n",
    "\n",
    "        print(\"Data validation checks passed.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Data loading error: {e}\")\n",
    "        traceback.print_exc() # Print detailed traceback\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion Binning\n",
    "def bin_proportions(proportions, bin_size=0.05):\n",
    "    \"\"\"\n",
    "    Bin proportions and normalize if the sum deviates significantly from 1.0.\n",
    "\n",
    "    Args:\n",
    "        proportions (array-like): Original proportion values.\n",
    "        bin_size (float): Bin resolution.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binned and potentially normalized proportions.\n",
    "    \"\"\"\n",
    "    # Ensure input is a numpy array for calculations\n",
    "    proportions = np.asarray(proportions)\n",
    "    \n",
    "    # Avoid division by zero if bin_size is zero\n",
    "    if bin_size <= 0:\n",
    "        raise ValueError(\"bin_size must be positive.\")\n",
    "        \n",
    "    binned_props = np.round(proportions / bin_size) * bin_size\n",
    "    total = np.sum(binned_props)\n",
    "\n",
    "    # Normalize only if the sum is significantly different from 1.0\n",
    "    # Use a small tolerance (epsilon) for floating point comparison\n",
    "    epsilon = 1e-6\n",
    "    if abs(total - 1.0) > 0.1 and abs(total) > epsilon: # Avoid division by near-zero total\n",
    "        print(f\"Warning: Sum of binned proportions ({total:.4f}) deviates > 0.1 from 1.0. Normalizing.\")\n",
    "        binned_props = binned_props / total\n",
    "    elif abs(total) <= epsilon and len(binned_props) > 0:\n",
    "         print(\"Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\")\n",
    "         # Return original binned props or zeros, depending on desired behavior\n",
    "         # Returning zeros might be safer if normalization is expected\n",
    "         # return np.zeros_like(binned_props)\n",
    "\n",
    "    return binned_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing for LSTM (Minor logging added)\n",
    "def process_transformer_predictions(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process transformer predictions for LSTM model input.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed data dictionary containing padded sequences, masks, etc.\n",
    "    \"\"\"\n",
    "    df = load_and_validate_data(transformer_predictions_file)\n",
    "    sequences = df['Sequence'].unique()\n",
    "    print(f\"Found {len(sequences)} unique sequences in the data.\")\n",
    "\n",
    "    X_data, y_increments, y_cumulative, masks = [], [], [], []\n",
    "    prop_scaler, inc_scaler, cum_scaler = StandardScaler(), StandardScaler(), StandardScaler()\n",
    "    processed_sequence_ids = [] # Keep track of sequences successfully processed\n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_data = df[df['Sequence'] == seq_id].sort_values('Step')\n",
    "        seq_len = len(seq_data)\n",
    "        if seq_len == 0:\n",
    "            print(f\"Warning: Sequence {seq_id} has no data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            binned_proportions = bin_proportions(seq_data['Predicted_Proportion'].values)\n",
    "            scaled_props = prop_scaler.fit_transform(binned_proportions.reshape(-1, 1)).flatten()\n",
    "            # Handle potential NaNs introduced by coerce during loading before scaling\n",
    "            gt_inc_valid = seq_data['GroundTruth_Increment'].dropna()\n",
    "            gt_cum_valid = seq_data['GroundTruth_Cumulative'].dropna()\n",
    "            if gt_inc_valid.empty or gt_cum_valid.empty:\n",
    "                 print(f\"Warning: Sequence {seq_id} has only NaN ground truth values after coercion. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            scaled_gt_inc = inc_scaler.fit_transform(gt_inc_valid.values.reshape(-1, 1)).flatten()\n",
    "            scaled_gt_cum = cum_scaler.fit_transform(gt_cum_valid.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "            # Ensure lengths match after potential dropna (this indicates an issue if they don't)\n",
    "            if len(scaled_gt_inc) != seq_len or len(scaled_gt_cum) != seq_len:\n",
    "                 print(f\"Warning: Length mismatch after handling NaNs in sequence {seq_id}. Check input data. Skipping.\")\n",
    "                 # This case needs careful handling - maybe impute NaNs instead of dropping?\n",
    "                 # For now, skipping the sequence if lengths don't match after dropna.\n",
    "                 continue\n",
    "\n",
    "\n",
    "            max_step = seq_data['Step'].max()\n",
    "            normalized_step = seq_data['Step'].values / max_step if max_step > 0 else np.zeros(seq_len)\n",
    "\n",
    "            features = np.column_stack([scaled_props, scaled_gt_inc, scaled_gt_cum, normalized_step])\n",
    "            increments = seq_data['GroundTruth_Increment'].values # Use original values with potential NaNs for target\n",
    "            cumulative = seq_data['GroundTruth_Cumulative'].values # Use original values with potential NaNs for target\n",
    "\n",
    "            X_data.append(features)\n",
    "            y_increments.append(increments)\n",
    "            y_cumulative.append(cumulative)\n",
    "            masks.append(np.ones(seq_len))\n",
    "            processed_sequence_ids.append(seq_id) # Add ID if processed successfully\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sequence {seq_id}: {e}. Skipping sequence.\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    if not X_data:\n",
    "         raise ValueError(\"No valid sequences found after processing. Cannot proceed.\")\n",
    "\n",
    "    num_processed_sequences = len(X_data)\n",
    "    print(f\"Successfully processed {num_processed_sequences} sequences.\")\n",
    "\n",
    "    max_length = max(len(x) for x in X_data)\n",
    "    num_features = X_data[0].shape[1]\n",
    "    print(f\"Padding sequences to max length: {max_length}\")\n",
    "\n",
    "    # Initialize padded arrays - use np.nan as fill value for targets might be better\n",
    "    X_padded = np.zeros((num_processed_sequences, max_length, num_features), dtype=np.float32)\n",
    "    y_increments_padded = np.full((num_processed_sequences, max_length), np.nan, dtype=np.float32)\n",
    "    y_cumulative_padded = np.full((num_processed_sequences, max_length), np.nan, dtype=np.float32)\n",
    "    masks_padded = np.zeros((num_processed_sequences, max_length), dtype=np.float32)\n",
    "\n",
    "    for i in range(num_processed_sequences):\n",
    "        seq_len = len(X_data[i])\n",
    "        X_padded[i, :seq_len, :] = X_data[i]\n",
    "        y_increments_padded[i, :seq_len] = y_increments[i]\n",
    "        y_cumulative_padded[i, :seq_len] = y_cumulative[i]\n",
    "        # Mask should be 0 where target is NaN, 1 otherwise\n",
    "        masks_padded[i, :seq_len] = (~np.isnan(y_increments[i])).astype(np.float32) # Example: mask based on increment NaNs\n",
    "\n",
    "    # Recalculate total times based on the potentially NaN-filled padded ground truth\n",
    "    y_total_times = []\n",
    "    for i in range(num_processed_sequences):\n",
    "        valid_indices = np.where(masks_padded[i] == 1)[0]\n",
    "        if len(valid_indices) > 0:\n",
    "            last_valid_index = valid_indices[-1]\n",
    "            y_total_times.append(y_cumulative_padded[i, last_valid_index])\n",
    "        else:\n",
    "            y_total_times.append(0) # Or np.nan if preferred\n",
    "    y_total_times = np.array(y_total_times, dtype=np.float32)\n",
    "\n",
    "    # Replace NaNs in target arrays with 0 for training (loss function needs numbers)\n",
    "    # The mask will handle ignoring these steps during loss calculation.\n",
    "    y_increments_padded = np.nan_to_num(y_increments_padded, nan=0.0)\n",
    "    y_cumulative_padded = np.nan_to_num(y_cumulative_padded, nan=0.0)\n",
    "    # y_total_times should already be numeric based on calculation above\n",
    "\n",
    "    print(\"Data processing and padding complete.\")\n",
    "    return {\n",
    "        'X': X_padded,\n",
    "        'y_increments': y_increments_padded,\n",
    "        'y_cumulative': y_cumulative_padded,\n",
    "        'y_total_times': y_total_times,\n",
    "        'masks': masks_padded, # Crucial: Mask reflects original NaNs\n",
    "        'sequences': processed_sequence_ids, # Use the list of successfully processed IDs\n",
    "        'df': df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Masked Loss Function (Updated mask handling logic)\n",
    "def custom_masked_huber_loss(mask_tensor, delta=1.0):\n",
    "    \"\"\"\n",
    "    Factory function to create a masked Huber loss.\n",
    "    Mask should be 1 for valid steps, 0 for padded/invalid steps.\n",
    "\n",
    "    Args:\n",
    "        mask_tensor (tf.Tensor): The mask indicating valid time steps (batch_size, seq_len).\n",
    "                                 Passed during training setup.\n",
    "        delta (float): Huber loss delta parameter.\n",
    "\n",
    "    Returns:\n",
    "        Callable: A loss function `masked_huber(y_true, y_pred)`.\n",
    "    \"\"\"\n",
    "    # Convert the persistent mask tensor to float32 once\n",
    "    mask_float32 = tf.cast(mask_tensor, tf.float32)\n",
    "\n",
    "    def masked_huber(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate Huber loss only for valid (masked == 1) time steps.\n",
    "\n",
    "        Args:\n",
    "            y_true (tf.Tensor): Ground truth tensor (batch_size, seq_len).\n",
    "            y_pred (tf.Tensor): Prediction tensor (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Scalar mean loss over the batch.\n",
    "        \"\"\"\n",
    "        # Slice the persistent mask for the current batch size dynamically\n",
    "        current_batch_size = tf.shape(y_true)[0]\n",
    "        # Assume mask_tensor corresponds row-wise to the full dataset\n",
    "        mask_batch = mask_float32[:current_batch_size]\n",
    "\n",
    "        # Ensure mask_batch has compatible shape (e.g., (batch_size, seq_len))\n",
    "        # This might require reshaping or broadcasting depending on exact shapes\n",
    "        # Assuming y_true, y_pred, mask_batch are all (batch_size, seq_len)\n",
    "\n",
    "        # Huber loss calculation\n",
    "        error = y_true - y_pred\n",
    "        abs_error = tf.abs(error)\n",
    "        quadratic = tf.minimum(abs_error, delta)\n",
    "        linear = abs_error - quadratic\n",
    "        huber_loss = 0.5 * quadratic**2 + delta * linear\n",
    "\n",
    "        # Apply the mask (element-wise multiplication)\n",
    "        masked_loss = huber_loss * mask_batch\n",
    "\n",
    "        # Calculate mean loss per sequence, avoiding division by zero\n",
    "        # Sum loss over sequence length dimension\n",
    "        sum_loss_per_sequence = tf.reduce_sum(masked_loss, axis=1)\n",
    "        # Sum mask over sequence length dimension to get count of valid tokens\n",
    "        valid_tokens_per_sequence = tf.reduce_sum(mask_batch, axis=1)\n",
    "\n",
    "        # Avoid division by zero for sequences with no valid tokens\n",
    "        # Replace zero counts with 1; the corresponding sum_loss will also be 0.\n",
    "        valid_tokens_safe = tf.where(valid_tokens_per_sequence == 0,\n",
    "                                     tf.ones_like(valid_tokens_per_sequence),\n",
    "                                     valid_tokens_per_sequence)\n",
    "\n",
    "        mean_loss_per_sequence = sum_loss_per_sequence / valid_tokens_safe\n",
    "\n",
    "        # Return the mean loss across the batch\n",
    "        return tf.reduce_mean(mean_loss_per_sequence)\n",
    "\n",
    "    # Assign a name for clarity in logs/history if possible\n",
    "    masked_huber.__name__ = f'masked_huber_delta_{delta}'\n",
    "    return masked_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved LSTM Model Definition\n",
    "class ImprovedTimeDiffLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    LSTM model with enhancements like LayerNorm, Attention, Residuals, and Regularization.\n",
    "    Predicts time differences, total time, and calculates cumulative times.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=64, dropout_rate=0.3, num_attention_heads=4, l2_reg=0.001):\n",
    "        \"\"\"\n",
    "        Initialize the model layers.\n",
    "\n",
    "        Args:\n",
    "            hidden_units (int): Number of units in the main LSTM layer.\n",
    "            dropout_rate (float): Dropout rate for LSTM and Attention layers.\n",
    "            num_attention_heads (int): Number of heads for MultiHeadAttention.\n",
    "            l2_reg (float): L2 regularization factor for the first LSTM kernel.\n",
    "        \"\"\"\n",
    "        super().__init__() # Use super().__init__() for Python 3 style\n",
    "\n",
    "        # Input normalization\n",
    "        self.input_normalization = layers.LayerNormalization(name=\"InputNorm\")\n",
    "\n",
    "        # --- LSTM Block ---\n",
    "        # First LSTM layer with regularization and return sequences\n",
    "        self.lstm_layer1 = layers.LSTM(\n",
    "            hidden_units,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate, # Be cautious with recurrent_dropout on GPU\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            name=\"LSTM1\"\n",
    "        )\n",
    "        # Dense layer for residual connection matching dimensions\n",
    "        # Ensure it outputs `hidden_units` to match lstm_layer1 output\n",
    "        self.residual_dense = layers.Dense(hidden_units, activation='relu', name=\"ResidualDense\")\n",
    "        self.add_layer1 = layers.Add(name=\"AddResidual1\") # Explicit Add layer\n",
    "        self.norm_layer1 = layers.LayerNormalization(name=\"Norm1\") # Normalize after residual\n",
    "\n",
    "        # Second LSTM layer\n",
    "        self.lstm_layer2 = layers.LSTM(\n",
    "            hidden_units // 2, # Reduce dimensionality\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate,\n",
    "            name=\"LSTM2\"\n",
    "        )\n",
    "\n",
    "        # --- Attention Block ---\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_attention_heads,\n",
    "            key_dim=hidden_units // 2, # Key dim often matches the query/value dim\n",
    "            dropout=dropout_rate,\n",
    "            name=\"MultiHeadAttention\"\n",
    "        )\n",
    "        self.add_layer2 = layers.Add(name=\"AddResidual2\") # Add attention output to LSTM output\n",
    "        self.norm_layer2 = layers.LayerNormalization(name=\"Norm2\") # Normalize after attention\n",
    "\n",
    "        # --- Output Heads ---\n",
    "        # 1. Time Difference Head (predicts increment for each step)\n",
    "        # Use 'relu' to enforce non-negativity for time differences\n",
    "        self.time_diff_head = layers.Dense(1, activation='relu', name='TimeDiffHead')\n",
    "\n",
    "        # 2. Total Time Head (predicts total duration for the sequence)\n",
    "        # Takes the aggregated sequence representation after attention\n",
    "        # Use 'softplus' or 'relu' for non-negativity\n",
    "        self.total_time_head = layers.Dense(1, activation='softplus', name='TotalTimeHead')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor (batch_size, seq_len, num_features).\n",
    "            training (bool): Whether the model is in training mode (for dropout).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (predicted_time_diffs, predicted_total_time, predicted_cumulative_times)\n",
    "        \"\"\"\n",
    "        # Input normalization\n",
    "        x = self.input_normalization(inputs)\n",
    "\n",
    "        # --- LSTM Block with Residual ---\n",
    "        lstm_out1 = self.lstm_layer1(x, training=training)\n",
    "        residual = self.residual_dense(x) # Project input for residual connection\n",
    "        lstm_out1 = self.add_layer1([lstm_out1, residual])\n",
    "        lstm_out1 = self.norm_layer1(lstm_out1) # Normalize after adding\n",
    "\n",
    "        # Second LSTM layer\n",
    "        lstm_out2 = self.lstm_layer2(lstm_out1, training=training)\n",
    "\n",
    "        # --- Attention Block with Residual ---\n",
    "        # Use lstm_out2 as query, key, and value for self-attention\n",
    "        attn_out = self.attention(query=lstm_out2, value=lstm_out2, key=lstm_out2, training=training)\n",
    "        # Add residual connection from before attention\n",
    "        attn_out = self.add_layer2([attn_out, lstm_out2])\n",
    "        attn_out = self.norm_layer2(attn_out) # Normalize after adding\n",
    "\n",
    "        # --- Output Generation ---\n",
    "        # 1. Time Differences (per step)\n",
    "        # Apply the dense head to the output of the attention block\n",
    "        time_diffs = self.time_diff_head(attn_out)\n",
    "        # Remove the last dimension (Dense adds a dim of 1)\n",
    "        time_diffs = tf.squeeze(time_diffs, axis=-1) # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # 2. Total Time (per sequence)\n",
    "        # Aggregate the sequence information (e.g., mean pooling over time)\n",
    "        sequence_encoding = tf.reduce_mean(attn_out, axis=1) # Shape: (batch_size, hidden_units//2)\n",
    "        total_time = self.total_time_head(sequence_encoding) # Shape: (batch_size, 1)\n",
    "\n",
    "        # 3. Cumulative Times (calculated from predicted differences)\n",
    "        # Ensure calculation happens correctly even if time_diffs has NaNs (shouldn't if using relu)\n",
    "        cumulative_times = tf.cumsum(time_diffs, axis=1) # Shape: (batch_size, seq_len)\n",
    "\n",
    "        return time_diffs, total_time, cumulative_times\n",
    "\n",
    "    # Optional: Build method to define input shape explicitly\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Function (Added build confirmation)\n",
    "def train_improved_lstm(transformer_predictions_file, epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Preprocess data, build, compile, and train the improved LSTM model.\n",
    "\n",
    "    Args:\n",
    "        transformer_predictions_file (str): Path to the transformer predictions CSV.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Training batch size.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained_model, training_history, processed_data)\n",
    "    \"\"\"\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Starting LSTM Model Training Process\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 1. Process Data\n",
    "    print(\"Processing data...\")\n",
    "    data = process_transformer_predictions(transformer_predictions_file)\n",
    "    X_train = data['X']\n",
    "    y_inc_train = data['y_increments']\n",
    "    y_cum_train = data['y_cumulative']\n",
    "    y_total_train = data['y_total_times']\n",
    "    masks_train = data['masks']\n",
    "\n",
    "    if X_train.shape[0] == 0:\n",
    "        print(\"Error: No data available for training after processing.\")\n",
    "        return None, None, data\n",
    "\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    print(f\"Input shape for LSTM: {input_shape}\")\n",
    "    print(f\"Number of sequences for training/validation: {X_train.shape[0]}\")\n",
    "    print(f\"Target shapes: Increments {y_inc_train.shape}, Cumulative {y_cum_train.shape}, Total {y_total_train.shape}\")\n",
    "    print(f\"Mask shape: {masks_train.shape}, Mask sum (total valid steps): {np.sum(masks_train)}\")\n",
    "\n",
    "    # 2. Build Model\n",
    "    print(\"Building model...\")\n",
    "    lstm_model = ImprovedTimeDiffLSTM()\n",
    "    # Explicitly build the model\n",
    "    lstm_model.build(input_shape=(None,) + input_shape)\n",
    "    # *** Add confirmation print ***\n",
    "    print(f\"Model built status: {lstm_model.built}\")\n",
    "    # Print summary *after* building\n",
    "    lstm_model.summary(line_length=100)\n",
    "\n",
    "    # 3. Define Loss Functions\n",
    "    print(\"Defining loss functions...\")\n",
    "    masked_huber_loss_fn = custom_masked_huber_loss(tf.constant(masks_train, dtype=tf.float32))\n",
    "    total_time_loss_fn = tf.keras.losses.MeanSquaredError(name='total_time_mse')\n",
    "    loss_functions = [masked_huber_loss_fn, total_time_loss_fn, masked_huber_loss_fn]\n",
    "    loss_weights = [0.4, 0.3, 0.3]\n",
    "    print(f\"Loss functions set. Weights: {loss_weights}\")\n",
    "\n",
    "    # 4. Define Optimizer\n",
    "    print(\"Configuring optimizer...\")\n",
    "    learning_rate_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.001, decay_steps=1000, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    optimizer = AdamW(learning_rate=learning_rate_schedule, weight_decay=0.001)\n",
    "\n",
    "    # 5. Compile Model\n",
    "    print(\"Compiling model...\")\n",
    "    lstm_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_functions,\n",
    "        loss_weights=loss_weights,\n",
    "    )\n",
    "    print(\"Model compiled successfully.\")\n",
    "\n",
    "    # 6. Define Callbacks\n",
    "    print(\"Configuring callbacks...\")\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 7. Train Model\n",
    "    print(f\"Starting training for {epochs} epochs with batch size {batch_size}...\")\n",
    "    history = lstm_model.fit(\n",
    "        X_train,\n",
    "        [y_inc_train, y_total_train, y_cum_train],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Training finished.\")\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history.get('val_loss', [np.nan])[-1]\n",
    "    print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    return lstm_model, history, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Generation Function (Changed output filename)\n",
    "def generate_lstm_predictions_from_transformer_csv(lstm_model, data):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained LSTM model and compare with transformer.\n",
    "\n",
    "    Args:\n",
    "        lstm_model (tf.keras.Model): The trained LSTM model.\n",
    "        data (dict): The processed data dictionary from `process_transformer_predictions`.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe containing original data, transformer predictions,\n",
    "                          LSTM predictions, and comparison metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Generating LSTM predictions...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    X_predict = data['X']\n",
    "    original_df = data['df']\n",
    "    processed_sequences = data['sequences']\n",
    "    masks_predict = data['masks']\n",
    "\n",
    "    if X_predict.shape[0] == 0:\n",
    "        print(\"Error: No data available for prediction.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Predicting on {X_predict.shape[0]} sequences...\")\n",
    "    try:\n",
    "        time_diffs_pred, total_time_pred, cumulative_times_pred = lstm_model.predict(X_predict)\n",
    "        print(\"Model prediction successful.\")\n",
    "        print(f\"Predicted shapes: Diffs {time_diffs_pred.shape}, Total {total_time_pred.shape}, Cumul {cumulative_times_pred.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model prediction: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    results_list = []\n",
    "    print(f\"Processing predictions for {len(processed_sequences)} sequences...\")\n",
    "\n",
    "    for seq_idx, seq_id in enumerate(processed_sequences):\n",
    "        seq_original_data = original_df[original_df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        seq_len = int(np.sum(masks_predict[seq_idx]))\n",
    "\n",
    "        if seq_len == 0:\n",
    "            print(f\"Warning: Sequence {seq_id} (index {seq_idx}) has length 0 in mask during prediction. Skipping.\")\n",
    "            continue\n",
    "        if seq_len > time_diffs_pred.shape[1]:\n",
    "             print(f\"Warning: Sequence {seq_id} (index {seq_idx}) mask length ({seq_len}) exceeds prediction dimension ({time_diffs_pred.shape[1]}). Clamping length.\")\n",
    "             seq_len = time_diffs_pred.shape[1]\n",
    "\n",
    "        current_time_diffs = time_diffs_pred[seq_idx, :seq_len]\n",
    "        current_cumulative = cumulative_times_pred[seq_idx, :seq_len]\n",
    "        current_total = total_time_pred[seq_idx, 0]\n",
    "\n",
    "        if len(seq_original_data) == seq_len:\n",
    "            seq_original_data['LSTM_Predicted_TimeDiff'] = current_time_diffs\n",
    "            seq_original_data['LSTM_Predicted_Cumulative'] = current_cumulative\n",
    "            seq_original_data['LSTM_Predicted_TotalTime'] = current_total\n",
    "        elif len(seq_original_data) > seq_len:\n",
    "             print(f\"Warning: Original data length ({len(seq_original_data)}) > mask length ({seq_len}) for seq {seq_id}. Padding predictions.\")\n",
    "             padded_diffs = np.pad(current_time_diffs, (0, len(seq_original_data) - seq_len), constant_values=np.nan)\n",
    "             padded_cumul = np.pad(current_cumulative, (0, len(seq_original_data) - seq_len), constant_values=np.nan)\n",
    "             seq_original_data['LSTM_Predicted_TimeDiff'] = padded_diffs\n",
    "             seq_original_data['LSTM_Predicted_Cumulative'] = padded_cumul\n",
    "             seq_original_data['LSTM_Predicted_TotalTime'] = current_total\n",
    "        else:\n",
    "             print(f\"Error: Original data length ({len(seq_original_data)}) < mask length ({seq_len}) for seq {seq_id}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        required_orig_cols = ['GroundTruth_Increment', 'Predicted_Increment',\n",
    "                              'GroundTruth_Cumulative', 'Predicted_Cumulative']\n",
    "        if not all(col in seq_original_data for col in required_orig_cols):\n",
    "             print(f\"Warning: Missing required original prediction columns for sequence {seq_id}. Cannot calculate improvements.\")\n",
    "             seq_original_data['TimeDiff_Improvement_Abs'] = np.nan\n",
    "             seq_original_data['Cumulative_Improvement_Abs'] = np.nan\n",
    "        else:\n",
    "            transformer_diff_error = abs(seq_original_data['GroundTruth_Increment'] - seq_original_data['Predicted_Increment'])\n",
    "            lstm_diff_error = abs(seq_original_data['GroundTruth_Increment'] - seq_original_data['LSTM_Predicted_TimeDiff'])\n",
    "            transformer_cum_error = abs(seq_original_data['GroundTruth_Cumulative'] - seq_original_data['Predicted_Cumulative'])\n",
    "            lstm_cum_error = abs(seq_original_data['GroundTruth_Cumulative'] - seq_original_data['LSTM_Predicted_Cumulative'])\n",
    "            seq_original_data['TimeDiff_Improvement_Abs'] = transformer_diff_error - lstm_diff_error\n",
    "            seq_original_data['Cumulative_Improvement_Abs'] = transformer_cum_error - lstm_cum_error\n",
    "\n",
    "        results_list.append(seq_original_data)\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"Error: No results generated after processing predictions.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    results_df = pd.concat(results_list, ignore_index=True)\n",
    "    print(f\"Finished processing predictions. Combined results shape: {results_df.shape}\")\n",
    "    print(\"Combined results head:\\n\", results_df.head())\n",
    "    print(\"Combined results tail:\\n\", results_df.tail())\n",
    "    print(\"Checking for NaNs in LSTM predictions:\")\n",
    "    print(results_df[['LSTM_Predicted_TimeDiff', 'LSTM_Predicted_Cumulative', 'LSTM_Predicted_TotalTime']].isnull().sum())\n",
    "\n",
    "    # *** Changed output filename ***\n",
    "    output_filename = 'predictions_lstm_refined_175974.csv'\n",
    "    output_path = os.path.abspath(output_filename)\n",
    "    try:\n",
    "        results_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Combined predictions successfully saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to CSV ({output_path}): {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Overall Model Performance Comparison (Mean Absolute Error)\")\n",
    "    print(\"-\" * 30)\n",
    "    comparison_cols = required_orig_cols + ['LSTM_Predicted_TimeDiff', 'LSTM_Predicted_Cumulative']\n",
    "    valid_results = results_df.dropna(subset=comparison_cols)\n",
    "    print(f\"Calculating MAE based on {len(valid_results)} valid rows (after dropping NaNs in comparison columns).\")\n",
    "\n",
    "    if valid_results.empty:\n",
    "        print(\"Warning: No valid data points found for calculating overall MAE.\")\n",
    "    else:\n",
    "        try:\n",
    "            transformer_time_diff_mae = np.mean(abs(valid_results['GroundTruth_Increment'] - valid_results['Predicted_Increment']))\n",
    "            lstm_time_diff_mae = np.mean(abs(valid_results['GroundTruth_Increment'] - valid_results['LSTM_Predicted_TimeDiff']))\n",
    "            transformer_cumulative_mae = np.mean(abs(valid_results['GroundTruth_Cumulative'] - valid_results['Predicted_Cumulative']))\n",
    "            lstm_cumulative_mae = np.mean(abs(valid_results['GroundTruth_Cumulative'] - valid_results['LSTM_Predicted_Cumulative']))\n",
    "\n",
    "            time_diff_improvement_pct = (1 - lstm_time_diff_mae / transformer_time_diff_mae) * 100 if transformer_time_diff_mae != 0 else float('inf') if lstm_time_diff_mae < transformer_time_diff_mae else 0\n",
    "            cumulative_improvement_pct = (1 - lstm_cumulative_mae / transformer_cumulative_mae) * 100 if transformer_cumulative_mae != 0 else float('inf') if lstm_cumulative_mae < transformer_cumulative_mae else 0\n",
    "\n",
    "            print(f\"Time Differences MAE:\")\n",
    "            print(f\"  Transformer: {transformer_time_diff_mae:.4f}\")\n",
    "            print(f\"  LSTM:        {lstm_time_diff_mae:.4f}\")\n",
    "            print(f\"  Improvement: {time_diff_improvement_pct:.2f}%\")\n",
    "            print(f\"\\nCumulative Times MAE:\")\n",
    "            print(f\"  Transformer: {transformer_cumulative_mae:.4f}\")\n",
    "            print(f\"  LSTM:        {lstm_cumulative_mae:.4f}\")\n",
    "            print(f\"  Improvement: {cumulative_improvement_pct:.2f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating overall MAE: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM model training, prediction, and visualization.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\" LSTM Model Training and Evaluation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # --- Configuration ---\n",
    "    transformer_predictions_file = \"predictions_transformer.csv\"\n",
    "    training_epochs = 100\n",
    "    training_batch_size = 32\n",
    "    output_suffix = \"_runFinal182625\" # Suffix for output files\n",
    "\n",
    "    lstm_model = None\n",
    "    history = None\n",
    "    data = None\n",
    "    results_df = None\n",
    "\n",
    "    try:\n",
    "        # --- Step 1: Train LSTM Model ---\n",
    "        print(\"\\n--- Step 1: Training LSTM Model ---\")\n",
    "        lstm_model, history, data = train_improved_lstm(\n",
    "            transformer_predictions_file,\n",
    "            epochs=training_epochs,\n",
    "            batch_size=training_batch_size\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Generate Predictions ---\n",
    "        print(\"\\n--- Step 2: Generating LSTM Predictions ---\")\n",
    "        if lstm_model is None or data is None:\n",
    "             print(\"Model training or data processing failed. Cannot generate predictions.\")\n",
    "        else:\n",
    "            # Pass suffix to prediction function if needed, or handle filenames inside main\n",
    "            results_df = generate_lstm_predictions_from_transformer_csv(lstm_model, data) # Filename handled inside\n",
    "\n",
    "        # --- Step 3: Display Sample Results ---\n",
    "        print(\"\\n--- Step 3: Displaying Sample Results ---\")\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            print(\"\\nSample Combined Predictions (Head):\")\n",
    "            display_cols = ['Sequence', 'Step', 'SourceID',\n",
    "                            'GroundTruth_Increment', 'Predicted_Increment', 'LSTM_Predicted_TimeDiff',\n",
    "                            'GroundTruth_Cumulative','Predicted_Cumulative','LSTM_Predicted_Cumulative',\n",
    "                            'TimeDiff_Improvement_Abs', 'Cumulative_Improvement_Abs']\n",
    "            display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "            print(results_df[display_cols].head(10).to_string())\n",
    "        else:\n",
    "            print(\"No results dataframe generated or it is empty.\")\n",
    "\n",
    "        # --- Step 4: Generate Visualizations ---\n",
    "        print(\"\\n--- Step 4: Generating Visualizations ---\")\n",
    "        # Pass suffix to visualization function if needed, or handle filenames inside main\n",
    "        visualize_results(results_df, history, num_samples=3) # Filename handled inside\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" Pipeline Execution Completed\")\n",
    "        # Provide paths to output files with the new suffix\n",
    "        print(f\"  - Combined predictions CSV saved to: {os.path.abspath(f'predictions_lstm_refined{output_suffix}.csv')}\")\n",
    "        print(f\"  - Training performance plot saved to: {os.path.abspath(f'training_performance{output_suffix}.png')}\")\n",
    "        print(f\"  - Sequence comparison plots saved as: sequence_<ID>_comparison{output_suffix}.png\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"\\nFatal Error: {fnf_error}\")\n",
    "        print(\"Please ensure the input CSV file exists and the path is correct.\")\n",
    "    except ValueError as val_error:\n",
    "        print(f\"\\nFatal Error: {val_error}\")\n",
    "        print(\"Please check the data format, required columns, and content in the input file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected fatal error occurred in main execution: {e}\")\n",
    "        print(\"\\n--- Traceback ---\")\n",
    "        traceback.print_exc()\n",
    "        print(\"--- End Traceback ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Function (Changed output filenames)\n",
    "def visualize_results(results_df, history, num_samples=3):\n",
    "    \"\"\"\n",
    "    Generate visualizations for training history and prediction comparisons.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): Dataframe with combined predictions.\n",
    "        history (tf.keras.callbacks.History): Training history object.\n",
    "        num_samples (int): Number of sample sequences to plot.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"-\" * 30)\n",
    "    print(\"Generating visualizations...\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- Plot Suffix ---\n",
    "    file_suffix = \"_run2\" # Suffix for plot filenames\n",
    "\n",
    "    if history is None:\n",
    "        print(\"Warning: Training history object is None. Skipping history plots.\")\n",
    "    else:\n",
    "        # --- 1. Plot Training History ---\n",
    "        try:\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history.history['loss'], label='Training Loss')\n",
    "            if 'val_loss' in history.history:\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            else:\n",
    "                 print(\"Warning: 'val_loss' not found in history.\")\n",
    "            plt.title('Model Loss During Training')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss (Weighted Sum)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plotted_specific_loss = False\n",
    "            loss_keys = [k for k in history.history.keys() if 'loss' in k and k != 'loss' and k != 'val_loss']\n",
    "            if loss_keys:\n",
    "                 for key in loss_keys[:1]:\n",
    "                     val_key = 'val_' + key\n",
    "                     plt.plot(history.history[key], label=f'Train {key}')\n",
    "                     if val_key in history.history:\n",
    "                         plt.plot(history.history[val_key], label=f'Val {key}')\n",
    "                     plt.title(f'{key.replace(\"_\",\" \").title()}')\n",
    "                     plotted_specific_loss = True\n",
    "                 plt.xlabel('Epoch')\n",
    "                 plt.ylabel('Loss')\n",
    "                 plt.legend()\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, 'No specific loss keys found', horizontalalignment='center', verticalalignment='center')\n",
    "                plt.title('Additional Training Metrics')\n",
    "                plt.xlabel('Epoch')\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            # *** Changed output filename ***\n",
    "            output_filename = f'training_performance{file_suffix}.png'\n",
    "            output_path = os.path.abspath(output_filename)\n",
    "            print(f\"Attempting to save training performance plot to: {output_path}\")\n",
    "            plt.savefig(output_filename)\n",
    "            print(f\"Successfully saved training performance plot.\")\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating or saving training history plot: {e}\")\n",
    "            traceback.print_exc()\n",
    "            plt.close()\n",
    "\n",
    "    # --- 2. Plot Prediction Comparison for Sample Sequences ---\n",
    "    if results_df is None or results_df.empty:\n",
    "        print(\"Warning: Results dataframe is empty or None. Skipping prediction comparison plots.\")\n",
    "        return\n",
    "\n",
    "    required_lstm_cols = ['LSTM_Predicted_TimeDiff', 'LSTM_Predicted_Cumulative']\n",
    "    if not all(col in results_df.columns for col in required_lstm_cols):\n",
    "        print(f\"Warning: Missing required LSTM prediction columns ({required_lstm_cols}) in results_df. Skipping comparison plots.\")\n",
    "        return\n",
    "    if results_df[required_lstm_cols].isnull().all().all():\n",
    "         print(f\"Warning: LSTM prediction columns ({required_lstm_cols}) contain only NaN values. Skipping comparison plots.\")\n",
    "         return\n",
    "\n",
    "    sequences_to_plot = results_df['Sequence'].unique()\n",
    "    if len(sequences_to_plot) == 0:\n",
    "        print(\"No unique sequences found in results_df to plot.\")\n",
    "        return\n",
    "\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(sequences_to_plot)\n",
    "    sample_sequences = sequences_to_plot[:min(num_samples, len(sequences_to_plot))]\n",
    "    print(f\"Attempting to plot prediction comparisons for {len(sample_sequences)} sample sequences: {sample_sequences}\")\n",
    "\n",
    "    plot_cols_increments = ['GroundTruth_Increment', 'Predicted_Increment', 'LSTM_Predicted_TimeDiff']\n",
    "    plot_cols_cumulative = ['GroundTruth_Cumulative', 'Predicted_Cumulative', 'LSTM_Predicted_Cumulative']\n",
    "    plot_labels = ['Ground Truth', 'Transformer Pred.', 'LSTM Pred.']\n",
    "    plot_styles = ['o-', 's--', '^-']\n",
    "\n",
    "    for seq_id in sample_sequences:\n",
    "        print(f\"\\nProcessing plot for Sequence ID: {seq_id}\")\n",
    "        seq_results = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "        if seq_results.empty:\n",
    "            print(f\"  Skipping plot for sequence {seq_id} as no data found in results_df.\")\n",
    "            continue\n",
    "        print(f\"  Data shape for sequence {seq_id}: {seq_results.shape}\")\n",
    "\n",
    "        try:\n",
    "            plt.figure(figsize=(15, 7))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            for i, col in enumerate(plot_cols_increments):\n",
    "                if col in seq_results.columns and not seq_results[col].isnull().all():\n",
    "                    plt.plot(seq_results['Step'], seq_results[col], plot_styles[i], label=plot_labels[i], alpha=0.8)\n",
    "                    print(f\"  Plotted '{col}' for increments.\")\n",
    "                else:\n",
    "                    print(f\"  Skipping plot for '{col}' (increments) - column missing or all NaN.\")\n",
    "            plt.title(f'Sequence {seq_id} - Time Increments')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Time Increment')\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            for i, col in enumerate(plot_cols_cumulative):\n",
    "                 if col in seq_results.columns and not seq_results[col].isnull().all():\n",
    "                    plt.plot(seq_results['Step'], seq_results[col], plot_styles[i], label=plot_labels[i], alpha=0.8)\n",
    "                    print(f\"  Plotted '{col}' for cumulative.\")\n",
    "                 else:\n",
    "                    print(f\"  Skipping plot for '{col}' (cumulative) - column missing or all NaN.\")\n",
    "            plt.title(f'Sequence {seq_id} - Cumulative Time')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Cumulative Time')\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            # *** Changed output filename ***\n",
    "            output_filename = f'sequence_{seq_id}_comparison{file_suffix}.png'\n",
    "            output_path = os.path.abspath(output_filename)\n",
    "            print(f\"  Attempting to save comparison plot to: {output_path}\")\n",
    "            plt.savefig(output_filename)\n",
    "            print(f\"  Successfully saved comparison plot for sequence {seq_id}.\")\n",
    "            plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating or saving comparison plot for sequence {seq_id}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            plt.close()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Visualization generation finished.\")\n",
    "    print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution Block\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM model training, prediction, and visualization.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\" LSTM Model Training and Evaluation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # --- Configuration ---\n",
    "    transformer_predictions_file = \"predictions_transformer_175974.csv\" # Ensure this file exists!\n",
    "    training_epochs = 100 # Use the original epoch count\n",
    "    training_batch_size = 32\n",
    "\n",
    "    lstm_model = None\n",
    "    history = None\n",
    "    data = None\n",
    "    results_df = None\n",
    "\n",
    "    try:\n",
    "        # --- Step 1: Train LSTM Model ---\n",
    "        print(\"\\n--- Step 1: Training LSTM Model ---\")\n",
    "        lstm_model, history, data = train_improved_lstm(\n",
    "            transformer_predictions_file,\n",
    "            epochs=training_epochs,\n",
    "            batch_size=training_batch_size\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Generate Predictions ---\n",
    "        print(\"\\n--- Step 2: Generating LSTM Predictions ---\")\n",
    "        if lstm_model is None or data is None:\n",
    "             print(\"Model training or data processing failed. Cannot generate predictions.\")\n",
    "             # Optionally: try loading a pre-trained model if available\n",
    "        else:\n",
    "            results_df = generate_lstm_predictions_from_transformer_csv(lstm_model, data)\n",
    "\n",
    "        # --- Step 3: Display Sample Results ---\n",
    "        print(\"\\n--- Step 3: Displaying Sample Results ---\")\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            print(\"\\nSample Combined Predictions (Head):\")\n",
    "            display_cols = ['Sequence', 'Step', 'SourceID',\n",
    "                            'GroundTruth_Increment', 'Predicted_Increment', 'LSTM_Predicted_TimeDiff',\n",
    "                            'GroundTruth_Cumulative','Predicted_Cumulative','LSTM_Predicted_Cumulative',\n",
    "                            'TimeDiff_Improvement_Abs', 'Cumulative_Improvement_Abs']\n",
    "            display_cols = [col for col in display_cols if col in results_df.columns] # Filter existing columns\n",
    "            print(results_df[display_cols].head(10).to_string())\n",
    "        else:\n",
    "            print(\"No results dataframe generated or it is empty.\")\n",
    "\n",
    "        # --- Step 4: Generate Visualizations ---\n",
    "        print(\"\\n--- Step 4: Generating Visualizations ---\")\n",
    "        # Pass history and results_df even if they are None/empty, the function handles it\n",
    "        visualize_results(results_df, history, num_samples=3)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\" Pipeline Execution Completed\")\n",
    "        # Provide paths to output files\n",
    "        print(f\"  - Combined predictions CSV saved to: {os.path.abspath('predictions_lstm_refined_1.csv')}\")\n",
    "        print(f\"  - Training performance plot saved to: {os.path.abspath('training_performance.png')}\")\n",
    "        print(\"  - Sequence comparison plots saved as: sequence_<ID>_comparison.png\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"\\nFatal Error: {fnf_error}\")\n",
    "        print(\"Please ensure the input CSV file exists and the path is correct.\")\n",
    "    except ValueError as val_error:\n",
    "        print(f\"\\nFatal Error: {val_error}\")\n",
    "        print(\"Please check the data format, required columns, and content in the input file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected fatal error occurred in main execution: {e}\")\n",
    "        print(\"\\n--- Traceback ---\")\n",
    "        traceback.print_exc()\n",
    "        print(\"--- End Traceback ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " LSTM Model Training and Evaluation Pipeline\n",
      "==================================================\n",
      "\n",
      "--- Step 1: Training LSTM Model ---\n",
      "------------------------------\n",
      "Starting LSTM Model Training Process\n",
      "------------------------------\n",
      "Processing data...\n",
      "Attempting to load data from: predictions_transformer_175651.csv\n",
      "Successfully loaded CSV. Shape: (2262, 8)\n",
      "Columns found: ['Sequence', 'Step', 'SourceID', 'Predicted_Proportion', 'Predicted_Increment', 'Predicted_Cumulative', 'GroundTruth_Increment', 'GroundTruth_Cumulative']\n",
      "All required columns are present.\n",
      "Data validation checks passed.\n",
      "Found 102 unique sequences in the data.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.8000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.6000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.3000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.4000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions is zero or near-zero. Cannot normalize.\n",
      "Warning: Sum of binned proportions (0.7000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1500) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.1000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (1.2000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Warning: Sum of binned proportions (0.5000) deviates > 0.1 from 1.0. Normalizing.\n",
      "Successfully processed 102 sequences.\n",
      "Padding sequences to max length: 54\n",
      "Data processing and padding complete.\n",
      "Input shape for LSTM: (54, 4)\n",
      "Number of sequences for training/validation: 102\n",
      "Target shapes: Increments (102, 54), Cumulative (102, 54), Total (102,)\n",
      "Mask shape: (102, 54), Mask sum (total valid steps): 2262.0\n",
      "Building model...\n",
      "Model built status: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"improved_time_diff_lstm_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"improved_time_diff_lstm_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                               </span><span style=\"font-weight: bold\"> Output Shape                    </span><span style=\"font-weight: bold\">           Param # </span>\n",
       "\n",
       " InputNorm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)              ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " LSTM1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                                ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " ResidualDense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " AddResidual1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                          ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " Norm1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                  ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " LSTM2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                                ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " MultiHeadAttention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)     ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " AddResidual2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                          ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " Norm2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                  ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " TimeDiffHead (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       " TotalTimeHead (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       ?                                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                              \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " InputNorm (\u001b[38;5;33mLayerNormalization\u001b[0m)              ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " LSTM1 (\u001b[38;5;33mLSTM\u001b[0m)                                ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " ResidualDense (\u001b[38;5;33mDense\u001b[0m)                       ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " AddResidual1 (\u001b[38;5;33mAdd\u001b[0m)                          ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " Norm1 (\u001b[38;5;33mLayerNormalization\u001b[0m)                  ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " LSTM2 (\u001b[38;5;33mLSTM\u001b[0m)                                ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " MultiHeadAttention (\u001b[38;5;33mMultiHeadAttention\u001b[0m)     ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " AddResidual2 (\u001b[38;5;33mAdd\u001b[0m)                          ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " Norm2 (\u001b[38;5;33mLayerNormalization\u001b[0m)                  ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " TimeDiffHead (\u001b[38;5;33mDense\u001b[0m)                        ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n",
       " TotalTimeHead (\u001b[38;5;33mDense\u001b[0m)                       ?                                      \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining loss functions...\n",
      "Loss functions set. Weights: [0.4, 0.3, 0.3]\n",
      "Configuring optimizer...\n",
      "Compiling model...\n",
      "Model compiled successfully.\n",
      "Configuring callbacks...\n",
      "Starting training for 100 epochs with batch size 32...\n",
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 299ms/step - loss: 140.5023 - masked_huber_delta_10_loss: 100.3728 - total_time_mse_loss: 0.1231 - val_loss: 185.5119 - val_masked_huber_delta_10_loss: 128.0700 - val_total_time_mse_loss: 0.0128\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 136.4797 - masked_huber_delta_10_loss: 96.6066 - total_time_mse_loss: 0.0376 - val_loss: 182.2976 - val_masked_huber_delta_10_loss: 124.9719 - val_total_time_mse_loss: 0.0124\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 133.5650 - masked_huber_delta_10_loss: 93.7995 - total_time_mse_loss: 0.0286 - val_loss: 180.6677 - val_masked_huber_delta_10_loss: 123.3679 - val_total_time_mse_loss: 0.0147\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 131.4601 - masked_huber_delta_10_loss: 91.7304 - total_time_mse_loss: 0.0306 - val_loss: 179.7974 - val_masked_huber_delta_10_loss: 122.4907 - val_total_time_mse_loss: 0.0165\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 130.0489 - masked_huber_delta_10_loss: 90.3267 - total_time_mse_loss: 0.0329 - val_loss: 179.1620 - val_masked_huber_delta_10_loss: 121.8703 - val_total_time_mse_loss: 0.0170\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 129.3261 - masked_huber_delta_10_loss: 89.5952 - total_time_mse_loss: 0.0339 - val_loss: 178.5178 - val_masked_huber_delta_10_loss: 121.2723 - val_total_time_mse_loss: 0.0155\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 128.5169 - masked_huber_delta_10_loss: 88.8613 - total_time_mse_loss: 0.0264 - val_loss: 177.8899 - val_masked_huber_delta_10_loss: 120.6954 - val_total_time_mse_loss: 0.0135\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 127.9783 - masked_huber_delta_10_loss: 88.3540 - total_time_mse_loss: 0.0260 - val_loss: 177.2751 - val_masked_huber_delta_10_loss: 120.1217 - val_total_time_mse_loss: 0.0123\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 127.4010 - masked_huber_delta_10_loss: 87.8374 - total_time_mse_loss: 0.0231 - val_loss: 176.7254 - val_masked_huber_delta_10_loss: 119.5860 - val_total_time_mse_loss: 0.0117\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 126.9735 - masked_huber_delta_10_loss: 87.3990 - total_time_mse_loss: 0.0233 - val_loss: 176.1646 - val_masked_huber_delta_10_loss: 119.0300 - val_total_time_mse_loss: 0.0107\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 126.7037 - masked_huber_delta_10_loss: 87.1631 - total_time_mse_loss: 0.0209 - val_loss: 175.7336 - val_masked_huber_delta_10_loss: 118.5903 - val_total_time_mse_loss: 0.0105\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 126.1154 - masked_huber_delta_10_loss: 86.5575 - total_time_mse_loss: 0.0244 - val_loss: 175.4015 - val_masked_huber_delta_10_loss: 118.2338 - val_total_time_mse_loss: 0.0104\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 125.5551 - masked_huber_delta_10_loss: 85.9875 - total_time_mse_loss: 0.0204 - val_loss: 175.1449 - val_masked_huber_delta_10_loss: 117.9551 - val_total_time_mse_loss: 0.0097\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 125.1874 - masked_huber_delta_10_loss: 85.5605 - total_time_mse_loss: 0.0211 - val_loss: 174.8838 - val_masked_huber_delta_10_loss: 117.6881 - val_total_time_mse_loss: 0.0088\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 124.8654 - masked_huber_delta_10_loss: 85.2437 - total_time_mse_loss: 0.0179 - val_loss: 174.6287 - val_masked_huber_delta_10_loss: 117.4363 - val_total_time_mse_loss: 0.0086\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 124.6524 - masked_huber_delta_10_loss: 85.0623 - total_time_mse_loss: 0.0192 - val_loss: 174.3853 - val_masked_huber_delta_10_loss: 117.1893 - val_total_time_mse_loss: 0.0084\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 124.1831 - masked_huber_delta_10_loss: 84.5559 - total_time_mse_loss: 0.0136 - val_loss: 174.1450 - val_masked_huber_delta_10_loss: 116.9431 - val_total_time_mse_loss: 0.0082\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 123.9609 - masked_huber_delta_10_loss: 84.3325 - total_time_mse_loss: 0.0148 - val_loss: 173.9058 - val_masked_huber_delta_10_loss: 116.6971 - val_total_time_mse_loss: 0.0081\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 123.5671 - masked_huber_delta_10_loss: 83.8945 - total_time_mse_loss: 0.0141 - val_loss: 173.6960 - val_masked_huber_delta_10_loss: 116.4892 - val_total_time_mse_loss: 0.0086\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 123.4626 - masked_huber_delta_10_loss: 83.8348 - total_time_mse_loss: 0.0136 - val_loss: 173.5106 - val_masked_huber_delta_10_loss: 116.3031 - val_total_time_mse_loss: 0.0098\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 123.1278 - masked_huber_delta_10_loss: 83.4806 - total_time_mse_loss: 0.0153 - val_loss: 173.2910 - val_masked_huber_delta_10_loss: 116.0771 - val_total_time_mse_loss: 0.0096\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 122.8607 - masked_huber_delta_10_loss: 83.1921 - total_time_mse_loss: 0.0139 - val_loss: 173.0469 - val_masked_huber_delta_10_loss: 115.8269 - val_total_time_mse_loss: 0.0086\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 122.5594 - masked_huber_delta_10_loss: 82.8637 - total_time_mse_loss: 0.0106 - val_loss: 172.7925 - val_masked_huber_delta_10_loss: 115.5627 - val_total_time_mse_loss: 0.0074\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 122.5879 - masked_huber_delta_10_loss: 82.8871 - total_time_mse_loss: 0.0111 - val_loss: 172.5671 - val_masked_huber_delta_10_loss: 115.3288 - val_total_time_mse_loss: 0.0070\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 122.0421 - masked_huber_delta_10_loss: 82.3017 - total_time_mse_loss: 0.0106 - val_loss: 172.4196 - val_masked_huber_delta_10_loss: 115.1851 - val_total_time_mse_loss: 0.0083\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 121.9154 - masked_huber_delta_10_loss: 82.2160 - total_time_mse_loss: 0.0090 - val_loss: 172.2448 - val_masked_huber_delta_10_loss: 115.0156 - val_total_time_mse_loss: 0.0097\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 121.7309 - masked_huber_delta_10_loss: 82.0561 - total_time_mse_loss: 0.0095 - val_loss: 172.0558 - val_masked_huber_delta_10_loss: 114.8237 - val_total_time_mse_loss: 0.0106\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 121.4058 - masked_huber_delta_10_loss: 81.7197 - total_time_mse_loss: 0.0120 - val_loss: 171.8104 - val_masked_huber_delta_10_loss: 114.5714 - val_total_time_mse_loss: 0.0101\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 121.1276 - masked_huber_delta_10_loss: 81.4117 - total_time_mse_loss: 0.0102 - val_loss: 171.5365 - val_masked_huber_delta_10_loss: 114.2720 - val_total_time_mse_loss: 0.0090\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 121.0238 - masked_huber_delta_10_loss: 81.2607 - total_time_mse_loss: 0.0066 - val_loss: 171.3104 - val_masked_huber_delta_10_loss: 114.0323 - val_total_time_mse_loss: 0.0086\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 120.9843 - masked_huber_delta_10_loss: 81.2100 - total_time_mse_loss: 0.0085 - val_loss: 171.1267 - val_masked_huber_delta_10_loss: 113.8585 - val_total_time_mse_loss: 0.0084\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 120.6133 - masked_huber_delta_10_loss: 80.8220 - total_time_mse_loss: 0.0107 - val_loss: 170.9678 - val_masked_huber_delta_10_loss: 113.7169 - val_total_time_mse_loss: 0.0081\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 120.4377 - masked_huber_delta_10_loss: 80.6982 - total_time_mse_loss: 0.0081 - val_loss: 170.8860 - val_masked_huber_delta_10_loss: 113.6507 - val_total_time_mse_loss: 0.0084\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 120.3462 - masked_huber_delta_10_loss: 80.6056 - total_time_mse_loss: 0.0091 - val_loss: 170.6172 - val_masked_huber_delta_10_loss: 113.3676 - val_total_time_mse_loss: 0.0070\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 120.0486 - masked_huber_delta_10_loss: 80.2876 - total_time_mse_loss: 0.0085 - val_loss: 170.4338 - val_masked_huber_delta_10_loss: 113.1773 - val_total_time_mse_loss: 0.0068\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 120.0334 - masked_huber_delta_10_loss: 80.2502 - total_time_mse_loss: 0.0078 - val_loss: 170.3065 - val_masked_huber_delta_10_loss: 113.0483 - val_total_time_mse_loss: 0.0068\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 119.8350 - masked_huber_delta_10_loss: 80.0482 - total_time_mse_loss: 0.0078 - val_loss: 170.1612 - val_masked_huber_delta_10_loss: 112.8980 - val_total_time_mse_loss: 0.0067\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 119.6521 - masked_huber_delta_10_loss: 79.8672 - total_time_mse_loss: 0.0060 - val_loss: 169.9994 - val_masked_huber_delta_10_loss: 112.7281 - val_total_time_mse_loss: 0.0064\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 119.7384 - masked_huber_delta_10_loss: 79.9469 - total_time_mse_loss: 0.0061 - val_loss: 169.7735 - val_masked_huber_delta_10_loss: 112.4877 - val_total_time_mse_loss: 0.0053\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 119.4533 - masked_huber_delta_10_loss: 79.6507 - total_time_mse_loss: 0.0046 - val_loss: 169.4426 - val_masked_huber_delta_10_loss: 112.1193 - val_total_time_mse_loss: 0.0040\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 119.1537 - masked_huber_delta_10_loss: 79.3119 - total_time_mse_loss: 0.0044 - val_loss: 169.1743 - val_masked_huber_delta_10_loss: 111.8093 - val_total_time_mse_loss: 0.0032\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 119.3207 - masked_huber_delta_10_loss: 79.3994 - total_time_mse_loss: 0.0039 - val_loss: 169.1305 - val_masked_huber_delta_10_loss: 111.8035 - val_total_time_mse_loss: 0.0036\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 119.0831 - masked_huber_delta_10_loss: 79.2164 - total_time_mse_loss: 0.0049 - val_loss: 169.2137 - val_masked_huber_delta_10_loss: 111.9186 - val_total_time_mse_loss: 0.0043\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 119.1202 - masked_huber_delta_10_loss: 79.3784 - total_time_mse_loss: 0.0052 - val_loss: 169.1259 - val_masked_huber_delta_10_loss: 111.8275 - val_total_time_mse_loss: 0.0045\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 119.0152 - masked_huber_delta_10_loss: 79.2327 - total_time_mse_loss: 0.0055 - val_loss: 168.9294 - val_masked_huber_delta_10_loss: 111.6217 - val_total_time_mse_loss: 0.0041\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 118.8829 - masked_huber_delta_10_loss: 79.0713 - total_time_mse_loss: 0.0056 - val_loss: 168.6450 - val_masked_huber_delta_10_loss: 111.3030 - val_total_time_mse_loss: 0.0034\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 118.7341 - masked_huber_delta_10_loss: 78.8764 - total_time_mse_loss: 0.0045 - val_loss: 168.4270 - val_masked_huber_delta_10_loss: 111.0667 - val_total_time_mse_loss: 0.0031\n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 118.5771 - masked_huber_delta_10_loss: 78.7212 - total_time_mse_loss: 0.0041 - val_loss: 168.3174 - val_masked_huber_delta_10_loss: 110.9592 - val_total_time_mse_loss: 0.0029\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 118.5272 - masked_huber_delta_10_loss: 78.6548 - total_time_mse_loss: 0.0042 - val_loss: 168.2047 - val_masked_huber_delta_10_loss: 110.8514 - val_total_time_mse_loss: 0.0026\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 118.3596 - masked_huber_delta_10_loss: 78.4683 - total_time_mse_loss: 0.0043 - val_loss: 168.1029 - val_masked_huber_delta_10_loss: 110.7555 - val_total_time_mse_loss: 0.0023\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 118.1200 - masked_huber_delta_10_loss: 78.2396 - total_time_mse_loss: 0.0040 - val_loss: 168.0726 - val_masked_huber_delta_10_loss: 110.7336 - val_total_time_mse_loss: 0.0021\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 118.1944 - masked_huber_delta_10_loss: 78.3424 - total_time_mse_loss: 0.0036 - val_loss: 168.0732 - val_masked_huber_delta_10_loss: 110.7316 - val_total_time_mse_loss: 0.0020\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 118.0372 - masked_huber_delta_10_loss: 78.1914 - total_time_mse_loss: 0.0034 - val_loss: 167.9339 - val_masked_huber_delta_10_loss: 110.5873 - val_total_time_mse_loss: 0.0018\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 117.9467 - masked_huber_delta_10_loss: 78.1327 - total_time_mse_loss: 0.0038 - val_loss: 167.6922 - val_masked_huber_delta_10_loss: 110.3410 - val_total_time_mse_loss: 0.0016\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 118.0285 - masked_huber_delta_10_loss: 78.1787 - total_time_mse_loss: 0.0027 - val_loss: 167.4306 - val_masked_huber_delta_10_loss: 110.0670 - val_total_time_mse_loss: 0.0015\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 117.6517 - masked_huber_delta_10_loss: 77.8068 - total_time_mse_loss: 0.0023 - val_loss: 167.2081 - val_masked_huber_delta_10_loss: 109.8347 - val_total_time_mse_loss: 0.0015\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 117.5117 - masked_huber_delta_10_loss: 77.6105 - total_time_mse_loss: 0.0024 - val_loss: 167.0790 - val_masked_huber_delta_10_loss: 109.7167 - val_total_time_mse_loss: 0.0017\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 117.5236 - masked_huber_delta_10_loss: 77.6599 - total_time_mse_loss: 0.0021 - val_loss: 167.0736 - val_masked_huber_delta_10_loss: 109.7495 - val_total_time_mse_loss: 0.0020\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 117.6021 - masked_huber_delta_10_loss: 77.8269 - total_time_mse_loss: 0.0024 - val_loss: 166.9312 - val_masked_huber_delta_10_loss: 109.6280 - val_total_time_mse_loss: 0.0022\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 117.1020 - masked_huber_delta_10_loss: 77.3225 - total_time_mse_loss: 0.0024 - val_loss: 166.7054 - val_masked_huber_delta_10_loss: 109.4073 - val_total_time_mse_loss: 0.0024\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 116.9929 - masked_huber_delta_10_loss: 77.2535 - total_time_mse_loss: 0.0020 - val_loss: 166.4751 - val_masked_huber_delta_10_loss: 109.2006 - val_total_time_mse_loss: 0.0026\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 117.2408 - masked_huber_delta_10_loss: 77.4294 - total_time_mse_loss: 0.0031 - val_loss: 166.3475 - val_masked_huber_delta_10_loss: 109.1296 - val_total_time_mse_loss: 0.0030\n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 116.8955 - masked_huber_delta_10_loss: 77.1682 - total_time_mse_loss: 0.0053 - val_loss: 165.9960 - val_masked_huber_delta_10_loss: 108.7105 - val_total_time_mse_loss: 0.0037\n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 117.2426 - masked_huber_delta_10_loss: 77.3754 - total_time_mse_loss: 0.0032 - val_loss: 165.7129 - val_masked_huber_delta_10_loss: 108.3047 - val_total_time_mse_loss: 0.0042\n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 116.9548 - masked_huber_delta_10_loss: 76.9746 - total_time_mse_loss: 0.0054 - val_loss: 165.5743 - val_masked_huber_delta_10_loss: 108.1023 - val_total_time_mse_loss: 0.0045\n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 116.7357 - masked_huber_delta_10_loss: 76.7299 - total_time_mse_loss: 0.0042 - val_loss: 165.6089 - val_masked_huber_delta_10_loss: 108.1941 - val_total_time_mse_loss: 0.0051\n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 116.4979 - masked_huber_delta_10_loss: 76.5538 - total_time_mse_loss: 0.0049 - val_loss: 165.6461 - val_masked_huber_delta_10_loss: 108.3035 - val_total_time_mse_loss: 0.0056\n",
      "Epoch 68/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 116.6568 - masked_huber_delta_10_loss: 76.7859 - total_time_mse_loss: 0.0059 - val_loss: 165.4358 - val_masked_huber_delta_10_loss: 108.0303 - val_total_time_mse_loss: 0.0053\n",
      "Epoch 69/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 116.6361 - masked_huber_delta_10_loss: 76.7508 - total_time_mse_loss: 0.0070 - val_loss: 165.1717 - val_masked_huber_delta_10_loss: 107.6989 - val_total_time_mse_loss: 0.0048\n",
      "Epoch 70/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 116.2772 - masked_huber_delta_10_loss: 76.2805 - total_time_mse_loss: 0.0057 - val_loss: 165.2046 - val_masked_huber_delta_10_loss: 107.7786 - val_total_time_mse_loss: 0.0049\n",
      "Epoch 71/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 116.2203 - masked_huber_delta_10_loss: 76.3125 - total_time_mse_loss: 0.0059 - val_loss: 165.3082 - val_masked_huber_delta_10_loss: 107.9820 - val_total_time_mse_loss: 0.0055\n",
      "Epoch 72/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 115.9996 - masked_huber_delta_10_loss: 76.2108 - total_time_mse_loss: 0.0075 - val_loss: 165.0380 - val_masked_huber_delta_10_loss: 107.7427 - val_total_time_mse_loss: 0.0056\n",
      "Epoch 73/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 116.2937 - masked_huber_delta_10_loss: 76.4352 - total_time_mse_loss: 0.0068 - val_loss: 164.7183 - val_masked_huber_delta_10_loss: 107.4122 - val_total_time_mse_loss: 0.0054\n",
      "Epoch 74/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 116.4227 - masked_huber_delta_10_loss: 76.5019 - total_time_mse_loss: 0.0058 - val_loss: 164.5777 - val_masked_huber_delta_10_loss: 107.2601 - val_total_time_mse_loss: 0.0052\n",
      "Epoch 75/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 116.1557 - masked_huber_delta_10_loss: 76.3357 - total_time_mse_loss: 0.0058 - val_loss: 164.3793 - val_masked_huber_delta_10_loss: 107.0303 - val_total_time_mse_loss: 0.0047\n",
      "Epoch 76/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 115.6835 - masked_huber_delta_10_loss: 75.7431 - total_time_mse_loss: 0.0050 - val_loss: 164.3027 - val_masked_huber_delta_10_loss: 106.9518 - val_total_time_mse_loss: 0.0044\n",
      "Epoch 77/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 115.6454 - masked_huber_delta_10_loss: 75.7582 - total_time_mse_loss: 0.0056 - val_loss: 164.1106 - val_masked_huber_delta_10_loss: 106.7388 - val_total_time_mse_loss: 0.0042\n",
      "Epoch 78/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 115.6659 - masked_huber_delta_10_loss: 75.8153 - total_time_mse_loss: 0.0047 - val_loss: 163.6888 - val_masked_huber_delta_10_loss: 106.2352 - val_total_time_mse_loss: 0.0036\n",
      "Epoch 79/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 115.6938 - masked_huber_delta_10_loss: 75.6792 - total_time_mse_loss: 0.0044 - val_loss: 163.6731 - val_masked_huber_delta_10_loss: 106.2401 - val_total_time_mse_loss: 0.0035\n",
      "Epoch 80/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 115.8284 - masked_huber_delta_10_loss: 75.9312 - total_time_mse_loss: 0.0033 - val_loss: 163.8998 - val_masked_huber_delta_10_loss: 106.5432 - val_total_time_mse_loss: 0.0036\n",
      "Epoch 81/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 115.1379 - masked_huber_delta_10_loss: 75.3688 - total_time_mse_loss: 0.0038 - val_loss: 164.0339 - val_masked_huber_delta_10_loss: 106.7249 - val_total_time_mse_loss: 0.0038\n",
      "Epoch 82/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 116.0443 - masked_huber_delta_10_loss: 76.3746 - total_time_mse_loss: 0.0044 - val_loss: 163.8350 - val_masked_huber_delta_10_loss: 106.4851 - val_total_time_mse_loss: 0.0037\n",
      "Epoch 83/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 115.1816 - masked_huber_delta_10_loss: 75.4301 - total_time_mse_loss: 0.0041 - val_loss: 163.4029 - val_masked_huber_delta_10_loss: 105.9584 - val_total_time_mse_loss: 0.0031\n",
      "Epoch 84/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 115.1180 - masked_huber_delta_10_loss: 75.1147 - total_time_mse_loss: 0.0039 - val_loss: 163.2182 - val_masked_huber_delta_10_loss: 105.7573 - val_total_time_mse_loss: 0.0025\n",
      "Epoch 85/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 115.5988 - masked_huber_delta_10_loss: 75.5655 - total_time_mse_loss: 0.0035 - val_loss: 163.2030 - val_masked_huber_delta_10_loss: 105.7866 - val_total_time_mse_loss: 0.0022\n",
      "Epoch 86/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 115.1404 - masked_huber_delta_10_loss: 75.1783 - total_time_mse_loss: 0.0041 - val_loss: 163.4984 - val_masked_huber_delta_10_loss: 106.1633 - val_total_time_mse_loss: 0.0023\n",
      "Epoch 87/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 115.6112 - masked_huber_delta_10_loss: 75.6807 - total_time_mse_loss: 0.0040 - val_loss: 163.7268 - val_masked_huber_delta_10_loss: 106.4298 - val_total_time_mse_loss: 0.0026\n",
      "Epoch 88/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 114.9793 - masked_huber_delta_10_loss: 75.3176 - total_time_mse_loss: 0.0048 - val_loss: 163.6071 - val_masked_huber_delta_10_loss: 106.3000 - val_total_time_mse_loss: 0.0030\n",
      "Epoch 89/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 115.2945 - masked_huber_delta_10_loss: 75.4479 - total_time_mse_loss: 0.0060 - val_loss: 163.2553 - val_masked_huber_delta_10_loss: 105.8569 - val_total_time_mse_loss: 0.0032\n",
      "Epoch 90/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 114.9620 - masked_huber_delta_10_loss: 75.1912 - total_time_mse_loss: 0.0055 - val_loss: 163.0584 - val_masked_huber_delta_10_loss: 105.6249 - val_total_time_mse_loss: 0.0033\n",
      "Epoch 91/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 114.5463 - masked_huber_delta_10_loss: 74.7704 - total_time_mse_loss: 0.0043 - val_loss: 163.2338 - val_masked_huber_delta_10_loss: 105.8915 - val_total_time_mse_loss: 0.0032\n",
      "Epoch 92/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 114.9179 - masked_huber_delta_10_loss: 75.1808 - total_time_mse_loss: 0.0041 - val_loss: 163.3256 - val_masked_huber_delta_10_loss: 106.0386 - val_total_time_mse_loss: 0.0031\n",
      "Epoch 93/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 114.2537 - masked_huber_delta_10_loss: 74.5889 - total_time_mse_loss: 0.0044 - val_loss: 162.9105 - val_masked_huber_delta_10_loss: 105.5341 - val_total_time_mse_loss: 0.0028\n",
      "Epoch 94/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 114.5270 - masked_huber_delta_10_loss: 74.7236 - total_time_mse_loss: 0.0039 - val_loss: 162.6669 - val_masked_huber_delta_10_loss: 105.2740 - val_total_time_mse_loss: 0.0025\n",
      "Epoch 95/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 114.2303 - masked_huber_delta_10_loss: 74.5374 - total_time_mse_loss: 0.0032 - val_loss: 162.4047 - val_masked_huber_delta_10_loss: 104.9830 - val_total_time_mse_loss: 0.0022\n",
      "Epoch 96/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 114.3671 - masked_huber_delta_10_loss: 74.5128 - total_time_mse_loss: 0.0034 - val_loss: 161.9705 - val_masked_huber_delta_10_loss: 104.4572 - val_total_time_mse_loss: 0.0020\n",
      "Epoch 97/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 114.4218 - masked_huber_delta_10_loss: 74.5886 - total_time_mse_loss: 0.0029 - val_loss: 161.6410 - val_masked_huber_delta_10_loss: 104.0257 - val_total_time_mse_loss: 0.0020\n",
      "Epoch 98/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 113.8678 - masked_huber_delta_10_loss: 73.9284 - total_time_mse_loss: 0.0029 - val_loss: 161.6520 - val_masked_huber_delta_10_loss: 104.0726 - val_total_time_mse_loss: 0.0020\n",
      "Epoch 99/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 113.9043 - masked_huber_delta_10_loss: 73.9803 - total_time_mse_loss: 0.0027 - val_loss: 161.7383 - val_masked_huber_delta_10_loss: 104.2471 - val_total_time_mse_loss: 0.0021\n",
      "Epoch 100/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 114.2454 - masked_huber_delta_10_loss: 74.3600 - total_time_mse_loss: 0.0033 - val_loss: 161.9843 - val_masked_huber_delta_10_loss: 104.5912 - val_total_time_mse_loss: 0.0023\n",
      "Restoring model weights from the end of the best epoch: 97.\n",
      "Training finished.\n",
      "Final Training Loss: 115.6291\n",
      "Final Validation Loss: 161.9843\n",
      "------------------------------\n",
      "\n",
      "--- Step 2: Generating LSTM Predictions ---\n",
      "\n",
      "------------------------------\n",
      "Generating LSTM predictions...\n",
      "------------------------------\n",
      "Predicting on 102 sequences...\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001792476A950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 366ms/stepWARNING:tensorflow:6 out of the last 14 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001792476A950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step\n",
      "Model prediction successful.\n",
      "Predicted shapes: Diffs (102, 54), Total (102, 1), Cumul (102, 54)\n",
      "Processing predictions for 102 sequences...\n",
      "Finished processing predictions. Combined results shape: (2262, 13)\n",
      "Combined results head:\n",
      "    Sequence  Step  SourceID  Predicted_Proportion  Predicted_Increment  \\\n",
      "0         0     1      11.0              0.042931            11.548487   \n",
      "1         0     2       4.0              0.043498            11.700955   \n",
      "2         0     3      13.0              0.043867            11.800332   \n",
      "3         0     4       5.0              0.044410            11.946395   \n",
      "4         0     5       9.0              0.044190            11.887192   \n",
      "\n",
      "   Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \\\n",
      "0             23.160145                   14.0                    14.0   \n",
      "1             34.861100                    1.0                    15.0   \n",
      "2             46.661430                    9.0                    24.0   \n",
      "3             58.607826                    9.0                    33.0   \n",
      "4             70.495020                   24.0                    57.0   \n",
      "\n",
      "   LSTM_Predicted_TimeDiff  LSTM_Predicted_Cumulative  \\\n",
      "0                20.715727                  20.715727   \n",
      "1                20.726475                  41.442200   \n",
      "2                20.726446                  62.168648   \n",
      "3                20.726007                  82.894653   \n",
      "4                20.726038                 103.620689   \n",
      "\n",
      "   LSTM_Predicted_TotalTime  TimeDiff_Improvement_Abs  \\\n",
      "0                  0.057968                 -4.264214   \n",
      "1                  0.057968                 -9.025520   \n",
      "2                  0.057968                 -8.926114   \n",
      "3                  0.057968                 -8.779612   \n",
      "4                  0.057968                  8.838846   \n",
      "\n",
      "   Cumulative_Improvement_Abs  \n",
      "0                    2.444418  \n",
      "1                   -6.581100  \n",
      "2                  -15.507218  \n",
      "3                  -24.286827  \n",
      "4                  -33.125669  \n",
      "Combined results tail:\n",
      "       Sequence  Step  SourceID  Predicted_Proportion  Predicted_Increment  \\\n",
      "2257       100    16       5.0              0.049685             65.28632   \n",
      "2258       100    17       8.0              0.048959             64.33275   \n",
      "2259       100    18       4.0              0.049774             65.40253   \n",
      "2260       100    19       5.0              0.050231             66.00403   \n",
      "2261       101     1      10.0              0.499096            668.28955   \n",
      "\n",
      "      Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \\\n",
      "2257             1118.2607                    2.0                  1308.0   \n",
      "2258             1182.5935                    0.0                  1308.0   \n",
      "2259             1247.9961                    6.0                  1314.0   \n",
      "2260             1314.0001                -1314.0                     0.0   \n",
      "2261             1339.0000                -1339.0                     0.0   \n",
      "\n",
      "      LSTM_Predicted_TimeDiff  LSTM_Predicted_Cumulative  \\\n",
      "2257                      0.0                 290.073853   \n",
      "2258                      0.0                 290.073853   \n",
      "2259                      0.0                 290.073853   \n",
      "2260                      0.0                 290.073853   \n",
      "2261                      0.0                   0.000000   \n",
      "\n",
      "      LSTM_Predicted_TotalTime  TimeDiff_Improvement_Abs  \\\n",
      "2257                  0.062749                  61.28632   \n",
      "2258                  0.062749                  64.33275   \n",
      "2259                  0.062749                  53.40253   \n",
      "2260                  0.062749                  66.00403   \n",
      "2261                  0.134322                 668.28955   \n",
      "\n",
      "      Cumulative_Improvement_Abs  \n",
      "2257                 -828.186847  \n",
      "2258                 -892.519647  \n",
      "2259                 -957.922247  \n",
      "2260                 1023.926247  \n",
      "2261                 1339.000000  \n",
      "Checking for NaNs in LSTM predictions:\n",
      "LSTM_Predicted_TimeDiff      0\n",
      "LSTM_Predicted_Cumulative    0\n",
      "LSTM_Predicted_TotalTime     0\n",
      "dtype: int64\n",
      "Combined predictions successfully saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\predictions_lstm_refined_175651.csv\n",
      "\n",
      "------------------------------\n",
      "Overall Model Performance Comparison (Mean Absolute Error)\n",
      "------------------------------\n",
      "Calculating MAE based on 2262 valid rows (after dropping NaNs in comparison columns).\n",
      "Time Differences MAE:\n",
      "  Transformer: 122.0897\n",
      "  LSTM:        107.0282\n",
      "  Improvement: 12.34%\n",
      "\n",
      "Cumulative Times MAE:\n",
      "  Transformer: 224.0239\n",
      "  LSTM:        356.9798\n",
      "  Improvement: -59.35%\n",
      "------------------------------\n",
      "\n",
      "--- Step 3: Displaying Sample Results ---\n",
      "\n",
      "Sample Combined Predictions (Head):\n",
      "   Sequence  Step  SourceID  GroundTruth_Increment  Predicted_Increment  LSTM_Predicted_TimeDiff  GroundTruth_Cumulative  Predicted_Cumulative  LSTM_Predicted_Cumulative  TimeDiff_Improvement_Abs  Cumulative_Improvement_Abs\n",
      "0         0     1      11.0                   14.0            11.548487                20.715727                    14.0             23.160145                  20.715727                 -4.264214                    2.444418\n",
      "1         0     2       4.0                    1.0            11.700955                20.726475                    15.0             34.861100                  41.442200                 -9.025520                   -6.581100\n",
      "2         0     3      13.0                    9.0            11.800332                20.726446                    24.0             46.661430                  62.168648                 -8.926114                  -15.507218\n",
      "3         0     4       5.0                    9.0            11.946395                20.726007                    33.0             58.607826                  82.894653                 -8.779612                  -24.286827\n",
      "4         0     5       9.0                   24.0            11.887192                20.726038                    57.0             70.495020                 103.620689                  8.838846                  -33.125669\n",
      "5         0     6       1.0                    6.0            11.843553                20.725718                    63.0             82.338570                 124.346405                 -8.882165                  -42.007835\n",
      "6         0     7       5.0                  116.0            11.797453                20.721937                   179.0             94.136024                 145.068344                  8.924484                   50.932320\n",
      "7         0     8       4.0                    2.0            11.668401                20.724161                   181.0            105.804430                 165.792511                 -9.055760                   59.988081\n",
      "8         0     9       5.0                   13.0            11.792318                20.712328                   194.0            117.596750                 186.504837                 -6.504646                   68.908087\n",
      "9         0    10       5.0                   16.0            11.837735                20.671982                   210.0            129.434480                 207.176819                 -0.509717                   77.742339\n",
      "\n",
      "--- Step 4: Generating Visualizations ---\n",
      "\n",
      "------------------------------\n",
      "Generating visualizations...\n",
      "------------------------------\n",
      "Attempting to save training performance plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\training_performance_run2.png\n",
      "Successfully saved training performance plot.\n",
      "Attempting to plot prediction comparisons for 3 sample sequences: [30 67 62]\n",
      "\n",
      "Processing plot for Sequence ID: 30\n",
      "  Data shape for sequence 30: (20, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_30_comparison_run2.png\n",
      "  Successfully saved comparison plot for sequence 30.\n",
      "\n",
      "Processing plot for Sequence ID: 67\n",
      "  Data shape for sequence 67: (26, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_67_comparison_run2.png\n",
      "  Successfully saved comparison plot for sequence 67.\n",
      "\n",
      "Processing plot for Sequence ID: 62\n",
      "  Data shape for sequence 62: (32, 13)\n",
      "  Plotted 'GroundTruth_Increment' for increments.\n",
      "  Plotted 'Predicted_Increment' for increments.\n",
      "  Plotted 'LSTM_Predicted_TimeDiff' for increments.\n",
      "  Plotted 'GroundTruth_Cumulative' for cumulative.\n",
      "  Plotted 'Predicted_Cumulative' for cumulative.\n",
      "  Plotted 'LSTM_Predicted_Cumulative' for cumulative.\n",
      "  Attempting to save comparison plot to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\sequence_62_comparison_run2.png\n",
      "  Successfully saved comparison plot for sequence 62.\n",
      "------------------------------\n",
      "Visualization generation finished.\n",
      "------------------------------\n",
      "\n",
      "==================================================\n",
      " Pipeline Execution Completed\n",
      "  - Combined predictions CSV saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\predictions_lstm_refined_1.csv\n",
      "  - Training performance plot saved to: c:\\Users\\lukis\\Documents\\GitHub\\Time-Series-Models\\PXChange\\training_performance.png\n",
      "  - Sequence comparison plots saved as: sequence_<ID>_comparison.png\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Script Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    # import matplotlib\n",
    "    # matplotlib.use('Agg') # Uncomment if running in a non-GUI environment\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
