{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau # Added ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import os # For checking file existence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # For target scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute increments and cumulative times from proportions and total time\n",
    "def calculate_times_from_proportions(proportions_per_step, total_time_for_sequence, mask_per_step):\n",
    "    \"\"\"\n",
    "    Calculates time increments and cumulative times from step-wise proportions and a total time.\n",
    "    \"\"\"\n",
    "    proportions_tf = tf.cast(proportions_per_step, tf.float32)\n",
    "    total_time_tf = tf.cast(total_time_for_sequence, tf.float32)\n",
    "    mask_tf = tf.cast(mask_per_step, tf.float32)\n",
    "\n",
    "    if len(tf.shape(total_time_tf)) == 1:\n",
    "        total_time_tf = tf.expand_dims(total_time_tf, axis=-1)\n",
    "\n",
    "    masked_proportions = proportions_tf * mask_tf\n",
    "    row_sums = tf.reduce_sum(masked_proportions, axis=1, keepdims=True)\n",
    "    row_sums = tf.where(tf.equal(row_sums, 0), tf.ones_like(row_sums), row_sums) \n",
    "    normalized_proportions = masked_proportions / row_sums\n",
    "    increments = normalized_proportions * total_time_tf \n",
    "    cumulative_times = tf.cumsum(increments, axis=1)\n",
    "    \n",
    "    increments *= mask_tf\n",
    "    cumulative_times *= mask_tf\n",
    "    normalized_proportions *= mask_tf\n",
    "\n",
    "    return normalized_proportions, increments, cumulative_times\n",
    "\n",
    "# %%\n",
    "class TotalTimeLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Enhanced LSTM model to predict only the total time of a sequence,\n",
    "    incorporating global sequence features and increased capacity.\n",
    "    Architecture: \n",
    "        Input1 (Sequential): BiLSTM -> GlobalAveragePooling1D \n",
    "        Input2 (Global): Global Features\n",
    "        Concatenate -> Dense -> Dropout -> Dense -> Dropout -> Dense (Output)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=192, # Increased LSTM units\n",
    "                 dense_units_1=128, # First dense layer after concat\n",
    "                 dense_units_2=64,  # Second intermediate dense layer\n",
    "                 dropout_rate=0.3):\n",
    "        super(TotalTimeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.dense_units_1 = dense_units_1\n",
    "        self.dense_units_2 = dense_units_2\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # --- Layers for sequential input ---\n",
    "        self.bi_lstm_layer = layers.Bidirectional(\n",
    "            layers.LSTM(self.hidden_units, \n",
    "                        return_sequences=True, \n",
    "                        dropout=self.dropout_rate, # Dropout on inputs to LSTM\n",
    "                        recurrent_dropout=0.2), # Reduced recurrent dropout\n",
    "            name=\"bidirectional_lstm_v9\"\n",
    "        )\n",
    "        self.global_avg_pool = layers.GlobalAveragePooling1D(name=\"global_avg_pooling_v9\")\n",
    "        \n",
    "        # --- Layers for combined features ---\n",
    "        self.concat_layer = layers.Concatenate(name=\"concatenate_features_v9\")\n",
    "        \n",
    "        self.dense_1 = layers.Dense(\n",
    "            self.dense_units_1, \n",
    "            activation='relu', # Using relu\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001), # Added L2 regularization\n",
    "            name=\"dense_1_v9\"\n",
    "        )\n",
    "        self.dropout_1 = layers.Dropout(self.dropout_rate, name=\"dropout_1_v9\")\n",
    "        \n",
    "        self.dense_2 = layers.Dense(\n",
    "            self.dense_units_2,\n",
    "            activation='relu', # Using relu\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.001), # Added L2 regularization\n",
    "            name=\"dense_2_v9\"\n",
    "        )\n",
    "        self.dropout_2 = layers.Dropout(self.dropout_rate, name=\"dropout_2_v9\")\n",
    "\n",
    "        self.total_time_head = layers.Dense(1, activation='linear', name=\"total_time_dense_v9\") \n",
    "        \n",
    "    def call(self, inputs, training=False): \n",
    "        sequence_input, global_features_input = inputs \n",
    "\n",
    "        mask_bool_seq = tf.reduce_any(tf.not_equal(sequence_input, 0.0), axis=-1)\n",
    "        x_seq = self.bi_lstm_layer(sequence_input, mask=mask_bool_seq, training=training)\n",
    "        x_seq_pooled = self.global_avg_pool(x_seq, mask=mask_bool_seq)\n",
    "        \n",
    "        combined_features = self.concat_layer([x_seq_pooled, global_features_input])\n",
    "        \n",
    "        x = self.dense_1(combined_features)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        total_time_pred = self.total_time_head(x)\n",
    "        \n",
    "        return total_time_pred\n",
    "\n",
    "# %%\n",
    "def process_input_data_for_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process the transformer predictions CSV to prepare data for LSTM training.\n",
    "    Adds more global features: actual sequence length, sum, mean, and std of Transformer proportions.\n",
    "    \"\"\"\n",
    "    print(f\"Processing data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "    df = pd.read_csv(transformer_predictions_file)\n",
    "    \n",
    "    required_cols = ['Predicted_Proportion', 'GroundTruth_Cumulative', 'GroundTruth_Increment', 'Sequence', 'Step']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"CSV must contain '{col}' column.\")\n",
    "\n",
    "    sequences = df['Sequence'].unique()\n",
    "    \n",
    "    X_sequential_data_list = []\n",
    "    X_global_features_list = []\n",
    "    y_total_times_list = [] \n",
    "    original_dfs_list = [] \n",
    "    transformer_proportions_list = [] \n",
    "    ground_truth_increments_list = []\n",
    "    ground_truth_cumulative_list = [] \n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_df = df[df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        if seq_df.empty:\n",
    "            original_dfs_list.append(seq_df) \n",
    "            continue\n",
    "        original_dfs_list.append(seq_df) \n",
    "\n",
    "        actual_sequence_length = float(len(seq_df)) # Ensure float for scaler\n",
    "        props_for_seq = seq_df['Predicted_Proportion'].values\n",
    "        sum_transformer_proportions = np.sum(props_for_seq)\n",
    "        mean_transformer_proportions = np.mean(props_for_seq) if actual_sequence_length > 0 else 0.0\n",
    "        std_transformer_proportions = np.std(props_for_seq) if actual_sequence_length > 1 else 0.0 # Std undefined for 1 element\n",
    "\n",
    "        current_max_steps = seq_df['Step'].max()\n",
    "        if current_max_steps == 0: current_max_steps = 1 \n",
    "        \n",
    "        sequential_features = np.column_stack([\n",
    "            props_for_seq,\n",
    "            seq_df['Step'].values / current_max_steps \n",
    "        ])\n",
    "        X_sequential_data_list.append(sequential_features)\n",
    "        \n",
    "        global_features_for_seq = np.array([\n",
    "            actual_sequence_length, \n",
    "            sum_transformer_proportions,\n",
    "            mean_transformer_proportions,\n",
    "            std_transformer_proportions\n",
    "        ], dtype=np.float32)\n",
    "        X_global_features_list.append(global_features_for_seq)\n",
    "        \n",
    "        gt_cumulative_for_seq = seq_df['GroundTruth_Cumulative'].values\n",
    "        total_time_for_seq = np.max(gt_cumulative_for_seq) if len(gt_cumulative_for_seq) > 0 else 0.0\n",
    "        y_total_times_list.append(total_time_for_seq)\n",
    "        \n",
    "        transformer_proportions_list.append(props_for_seq)\n",
    "        ground_truth_increments_list.append(seq_df['GroundTruth_Increment'].values)\n",
    "        ground_truth_cumulative_list.append(gt_cumulative_for_seq) \n",
    "\n",
    "    if not X_sequential_data_list:\n",
    "        raise ValueError(\"No valid sequences processed.\")\n",
    "\n",
    "    y_total_times_array_unpadded = np.array(y_total_times_list)\n",
    "    print(f\"\\nStatistics for TARGET y_total_times_list (max GT_Cumulative per seq, {len(y_total_times_array_unpadded)} sequences):\")\n",
    "    print(f\"  Mean: {np.mean(y_total_times_array_unpadded):.4f}, Std Dev: {np.std(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Min: {np.min(y_total_times_array_unpadded):.4f}, Max: {np.max(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Number of zeros (<=1e-6): {np.sum(y_total_times_array_unpadded <= 1e-6)}\")\n",
    "    print(f\"  Number non-positive (<=0): {np.sum(y_total_times_array_unpadded <= 0)}\\n\")\n",
    "\n",
    "    max_length_sequential = max(len(x) for x in X_sequential_data_list) if X_sequential_data_list else 0\n",
    "    if max_length_sequential == 0: raise ValueError(\"Max length for sequential features is 0.\")\n",
    "    num_sequential_features = X_sequential_data_list[0].shape[1] if X_sequential_data_list and X_sequential_data_list[0].shape[0] > 0 else 0\n",
    "    if num_sequential_features == 0: raise ValueError(\"Number of sequential features is 0.\")\n",
    "    num_global_features = X_global_features_list[0].shape[0] if X_global_features_list else 0\n",
    "    if num_global_features == 0 : raise ValueError(\"Number of global features is 0.\")\n",
    "\n",
    "    X_sequential_padded = np.zeros((len(X_sequential_data_list), max_length_sequential, num_sequential_features), dtype=np.float32)\n",
    "    masks_padded_float = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32) \n",
    "    transformer_proportions_padded = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "    gt_increments_padded_original = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "    gt_cumulative_padded_original = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(X_sequential_data_list)):\n",
    "        seq_len = len(X_sequential_data_list[i])\n",
    "        if seq_len > 0:\n",
    "            X_sequential_padded[i, :seq_len, :] = X_sequential_data_list[i]\n",
    "            masks_padded_float[i, :seq_len] = 1.0 \n",
    "            transformer_proportions_padded[i, :seq_len] = transformer_proportions_list[i]\n",
    "            gt_increments_padded_original[i, :seq_len] = ground_truth_increments_list[i]\n",
    "            gt_cumulative_padded_original[i, :seq_len] = ground_truth_cumulative_list[i]\n",
    "        \n",
    "    y_total_times_np = np.array(y_total_times_list, dtype=np.float32)\n",
    "    X_global_features_np = np.array(X_global_features_list, dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        'X_sequential_input': X_sequential_padded, \n",
    "        'X_global_features_input': X_global_features_np, \n",
    "        'y_lstm_target_total_times': y_total_times_np,\n",
    "        'masks_for_calc': masks_padded_float, \n",
    "        'sequences_ids': sequences, \n",
    "        'original_dfs': original_dfs_list, \n",
    "        'transformer_proportions_padded': transformer_proportions_padded, \n",
    "        'gt_increments_padded_original': gt_increments_padded_original,\n",
    "        'gt_cumulative_padded_original': gt_cumulative_padded_original,\n",
    "        'max_len_sequential': max_length_sequential,\n",
    "        'num_sequential_features': num_sequential_features,\n",
    "        'num_global_features': num_global_features\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def train_total_time_lstm(transformer_predictions_file, epochs=50, batch_size=32, val_split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Train the enhanced LSTM model (with more global features) to predict total_time.\n",
    "    \"\"\"\n",
    "    print(\"Processing data for LSTM training...\")\n",
    "    data_for_lstm = process_input_data_for_lstm(transformer_predictions_file)\n",
    "    \n",
    "    print(f\"Num sequential features: {data_for_lstm['num_sequential_features']}, Max seq length: {data_for_lstm['max_len_sequential']}\")\n",
    "    print(f\"Num global features: {data_for_lstm['num_global_features']}\")\n",
    "\n",
    "    lstm_model = TotalTimeLSTM(hidden_units=192, dense_units_1=128, dense_units_2=64, dropout_rate=0.3) \n",
    "    \n",
    "    X_sequential_all = data_for_lstm['X_sequential_input']\n",
    "    X_global_all = data_for_lstm['X_global_features_input']\n",
    "    y_targets_all = data_for_lstm['y_lstm_target_total_times']\n",
    "    \n",
    "    if len(X_sequential_all) > 0:\n",
    "        sample_seq_input_for_build = tf.convert_to_tensor(X_sequential_all[:1], dtype=tf.float32)\n",
    "        sample_glob_input_for_build = tf.convert_to_tensor(X_global_all[:1], dtype=tf.float32)\n",
    "        _ = lstm_model((sample_seq_input_for_build, sample_glob_input_for_build)) \n",
    "        print(\"\\nEnhanced LSTM Model Summary (v9 - after sample call):\")\n",
    "        lstm_model.summary(expand_nested=True) \n",
    "    else:\n",
    "        print(\"Warning: No data to build model with sample call.\")\n",
    "        # Fallback build if needed, though less ideal for multi-input subclassed models\n",
    "        # seq_input_shape = (None, data_for_lstm['max_len_sequential'], data_for_lstm['num_sequential_features'])\n",
    "        # glob_input_shape = (None, data_for_lstm['num_global_features'])\n",
    "        # lstm_model.build(input_shape=[seq_input_shape, glob_input_shape]) # This might not work as expected\n",
    "        # lstm_model.summary(expand_nested=True)\n",
    "\n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse' ) # Kept reduced LR\n",
    "    \n",
    "    if np.any(np.isnan(X_sequential_all)) or np.any(np.isinf(X_sequential_all)): print(\"CRITICAL WARNING: NaN/Inf in X_sequential_all.\")\n",
    "    if np.any(np.isnan(X_global_all)) or np.any(np.isinf(X_global_all)): print(\"CRITICAL WARNING: NaN/Inf in X_global_all.\")\n",
    "    if np.any(np.isnan(y_targets_all)) or np.any(np.isinf(y_targets_all)): print(\"CRITICAL WARNING: NaN/Inf in y_targets_all.\")\n",
    "    if len(y_targets_all) > 0 and np.all(np.abs(y_targets_all) <= 1e-6) : print(\"CRITICAL WARNING: All target total times are near zero.\")\n",
    "\n",
    "    target_scaler = StandardScaler()\n",
    "    y_targets_all_reshaped = y_targets_all.reshape(-1, 1)\n",
    "\n",
    "    global_feature_scaler = StandardScaler()\n",
    "    # X_global_all is already a numpy array from process_input_data_for_lstm\n",
    "\n",
    "    indices = np.arange(len(X_sequential_all))\n",
    "\n",
    "    if len(X_sequential_all) < 10: # Increased minimum for a meaningful split \n",
    "        print(\"Warning: Very few samples (<10), using all for training.\")\n",
    "        X_train_seq = X_sequential_all\n",
    "        X_train_glob_scaled = global_feature_scaler.fit_transform(X_global_all)\n",
    "        y_train_scaled = target_scaler.fit_transform(y_targets_all_reshaped)\n",
    "        validation_data_for_fit = None\n",
    "    else:\n",
    "        train_indices, val_indices = train_test_split(indices, test_size=val_split_ratio, random_state=42, shuffle=True)\n",
    "        \n",
    "        X_train_seq = X_sequential_all[train_indices]\n",
    "        X_val_seq = X_sequential_all[val_indices]\n",
    "        \n",
    "        X_train_glob = X_global_all[train_indices]\n",
    "        X_val_glob = X_global_all[val_indices]\n",
    "        X_train_glob_scaled = global_feature_scaler.fit_transform(X_train_glob) \n",
    "        X_val_glob_scaled = global_feature_scaler.transform(X_val_glob)     \n",
    "\n",
    "        y_train_orig_reshaped = y_targets_all_reshaped[train_indices]\n",
    "        y_val_orig_reshaped = y_targets_all_reshaped[val_indices]\n",
    "        \n",
    "        y_train_scaled = target_scaler.fit_transform(y_train_orig_reshaped) \n",
    "        y_val_scaled = target_scaler.transform(y_val_orig_reshaped)         \n",
    "        \n",
    "        validation_data_for_fit = ([X_val_seq, X_val_glob_scaled], y_val_scaled) \n",
    "        \n",
    "        print(f\"\\nManually split data: {len(X_train_seq)} train, {len(X_val_seq)} validation samples.\")\n",
    "        print(\"Training target statistics (y_train - original scale):\")\n",
    "        print(f\"  Mean: {np.mean(y_train_orig_reshaped.flatten()):.4f}, Std: {np.std(y_train_orig_reshaped.flatten()):.4f}\")\n",
    "        print(\"Validation target statistics (y_val - original scale):\")\n",
    "        print(f\"  Mean: {np.mean(y_val_orig_reshaped.flatten()):.4f}, Std: {np.std(y_val_orig_reshaped.flatten()):.4f}\\n\")\n",
    "\n",
    "    callbacks_list = [\n",
    "        EarlyStopping(monitor='val_loss', patience=35, restore_best_weights=True, verbose=1), # Increased patience\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6, verbose=1) # Added LR scheduler\n",
    "    ]\n",
    "    \n",
    "    print(\"Starting LSTM model training (with scaled targets and global features)...\")\n",
    "    history = lstm_model.fit(\n",
    "        [X_train_seq, X_train_glob_scaled], y_train_scaled,          \n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        validation_data=validation_data_for_fit, \n",
    "        callbacks=callbacks_list, verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM training finished.\")\n",
    "    data_for_lstm['target_scaler'] = target_scaler\n",
    "    data_for_lstm['global_feature_scaler'] = global_feature_scaler \n",
    "    return lstm_model, history, data_for_lstm\n",
    "\n",
    "# %%\n",
    "def generate_refined_predictions_with_lstm(lstm_model, processed_data):\n",
    "    \"\"\"\n",
    "    Generate refined time predictions. LSTM_Predicted_TotalTime only on last row of seq.\n",
    "    Output CSV changed to _v9.\n",
    "    \"\"\"\n",
    "    print(\"Generating refined predictions using LSTM's total time...\")\n",
    "    \n",
    "    X_sequential_input_all = processed_data['X_sequential_input']\n",
    "    X_global_features_input_all_unscaled = processed_data['X_global_features_input']\n",
    "    \n",
    "    global_feature_scaler = processed_data['global_feature_scaler']\n",
    "    X_global_features_input_all_scaled = global_feature_scaler.transform(X_global_features_input_all_unscaled)\n",
    "\n",
    "    lstm_predicted_scaled_total_times = lstm_model.predict(\n",
    "        [X_sequential_input_all, X_global_features_input_all_scaled] \n",
    "    ) \n",
    "    \n",
    "    target_scaler = processed_data['target_scaler']\n",
    "    lstm_predicted_total_times_original_scale = target_scaler.inverse_transform(lstm_predicted_scaled_total_times)\n",
    "    lstm_predicted_total_times_original_scale = np.squeeze(lstm_predicted_total_times_original_scale)\n",
    "    lstm_predicted_total_times_original_scale = np.maximum(0, lstm_predicted_total_times_original_scale) \n",
    "\n",
    "    transformer_step_proportions = processed_data['transformer_proportions_padded'] \n",
    "    masks_for_calc = processed_data['masks_for_calc'] \n",
    "\n",
    "    _, lstm_refined_increments, lstm_refined_cumulative = calculate_times_from_proportions(\n",
    "        transformer_step_proportions,\n",
    "        lstm_predicted_total_times_original_scale, \n",
    "        masks_for_calc \n",
    "    )\n",
    "\n",
    "    lstm_refined_increments_np = lstm_refined_increments.numpy()\n",
    "    lstm_refined_cumulative_np = lstm_refined_cumulative.numpy()\n",
    "\n",
    "    results_list_df = []\n",
    "    original_dfs_from_processing = processed_data['original_dfs'] \n",
    "\n",
    "    for i, seq_id in enumerate(processed_data['sequences_ids']):\n",
    "        if i >= len(original_dfs_from_processing): continue\n",
    "        current_seq_df_base = original_dfs_from_processing[i].copy()\n",
    "        seq_len = len(current_seq_df_base)\n",
    "        if seq_len == 0:\n",
    "            if current_seq_df_base.empty: results_list_df.append(current_seq_df_base) \n",
    "            continue\n",
    "        if i >= len(lstm_predicted_total_times_original_scale): continue\n",
    "\n",
    "        # Initialize LSTM_Predicted_TotalTime with NaN\n",
    "        current_seq_df_base['LSTM_Predicted_TotalTime'] = np.nan\n",
    "        # Set the predicted total time only for the last row of the sequence\n",
    "        if seq_len > 0:\n",
    "            current_seq_df_base.iloc[-1, current_seq_df_base.columns.get_loc('LSTM_Predicted_TotalTime')] = lstm_predicted_total_times_original_scale[i]\n",
    "        \n",
    "        current_seq_df_base['LSTM_Predicted_Increment'] = lstm_refined_increments_np[i, :seq_len]\n",
    "        current_seq_df_base['LSTM_Predicted_Cumulative'] = lstm_refined_cumulative_np[i, :seq_len]\n",
    "        \n",
    "        if 'GroundTruth_Increment' in current_seq_df_base.columns and 'Predicted_Increment' in current_seq_df_base.columns:\n",
    "            gt_increment = current_seq_df_base['GroundTruth_Increment'].fillna(0); transformer_pred_increment = current_seq_df_base['Predicted_Increment'].fillna(0); lstm_pred_increment = current_seq_df_base['LSTM_Predicted_Increment'].fillna(0)\n",
    "            diff_transformer = np.abs(gt_increment - transformer_pred_increment); diff_lstm = np.abs(gt_increment - lstm_pred_increment)\n",
    "            current_seq_df_base['Increment_MAE_Transformer'] = diff_transformer; current_seq_df_base['Increment_MAE_LSTM'] = diff_lstm\n",
    "            current_seq_df_base['Increment_Improvement_Pct'] = np.where(diff_transformer > 1e-6, (diff_transformer - diff_lstm) / diff_transformer * 100, 0 )\n",
    "        if 'GroundTruth_Cumulative' in current_seq_df_base.columns and 'Predicted_Cumulative' in current_seq_df_base.columns:\n",
    "            gt_cumulative = current_seq_df_base['GroundTruth_Cumulative'].fillna(0); transformer_pred_cumulative = current_seq_df_base['Predicted_Cumulative'].fillna(0); lstm_pred_cumulative = current_seq_df_base['LSTM_Predicted_Cumulative'].fillna(0)\n",
    "            diff_transformer_cum = np.abs(gt_cumulative - transformer_pred_cumulative); diff_lstm_cum = np.abs(gt_cumulative - lstm_pred_cumulative)\n",
    "            current_seq_df_base['Cumulative_MAE_Transformer'] = diff_transformer_cum; current_seq_df_base['Cumulative_MAE_LSTM'] = diff_lstm_cum\n",
    "            current_seq_df_base['Cumulative_Improvement_Pct'] = np.where(diff_transformer_cum > 1e-6, (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100, 0 )\n",
    "        results_list_df.append(current_seq_df_base)\n",
    "    \n",
    "    if not results_list_df: return pd.DataFrame()\n",
    "    final_results_df = pd.concat(results_list_df, ignore_index=True)\n",
    "    \n",
    "    output_filename = 'combined_model_results_SN_175651_175974_182625_v9.csv' # Changed filename\n",
    "    final_results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Combined and refined predictions saved to {output_filename}\")\n",
    "    \n",
    "    if not final_results_df.empty:\n",
    "        if 'Increment_MAE_Transformer' in final_results_df.columns and 'Increment_MAE_LSTM' in final_results_df.columns:\n",
    "            valid_inc_mae_transformer = final_results_df['Increment_MAE_Transformer'].dropna(); valid_inc_mae_lstm = final_results_df['Increment_MAE_LSTM'].dropna()\n",
    "            if not valid_inc_mae_transformer.empty and not valid_inc_mae_lstm.empty:\n",
    "                avg_transformer_inc_mae = valid_inc_mae_transformer.mean(); avg_lstm_inc_mae = valid_inc_mae_lstm.mean()\n",
    "                print(f\"\\nTransformer MAE (Increments): {avg_transformer_inc_mae:.4f}, LSTM-Refined MAE (Increments): {avg_lstm_inc_mae:.4f}\")\n",
    "                if avg_transformer_inc_mae > 1e-6: print(f\"Improvement (Increments): {(avg_transformer_inc_mae - avg_lstm_inc_mae) / avg_transformer_inc_mae * 100:.2f}%\")\n",
    "        if 'Cumulative_MAE_Transformer' in final_results_df.columns and 'Cumulative_MAE_LSTM' in final_results_df.columns:\n",
    "            valid_cum_mae_transformer = final_results_df['Cumulative_MAE_Transformer'].dropna(); valid_cum_mae_lstm = final_results_df['Cumulative_MAE_LSTM'].dropna()\n",
    "            if not valid_cum_mae_transformer.empty and not valid_cum_mae_lstm.empty:\n",
    "                avg_transformer_cum_mae = valid_cum_mae_transformer.mean(); avg_lstm_cum_mae = valid_cum_mae_lstm.mean()\n",
    "                print(f\"Transformer MAE (Cumulative): {avg_transformer_cum_mae:.4f}, LSTM-Refined MAE (Cumulative): {avg_lstm_cum_mae:.4f}\")\n",
    "                if avg_transformer_cum_mae > 1e-6: print(f\"Improvement (Cumulative): {(avg_transformer_cum_mae - avg_lstm_cum_mae) / avg_transformer_cum_mae * 100:.2f}%\")\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data['y_lstm_target_total_times'] \n",
    "    if len(lstm_predicted_total_times_original_scale) == len(gt_total_times_for_lstm_training):\n",
    "        mae_total_time_lstm = np.mean(np.abs(gt_total_times_for_lstm_training - lstm_predicted_total_times_original_scale))\n",
    "        print(f\"\\nLSTM MAE for Total Time (vs Max GT Cumulative): {mae_total_time_lstm:.4f}\")\n",
    "\n",
    "    return final_results_df\n",
    "\n",
    "# %%\n",
    "def visualize_lstm_results(results_df, processed_data, lstm_model, training_history):\n",
    "    if results_df.empty: print(\"Results DataFrame is empty, skipping visualizations.\"); return\n",
    "    print(\"Generating visualizations for LSTM results...\")\n",
    "    plt.style.use('ggplot')\n",
    "    if training_history and hasattr(training_history, 'history'):\n",
    "        if 'loss' in training_history.history and 'val_loss' in training_history.history:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "            plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "            if 'lr' in training_history.history:\n",
    "                ax2 = plt.gca().twinx()\n",
    "                ax2.plot(training_history.history['lr'], label='Learning Rate', color='g', linestyle='--')\n",
    "                ax2.set_ylabel('Learning Rate')\n",
    "                ax2.legend(loc='upper center')\n",
    "            plt.title('LSTM Model Loss (Predicting Scaled Total Time)') \n",
    "            plt.xlabel('Epoch'); plt.ylabel('Mean Squared Error (Scaled Loss)'); plt.legend(loc='upper left'); plt.tight_layout()\n",
    "            plt.savefig('lstm_total_time_training_loss_v9.png'); print(\"Saved LSTM training loss plot.\"); plt.close()\n",
    "    else: print(\"Warning: Training history not available or malformed.\")\n",
    "\n",
    "    sample_sequence_ids = results_df['Sequence'].unique()\n",
    "    if len(sample_sequence_ids) == 0 : print(\"No sequences in results_df for plotting.\"); return\n",
    "    sample_sequence_ids = sample_sequence_ids[:min(5, len(sample_sequence_ids))]\n",
    "    \n",
    "    if len(sample_sequence_ids) > 0:\n",
    "        num_plots = len(sample_sequence_ids); fig_height = max(8, 3 * num_plots) \n",
    "        plt.figure(figsize=(15, fig_height)) # Cumulative Plot\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Cumulative'], 'o-', label='GT Cumul.', ms=4)\n",
    "            if 'Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Cumulative'], 's--', label='Transformer Cumul.', ms=4)\n",
    "            if 'LSTM_Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Cumulative'], '^-.', label='LSTM-Refined Cumul.', ms=4)\n",
    "            # Display the single LSTM_Predicted_TotalTime for the sequence\n",
    "            lstm_total_time_for_seq = seq_data_plot['LSTM_Predicted_TotalTime'].dropna().unique()\n",
    "            if len(lstm_total_time_for_seq) == 1:\n",
    "                 plt.axhline(y=lstm_total_time_for_seq[0], color='purple', linestyle=':', label=f'LSTM Total Pred: {lstm_total_time_for_seq[0]:.2f}')\n",
    "            plt.title(f'Cumulative Times: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Cumulative Time'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_cumulative_time_comparison_v9.png'); print(\"Saved cumulative time comparison plot.\"); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(15, fig_height)) # Increment Plot\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Increment'], 'o-', label='GT Incr.', ms=4)\n",
    "            if 'Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Increment'], 's--', label='Transformer Incr.', ms=4)\n",
    "            if 'LSTM_Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Increment'], '^-.', label='LSTM-Refined Incr.', ms=4)\n",
    "            plt.title(f'Time Increments: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Time Increment'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_increment_comparison_v9.png'); print(\"Saved increment comparison plot.\"); plt.close()\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data.get('y_lstm_target_total_times', np.array([])) \n",
    "    if lstm_model is not None and 'X_sequential_input' in processed_data and 'X_global_features_input' in processed_data and 'target_scaler' in processed_data:\n",
    "        X_seq_tensor = tf.convert_to_tensor(processed_data['X_sequential_input'], dtype=tf.float32)\n",
    "        X_glob_unscaled = processed_data['X_global_features_input']\n",
    "        X_glob_scaled_for_plot = processed_data['global_feature_scaler'].transform(X_glob_unscaled)\n",
    "        X_glob_tensor = tf.convert_to_tensor(X_glob_scaled_for_plot, dtype=tf.float32)\n",
    "        \n",
    "        lstm_pred_scaled_total_t = lstm_model.predict([X_seq_tensor, X_glob_tensor])\n",
    "        lstm_pred_original_scale_total_t = processed_data['target_scaler'].inverse_transform(lstm_pred_scaled_total_t).squeeze()\n",
    "        \n",
    "        if lstm_pred_original_scale_total_t.ndim == 0: lstm_pred_original_scale_total_t = np.array([lstm_pred_original_scale_total_t])\n",
    "            \n",
    "        if gt_total_times_for_lstm_training.size > 0 and lstm_pred_original_scale_total_t.size > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1); \n",
    "            plt.hist(gt_total_times_for_lstm_training, bins=30, alpha=0.7, label='GT Total Times (Max Cumul.)')\n",
    "            plt.hist(lstm_pred_original_scale_total_t, bins=30, alpha=0.7, label='LSTM Pred Total Times (Original Scale)')\n",
    "            plt.xlabel('Total Time'); plt.ylabel('Frequency'); plt.title('Distribution of Total Times'); plt.legend()\n",
    "            plt.subplot(1, 2, 2); \n",
    "            if len(gt_total_times_for_lstm_training) == len(lstm_pred_original_scale_total_t):\n",
    "                errors_total_time = gt_total_times_for_lstm_training - lstm_pred_original_scale_total_t\n",
    "                plt.hist(errors_total_time, bins=30, alpha=0.7, color='red')\n",
    "                plt.xlabel('Prediction Error (GT Max Cumul. - Pred)'); plt.ylabel('Frequency'); plt.title('LSTM Total Time Prediction Errors')\n",
    "                if errors_total_time.size > 0: \n",
    "                    mean_error_val = errors_total_time.mean(); plt.axvline(mean_error_val, color='k', ls='--', lw=1, label=f'Mean Error: {mean_error_val:.2f}')\n",
    "                plt.legend()\n",
    "            else: print(\"Warning: Mismatch length GT total times and predictions for error histogram.\")\n",
    "            plt.tight_layout(); plt.savefig('lstm_total_time_prediction_analysis_v9.png'); print(\"Saved total time prediction analysis plot.\"); plt.close()\n",
    "        else: print(\"Warning: Not enough data for total time distribution plots.\")\n",
    "    else: print(\"Warning: LSTM model, input data, or scaler missing for total time prediction plot.\")\n",
    "    print(\"Visualizations for LSTM completed!\")\n",
    "\n",
    "# %%\n",
    "def main_lstm_total_time_flow():\n",
    "    try:\n",
    "        transformer_predictions_file = \"combined_model_results_SN_175651_175974_182625.csv\" \n",
    "        if not os.path.exists(transformer_predictions_file):\n",
    "            print(f\"Error: Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "            print(\"Creating DUMMY CSV for testing flow.\")\n",
    "            dummy_data = []\n",
    "            for seq_idx in range(300): \n",
    "                num_steps = np.random.randint(10, 60) \n",
    "                steps = np.arange(1, num_steps + 1)\n",
    "                gt_increments = np.random.lognormal(mean=2.0, sigma=0.7, size=num_steps) + 0.1 \n",
    "                gt_increments = np.maximum(gt_increments, 0.01) \n",
    "                gt_cumulative = np.cumsum(gt_increments)\n",
    "                \n",
    "                raw_props = np.random.rand(num_steps) + 0.05 \n",
    "                pred_proportions = raw_props / raw_props.sum() \n",
    "                \n",
    "                actual_sequence_total_time = gt_cumulative[-1] if num_steps > 0 else 1.0\n",
    "                actual_sequence_total_time = max(actual_sequence_total_time, 1.0) \n",
    "\n",
    "                dummy_transformer_effective_total_time = actual_sequence_total_time * np.random.normal(loc=1.0, scale=0.4) \n",
    "                dummy_transformer_effective_total_time = max(dummy_transformer_effective_total_time, 0.1)\n",
    "\n",
    "                pred_increments_from_transformer = pred_proportions * dummy_transformer_effective_total_time\n",
    "                pred_cumulative_from_transformer = np.cumsum(pred_increments_from_transformer)\n",
    "\n",
    "                for s_idx in range(num_steps):\n",
    "                    dummy_data.append({\n",
    "                        'Sequence': seq_idx, 'Step': steps[s_idx], 'SourceID': f'MRI_DUMMY_{s_idx%5 +1}',\n",
    "                        'Predicted_Proportion': pred_proportions[s_idx], \n",
    "                        'Predicted_Increment': pred_increments_from_transformer[s_idx],\n",
    "                        'Predicted_Cumulative': pred_cumulative_from_transformer[s_idx], \n",
    "                        'GroundTruth_Increment': gt_increments[s_idx], \n",
    "                        'GroundTruth_Cumulative': gt_cumulative[s_idx]  })\n",
    "            if not dummy_data: \n",
    "                 dummy_data.append({ 'Sequence': 0, 'Step': 1, 'SourceID': 'MRI_DUMMY_0', 'Predicted_Proportion': 1.0, \n",
    "                                     'Predicted_Increment': 10.0, 'Predicted_Cumulative': 10.0, \n",
    "                                     'GroundTruth_Increment': 10.0, 'GroundTruth_Cumulative': 10.0})\n",
    "            dummy_df = pd.DataFrame(dummy_data)\n",
    "            dummy_df.to_csv(transformer_predictions_file, index=False)\n",
    "            print(f\"Dummy '{transformer_predictions_file}' created with {len(dummy_df)} rows and {len(dummy_df['Sequence'].unique())} sequences.\")\n",
    "        \n",
    "        lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
    "            transformer_predictions_file, epochs=200, batch_size=32 ) \n",
    "        \n",
    "        if lstm_model is None or processed_lstm_data is None:\n",
    "            print(\"LSTM training failed or returned None. Exiting.\"); return\n",
    "\n",
    "        refined_results_df = generate_refined_predictions_with_lstm(lstm_model, processed_lstm_data)\n",
    "        if not refined_results_df.empty:\n",
    "            print(\"\\nSample of Refined Predictions (LSTM Total Time Approach - v9):\")\n",
    "            display_cols = [ 'Sequence', 'Step', 'SourceID', \n",
    "                             'Predicted_Increment', 'LSTM_Predicted_Increment', 'GroundTruth_Increment', \n",
    "                             'Predicted_Cumulative', 'LSTM_Predicted_Cumulative', 'GroundTruth_Cumulative',\n",
    "                             'LSTM_Predicted_TotalTime', 'Increment_Improvement_Pct']\n",
    "            actual_display_cols = [col for col in display_cols if col in refined_results_df.columns]\n",
    "            print(refined_results_df[actual_display_cols].head(20)) # Show more rows to see NaN behavior\n",
    "            print(\"\\nGenerating visualizations for LSTM (total time approach - v9)...\")\n",
    "            visualize_lstm_results(refined_results_df, processed_lstm_data, lstm_model, lstm_history)\n",
    "        else: print(\"No refined predictions were generated by the LSTM flow.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM (total time) main function: {e}\"); import traceback; traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for LSTM training...\n",
      "Processing data from: combined_model_results_SN_175651_175974_182625.csv\n",
      "Error in LSTM (total time) main function: CSV must contain 'Predicted_Proportion' column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_5448\\317463272.py\", line 527, in main_lstm_total_time_flow\n",
      "    lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_5448\\317463272.py\", line 215, in train_total_time_lstm\n",
      "    data_for_lstm = process_input_data_for_lstm(transformer_predictions_file)\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_5448\\317463272.py\", line 109, in process_input_data_for_lstm\n",
      "    raise ValueError(f\"CSV must contain '{col}' column.\")\n",
      "ValueError: CSV must contain 'Predicted_Proportion' column.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main_lstm_total_time_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
