{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Constants and Configuration ---\n",
    "\n",
    "# This should match the MAX_SEQ_LEN from the Transformer model for consistency.\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "\n",
    "def load_and_preprocess_data(proportions_file, ground_truth_file):\n",
    "    \"\"\"\n",
    "    Loads the predicted proportions from the first model and aligns them with the\n",
    "    true total time for each sequence. It also extracts the number of steps as a\n",
    "    key feature.\n",
    "    \n",
    "    Args:\n",
    "        proportions_file (str): Path to the CSV file containing predicted proportions.\n",
    "        ground_truth_file (str): Path to the original data file to get the true total time.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - Padded sequences of proportions (X_seq).\n",
    "        - An array of the number of steps for each sequence (X_steps).\n",
    "        - An array of total times (y).\n",
    "        - The dataframe from the proportions_file for final output generation.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(proportions_file):\n",
    "        print(f\"❌ Error: Proportions file not found at '{proportions_file}'\")\n",
    "        return None, None, None, None\n",
    "    if not os.path.exists(ground_truth_file):\n",
    "        print(f\"❌ Error: Ground truth data file not found at '{ground_truth_file}'\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Load the predictions from the first model\n",
    "    props_df = pd.read_csv(proportions_file)\n",
    "    \n",
    "    # Load the original data to get the true total time\n",
    "    truth_df = pd.read_csv(ground_truth_file)\n",
    "    \n",
    "    # --- Re-calculate the true total time using the definitive logic ---\n",
    "    truth_df['step_duration'] = truth_df.groupby('SeqOrder')['timediff'].diff().fillna(truth_df['timediff'])\n",
    "    truth_df['step_duration'] = truth_df['step_duration'].clip(lower=0)\n",
    "    truth_df['Step'] = truth_df.groupby('SeqOrder').cumcount()\n",
    "    \n",
    "    end_marker_step = truth_df[truth_df['sourceID'] == 10].groupby('SeqOrder')['Step'].first()\n",
    "    truth_df['end_marker_step'] = truth_df['SeqOrder'].map(end_marker_step)\n",
    "    truth_df.loc[truth_df['Step'] > truth_df['end_marker_step'], 'step_duration'] = 0\n",
    "    \n",
    "    total_times = truth_df.groupby('SeqOrder')['step_duration'].sum()\n",
    "\n",
    "    # --- Prepare data for the LSTM ---\n",
    "    X_sequences = []\n",
    "    X_num_steps = []\n",
    "    \n",
    "    # The input features are the predicted proportions and the number of steps\n",
    "    for _, g in props_df.groupby('SeqOrder'):\n",
    "        X_sequences.append(g['predicted_proportion'].values.reshape(-1, 1))\n",
    "        X_num_steps.append(len(g)) # Add the number of steps as a feature\n",
    "    \n",
    "    # The target variable is the true total time\n",
    "    y_total_times = props_df['SeqOrder'].unique()\n",
    "    y_sequences = np.array([total_times.get(seq_id, 0) for seq_id in y_total_times])\n",
    "\n",
    "    # Pad the proportion sequences to a uniform length\n",
    "    X_padded_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        X_sequences, maxlen=MAX_SEQ_LEN, padding='post', dtype='float32'\n",
    "    )\n",
    "    \n",
    "    X_steps_arr = np.array(X_num_steps, dtype='float32').reshape(-1, 1)\n",
    "\n",
    "    print(f\"Successfully processed {len(X_padded_seq)} sequences.\")\n",
    "    \n",
    "    return X_padded_seq, X_steps_arr, y_sequences.reshape(-1, 1), props_df\n",
    "\n",
    "\n",
    "# --- 3. LSTM Model Architecture ---\n",
    "\n",
    "def build_lstm_model(sequence_shape, scalar_shape):\n",
    "    \"\"\"\n",
    "    Builds the simplified, dual-input LSTM model for total time prediction.\n",
    "    \"\"\"\n",
    "    # --- Input Branch 1: Sequence Data (Proportions) ---\n",
    "    sequence_input = layers.Input(shape=sequence_shape, name='sequence_input')\n",
    "    masked_sequence = layers.Masking(mask_value=0.)(sequence_input)\n",
    "    # Simplified to one LSTM layer, which is more appropriate for a small dataset\n",
    "    lstm_out = layers.LSTM(32, return_sequences=False)(masked_sequence)\n",
    "    \n",
    "    # --- Input Branch 2: Scalar Data (Number of Steps) ---\n",
    "    scalar_input = layers.Input(shape=scalar_shape, name='scalar_input')\n",
    "    \n",
    "    # --- Merged Branch ---\n",
    "    concatenated = layers.concatenate([lstm_out, scalar_input])\n",
    "    x = layers.Dense(16, activation='relu')(concatenated)\n",
    "    \n",
    "    # The final output layer has one neuron.\n",
    "    outputs = layers.Dense(1, name='total_time_output')(x) # Linear activation for scaled output\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[sequence_input, scalar_input], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- 4. Training and Prediction Orchestration ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the data processing, training, and prediction.\"\"\"\n",
    "    \n",
    "    # Input file from the previous (Transformer) model\n",
    "    proportions_file = 'prediction_176401_proportions_final_all.csv'\n",
    "    # Original data file to get the ground truth total time\n",
    "    ground_truth_file = 'data/176401/encoded_176401_condensed_full.csv'\n",
    "    # Final output file\n",
    "    output_predictions_file = 'prediction_176401_total_time_all.csv'\n",
    "    \n",
    "    X_seq, X_steps, y, props_df = load_and_preprocess_data(proportions_file, ground_truth_file)\n",
    "    if X_seq is None:\n",
    "        return\n",
    "\n",
    "    # --- Prepare data for training with scaling ---\n",
    "    X_seq_train, X_seq_val, X_steps_train, X_steps_val, y_train, y_val = train_test_split(\n",
    "        X_seq, X_steps, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # --- Scaling Features ---\n",
    "    # Scale the scalar input (number of steps)\n",
    "    scaler_steps = StandardScaler()\n",
    "    X_steps_train_scaled = scaler_steps.fit_transform(X_steps_train)\n",
    "    X_steps_val_scaled = scaler_steps.transform(X_steps_val)\n",
    "    \n",
    "    # Scale the target variable (total time)\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    \n",
    "    print(f\"\\nData shapes (Train): X_seq={X_seq_train.shape}, X_steps={X_steps_train_scaled.shape}, y={y_train_scaled.shape}\")\n",
    "    print(f\"Data shapes (Val):   X_seq={X_seq_val.shape}, X_steps={X_steps_val_scaled.shape}, y={y_val_scaled.shape}\")\n",
    "\n",
    "    # Define the input shapes for the dual-input model\n",
    "    sequence_shape = X_seq_train.shape[1:]\n",
    "    scalar_shape = (1,)\n",
    "    model = build_lstm_model(sequence_shape, scalar_shape)\n",
    "    \n",
    "    # Compile the model with Mean Squared Error, as we are now predicting a scaled value.\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), # A slightly higher learning rate can help\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"\\n--- Starting LSTM Model Training ---\")\n",
    "    model.fit(\n",
    "        [X_seq_train, X_steps_train_scaled],\n",
    "        y_train_scaled,\n",
    "        validation_data=([X_seq_val, X_steps_val_scaled], y_val_scaled),\n",
    "        epochs=200, # Increased epochs\n",
    "        batch_size=32,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)]\n",
    "    )\n",
    "    print(\"--- LSTM Model Training Finished ---\\n\")\n",
    "\n",
    "    # --- Generate Predictions and Create Final Output ---\n",
    "    print(\"--- Generating total time predictions for the entire dataset ---\")\n",
    "    # Scale the full X_steps data before prediction\n",
    "    X_steps_scaled = scaler_steps.transform(X_steps)\n",
    "    scaled_predictions = model.predict([X_seq, X_steps_scaled])\n",
    "    \n",
    "    # IMPORTANT: Inverse transform the predictions to get them back to the original scale (seconds)\n",
    "    predicted_times = scaler_y.inverse_transform(scaled_predictions).flatten()\n",
    "    \n",
    "    # Create a mapping from SeqOrder to the predicted time\n",
    "    seq_order_to_time = dict(zip(props_df['SeqOrder'].unique(), predicted_times))\n",
    "    \n",
    "    # Add a new column for the predicted total time\n",
    "    props_df['predicted_total_time'] = np.nan\n",
    "\n",
    "    # Find the index of the first occurrence of sourceID 10 for each sequence\n",
    "    end_marker_indices = props_df[props_df['sourceID'] == 10].groupby('SeqOrder')['Step'].idxmin()\n",
    "\n",
    "    # Place the predicted total time on the row with sourceID == 10\n",
    "    for seq_order, idx in end_marker_indices.items():\n",
    "        if seq_order in seq_order_to_time:\n",
    "            props_df.loc[idx, 'predicted_total_time'] = seq_order_to_time[seq_order]\n",
    "\n",
    "    # Save the final, merged dataframe to a new CSV file\n",
    "    props_df.to_csv(output_predictions_file, index=False)\n",
    "    print(f\"✅ Final predictions with total time saved to '{output_predictions_file}'\")\n",
    "\n",
    "    print(\"\\n--- Sample of Final Predictions ---\")\n",
    "    # Display rows around a sequence end to verify the output\n",
    "    sample_output = props_df[props_df['SeqOrder'] == 0]\n",
    "    print(sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 35 sequences.\n",
      "\n",
      "Data shapes (Train): X_seq=(28, 128, 1), X_steps=(28, 1), y=(28, 1)\n",
      "Data shapes (Val):   X_seq=(7, 128, 1), X_steps=(7, 1), y=(7, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ sequence_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequence_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequence_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Any</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> │ masking_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ any_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ scalar_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ scalar_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ total_time_output   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ sequence_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ sequence_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ masking_2 (\u001b[38;5;33mMasking\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ sequence_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ any_2 (\u001b[38;5;33mAny\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m4,352\u001b[0m │ masking_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ any_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ scalar_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ scalar_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m544\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ total_time_output   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,913</span> (19.19 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,913\u001b[0m (19.19 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,913</span> (19.19 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,913\u001b[0m (19.19 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting LSTM Model Training ---\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['sequence_input', 'scalar_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 1.1225 - mae: 0.5854 - val_loss: 21.9973 - val_mae: 1.9875\n",
      "Epoch 2/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.0902 - mae: 0.5983 - val_loss: 21.7844 - val_mae: 2.0173\n",
      "Epoch 3/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.0638 - mae: 0.6221 - val_loss: 21.5106 - val_mae: 2.0505\n",
      "Epoch 4/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.0370 - mae: 0.6460 - val_loss: 21.3903 - val_mae: 2.1036\n",
      "Epoch 5/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.0133 - mae: 0.6714 - val_loss: 21.3847 - val_mae: 2.1697\n",
      "Epoch 6/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.0053 - mae: 0.7082 - val_loss: 21.4308 - val_mae: 2.1878\n",
      "Epoch 7/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.0024 - mae: 0.7123 - val_loss: 21.5324 - val_mae: 2.1748\n",
      "Epoch 8/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 1.0000 - mae: 0.6970 - val_loss: 21.7198 - val_mae: 2.1429\n",
      "Epoch 9/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.9974 - mae: 0.6648 - val_loss: 21.8763 - val_mae: 2.1283\n",
      "Epoch 10/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9990 - mae: 0.6516 - val_loss: 21.8745 - val_mae: 2.1483\n",
      "Epoch 11/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.9995 - mae: 0.6607 - val_loss: 21.8357 - val_mae: 2.1616\n",
      "Epoch 12/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.9951 - mae: 0.6667 - val_loss: 21.7983 - val_mae: 2.1630\n",
      "Epoch 13/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.9923 - mae: 0.6687 - val_loss: 21.7268 - val_mae: 2.1680\n",
      "Epoch 14/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.9911 - mae: 0.6745 - val_loss: 21.6710 - val_mae: 2.1705\n",
      "Epoch 15/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.9908 - mae: 0.6797 - val_loss: 21.6558 - val_mae: 2.1643\n",
      "Epoch 16/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9916 - mae: 0.6775 - val_loss: 21.6649 - val_mae: 2.1613\n",
      "Epoch 17/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.9914 - mae: 0.6746 - val_loss: 21.6895 - val_mae: 2.1620\n",
      "Epoch 18/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.9891 - mae: 0.6724 - val_loss: 21.7196 - val_mae: 2.1609\n",
      "Epoch 19/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.9889 - mae: 0.6707 - val_loss: 21.7345 - val_mae: 2.1654\n",
      "Epoch 20/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9885 - mae: 0.6727 - val_loss: 21.7421 - val_mae: 2.1723\n",
      "Epoch 21/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.9876 - mae: 0.6756 - val_loss: 21.7661 - val_mae: 2.1704\n",
      "Epoch 22/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.9875 - mae: 0.6737 - val_loss: 21.7974 - val_mae: 2.1612\n",
      "Epoch 23/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.9872 - mae: 0.6686 - val_loss: 21.8050 - val_mae: 2.1571\n",
      "Epoch 24/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9866 - mae: 0.6668 - val_loss: 21.7817 - val_mae: 2.1599\n",
      "Epoch 25/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9863 - mae: 0.6697 - val_loss: 21.7517 - val_mae: 2.1636\n",
      "--- LSTM Model Training Finished ---\n",
      "\n",
      "--- Generating total time predictions for the entire dataset ---\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FDC9415510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 166ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['sequence_input', 'scalar_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001FDC9415510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "✅ Final predictions with total time saved to 'prediction_176401_total_time.csv'\n",
      "\n",
      "--- Sample of Final Predictions ---\n",
      "    SeqOrder  Step  sourceID  timediff  step_duration  true_proportion  \\\n",
      "0          0     0        11         0            0.0         0.000000   \n",
      "1          0     1         4         4            4.0         0.011799   \n",
      "2          0     2         5        13            9.0         0.026549   \n",
      "3          0     3         5        14            1.0         0.002950   \n",
      "4          0     4         5        28           14.0         0.041298   \n",
      "5          0     5         0        28            0.0         0.000000   \n",
      "6          0     6         1        36            8.0         0.023599   \n",
      "7          0     7         1        45            9.0         0.026549   \n",
      "8          0     8         1       106           61.0         0.179941   \n",
      "9          0     9         5       140           34.0         0.100295   \n",
      "10         0    10         1       195           55.0         0.162242   \n",
      "11         0    11         4       269           74.0         0.218289   \n",
      "12         0    12         5       270            1.0         0.002950   \n",
      "13         0    13         5       276            6.0         0.017699   \n",
      "14         0    14         4       276            0.0         0.000000   \n",
      "15         0    15         5       278            2.0         0.005900   \n",
      "16         0    16         0       279            1.0         0.002950   \n",
      "17         0    17         8       279            0.0         0.000000   \n",
      "18         0    18         4       279            0.0         0.000000   \n",
      "19         0    19         5       285            6.0         0.017699   \n",
      "20         0    20         0       315           30.0         0.088496   \n",
      "21         0    21         0       315            0.0         0.000000   \n",
      "22         0    22         9       315            0.0         0.000000   \n",
      "23         0    23         0       321            6.0         0.017699   \n",
      "24         0    24        10       339           18.0         0.053097   \n",
      "\n",
      "    predicted_proportion  predicted_total_time  \n",
      "0               0.038990                   NaN  \n",
      "1               0.038989                   NaN  \n",
      "2               0.038989                   NaN  \n",
      "3               0.038989                   NaN  \n",
      "4               0.038989                   NaN  \n",
      "5               0.038988                   NaN  \n",
      "6               0.038988                   NaN  \n",
      "7               0.038988                   NaN  \n",
      "8               0.038989                   NaN  \n",
      "9               0.038989                   NaN  \n",
      "10              0.038989                   NaN  \n",
      "11              0.040261                   NaN  \n",
      "12              0.040894                   NaN  \n",
      "13              0.041260                   NaN  \n",
      "14              0.038989                   NaN  \n",
      "15              0.038989                   NaN  \n",
      "16              0.038989                   NaN  \n",
      "17              0.038989                   NaN  \n",
      "18              0.038989                   NaN  \n",
      "19              0.038989                   NaN  \n",
      "20              0.038988                   NaN  \n",
      "21              0.038988                   NaN  \n",
      "22              0.038989                   NaN  \n",
      "23              0.038988                   NaN  \n",
      "24              0.038989            342.595673  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
