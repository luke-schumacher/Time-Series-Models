{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Constants and Configuration ---\n",
    "\n",
    "# This should match the MAX_SEQ_LEN from the Transformer model for consistency.\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# --- 2. Data Loading and Preprocessing ---\n",
    "\n",
    "def load_and_preprocess_data(proportions_file, ground_truth_file):\n",
    "    \"\"\"\n",
    "    Loads predicted proportions, extracts statistical features, and aligns them with\n",
    "    the true total time for each sequence.\n",
    "    \n",
    "    Args:\n",
    "        proportions_file (str): Path to the CSV containing predicted proportions.\n",
    "        ground_truth_file (str): Path to the original data file for true total time.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - Padded sequences of proportions (X_seq).\n",
    "        - An array of step counts for each sequence (X_steps).\n",
    "        - An array of statistical features for each sequence (X_stats).\n",
    "        - An array of total times (y).\n",
    "        - The dataframe from the proportions_file for final output generation.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(proportions_file):\n",
    "        print(f\"❌ Error: Proportions file not found at '{proportions_file}'\")\n",
    "        return None, None, None, None, None\n",
    "    if not os.path.exists(ground_truth_file):\n",
    "        print(f\"❌ Error: Ground truth data file not found at '{ground_truth_file}'\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    props_df = pd.read_csv(proportions_file)\n",
    "    truth_df = pd.read_csv(ground_truth_file)\n",
    "    \n",
    "    # --- Re-calculate the true total time using the definitive logic ---\n",
    "    truth_df['step_duration'] = truth_df.groupby('SeqOrder')['timediff'].diff().fillna(truth_df['timediff'])\n",
    "    truth_df['step_duration'] = truth_df['step_duration'].clip(lower=0)\n",
    "    truth_df['Step'] = truth_df.groupby('SeqOrder').cumcount()\n",
    "    \n",
    "    end_marker_step = truth_df[truth_df['sourceID'] == 10].groupby('SeqOrder')['Step'].first()\n",
    "    truth_df['end_marker_step'] = truth_df['SeqOrder'].map(end_marker_step)\n",
    "    truth_df.loc[truth_df['Step'] > truth_df['end_marker_step'], 'step_duration'] = 0\n",
    "    \n",
    "    total_times = truth_df.groupby('SeqOrder')['step_duration'].sum()\n",
    "\n",
    "    # --- Prepare data for the LSTM ---\n",
    "    X_sequences, X_num_steps, X_stats = [], [], []\n",
    "    \n",
    "    for _, g in props_df.groupby('SeqOrder'):\n",
    "        proportions = g['predicted_proportion'].values\n",
    "        X_sequences.append(proportions.reshape(-1, 1))\n",
    "        X_num_steps.append(len(g))\n",
    "        \n",
    "        # --- Feature Engineering: Create a richer set of statistical features ---\n",
    "        stats = [\n",
    "            np.mean(proportions),\n",
    "            np.std(proportions),\n",
    "            np.max(proportions),\n",
    "            np.percentile(proportions, 25), # 25th percentile\n",
    "            np.median(proportions),      # 50th percentile\n",
    "            np.percentile(proportions, 75)  # 75th percentile\n",
    "        ]\n",
    "        X_stats.append(stats)\n",
    "    \n",
    "    y_total_times = props_df['SeqOrder'].unique()\n",
    "    y_sequences = np.array([total_times.get(seq_id, 0) for seq_id in y_total_times])\n",
    "\n",
    "    X_padded_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        X_sequences, maxlen=MAX_SEQ_LEN, padding='post', dtype='float32'\n",
    "    )\n",
    "    \n",
    "    X_steps_arr = np.array(X_num_steps, dtype='float32').reshape(-1, 1)\n",
    "    X_stats_arr = np.array(X_stats, dtype='float32')\n",
    "\n",
    "    print(f\"Successfully processed {len(X_padded_seq)} sequences.\")\n",
    "    \n",
    "    return X_padded_seq, X_steps_arr, X_stats_arr, y_sequences.reshape(-1, 1), props_df\n",
    "\n",
    "\n",
    "# --- 3. Residual Model Architecture with Attention ---\n",
    "\n",
    "def build_residual_attention_model(sequence_shape, scalar_shape, stats_shape):\n",
    "    \"\"\"\n",
    "    Builds a multi-input model with a residual architecture.\n",
    "    \"\"\"\n",
    "    # --- Define All Inputs ---\n",
    "    sequence_input = layers.Input(shape=sequence_shape, name='sequence_input')\n",
    "    scalar_input = layers.Input(shape=scalar_shape, name='scalar_input')\n",
    "    stats_input = layers.Input(shape=stats_shape, name='stats_input')\n",
    "\n",
    "    # --- Branch 1: Baseline Model (using non-sequential features) ---\n",
    "    baseline_features = layers.concatenate([scalar_input, stats_input])\n",
    "    baseline_out = layers.Dense(32, activation='relu')(baseline_features)\n",
    "    baseline_out = layers.Dense(16, activation='relu')(baseline_out)\n",
    "    baseline_prediction = layers.Dense(1, name='baseline_prediction')(baseline_out)\n",
    "\n",
    "    # --- Branch 2: Specialist Model (predicting the residual) ---\n",
    "    masked_sequence = layers.Masking(mask_value=0.)(sequence_input)\n",
    "    gru_out = layers.Bidirectional(layers.GRU(64, return_sequences=True))(masked_sequence)\n",
    "    \n",
    "    # Using MultiHeadAttention for robust self-attention\n",
    "    # The output of Bidirectional GRU is 128 (64 forward + 64 backward)\n",
    "    attention_out = layers.MultiHeadAttention(num_heads=4, key_dim=128)(query=gru_out, value=gru_out, key=gru_out)\n",
    "    context_vector = layers.GlobalAveragePooling1D()(attention_out)\n",
    "    \n",
    "    # The specialist branch also gets the non-sequential features to have full context\n",
    "    specialist_features = layers.concatenate([context_vector, scalar_input, stats_input])\n",
    "    x = layers.Dense(64, activation='relu')(specialist_features)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    residual_prediction = layers.Dense(1, name='residual_prediction')(x)\n",
    "\n",
    "    # --- Final Output ---\n",
    "    # The final prediction is the sum of the baseline and the specialist's correction\n",
    "    final_prediction = layers.Add(name='final_prediction')([baseline_prediction, residual_prediction])\n",
    "    \n",
    "    model = tf.keras.Model(\n",
    "        inputs=[sequence_input, scalar_input, stats_input], \n",
    "        outputs=final_prediction\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 4. Visualization Function ---\n",
    "\n",
    "def create_visualizations(results_df, output_dir='visualizations'):\n",
    "    \"\"\"Generates and saves plots comparing true vs. predicted total time.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    true_times = results_df['true_total_time']\n",
    "    predicted_times = results_df['predicted_total_time']\n",
    "    \n",
    "    # --- Scatter Plot ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(true_times, predicted_times, alpha=0.6, label='Predictions')\n",
    "    # Add a line for perfect prediction\n",
    "    lims = [\n",
    "        np.min([plt.xlim(), plt.ylim()]),\n",
    "        np.max([plt.xlim(), plt.ylim()]),\n",
    "    ]\n",
    "    plt.plot(lims, lims, 'r--', alpha=0.75, zorder=0, label='Perfect Prediction')\n",
    "    plt.xlabel(\"True Total Time (seconds)\")\n",
    "    plt.ylabel(\"Predicted Total Time (seconds)\")\n",
    "    plt.title(\"True vs. Predicted Total Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, 'true_vs_predicted_scatter.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Error Histogram ---\n",
    "    errors = predicted_times - true_times\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=30, alpha=0.7) # Increased bins for more detail\n",
    "    plt.xlabel(\"Prediction Error (Predicted - True)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Prediction Errors\")\n",
    "    plt.grid(True)\n",
    "    plt.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "    plt.savefig(os.path.join(output_dir, 'prediction_error_histogram.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"✅ Basic visualizations saved to '{output_dir}' directory.\")\n",
    "\n",
    "def create_advanced_visualizations(results_df, output_dir='visualizations'):\n",
    "    \"\"\"Generates and saves advanced diagnostic plots.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    true_times = results_df['true_total_time']\n",
    "    predicted_times = results_df['predicted_total_time']\n",
    "    residuals = true_times - predicted_times\n",
    "\n",
    "    # --- Residuals vs. Predicted Plot ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=predicted_times, y=residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Total Time (seconds)\")\n",
    "    plt.ylabel(\"Residuals (True - Predicted)\")\n",
    "    plt.title(\"Residuals vs. Predicted Values\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, 'residuals_vs_predicted.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Predicted vs. True Distribution Plot ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(true_times, color=\"blue\", label='True Values', kde=True, stat=\"density\", linewidth=0)\n",
    "    sns.histplot(predicted_times, color=\"red\", label='Predicted Values', kde=True, stat=\"density\", linewidth=0)\n",
    "    plt.title(\"Distribution of Predicted vs. True Values\")\n",
    "    plt.xlabel(\"Total Time (seconds)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, 'predicted_vs_true_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"✅ Advanced visualizations saved to '{output_dir}' directory.\")\n",
    "\n",
    "\n",
    "# --- 5. Training and Prediction Orchestration ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the data processing, training, and prediction.\"\"\"\n",
    "    \n",
    "    proportions_file = 'prediction_176401_proportions_final_all.csv'\n",
    "    ground_truth_file = 'data/176401/encoded_176401_condensed_full.csv'\n",
    "    output_predictions_file = 'prediction_176401_total_time_full.csv'\n",
    "    \n",
    "    X_seq, X_steps, X_stats, y, props_df = load_and_preprocess_data(proportions_file, ground_truth_file)\n",
    "    if X_seq is None:\n",
    "        return\n",
    "\n",
    "    # --- Prepare data for training with scaling ---\n",
    "    X_seq_train, X_seq_val, X_steps_train, X_steps_val, X_stats_train, X_stats_val, y_train, y_val = train_test_split(\n",
    "        X_seq, X_steps, X_stats, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # --- Scaling Features ---\n",
    "    scaler_steps = StandardScaler()\n",
    "    X_steps_train_scaled = scaler_steps.fit_transform(X_steps_train)\n",
    "    X_steps_val_scaled = scaler_steps.transform(X_steps_val)\n",
    "\n",
    "    scaler_stats = StandardScaler()\n",
    "    X_stats_train_scaled = scaler_stats.fit_transform(X_stats_train)\n",
    "    X_stats_val_scaled = scaler_stats.transform(X_stats_val)\n",
    "    \n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_val_scaled = scaler_y.transform(y_val)\n",
    "    \n",
    "    # Define input shapes\n",
    "    sequence_shape, scalar_shape, stats_shape = X_seq_train.shape[1:], (1,), X_stats_train.shape[1:]\n",
    "    model = build_residual_attention_model(sequence_shape, scalar_shape, stats_shape)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.Huber(),\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    model.fit(\n",
    "        [X_seq_train, X_steps_train_scaled, X_stats_train_scaled],\n",
    "        y_train_scaled,\n",
    "        validation_data=([X_seq_val, X_steps_val_scaled, X_stats_val_scaled], y_val_scaled),\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)]\n",
    "    )\n",
    "    print(\"--- LSTM Model Training Finished ---\\n\")\n",
    "\n",
    "    # --- Generate Predictions and Create Final Output ---\n",
    "    print(\"--- Generating total time predictions for the entire dataset ---\")\n",
    "    X_steps_scaled = scaler_steps.transform(X_steps)\n",
    "    X_stats_scaled = scaler_stats.transform(X_stats)\n",
    "    scaled_predictions = model.predict([X_seq, X_steps_scaled, X_stats_scaled])\n",
    "    \n",
    "    predicted_times = scaler_y.inverse_transform(scaled_predictions).flatten()\n",
    "    \n",
    "    seq_order_to_time = dict(zip(props_df['SeqOrder'].unique(), predicted_times))\n",
    "    \n",
    "    props_df['predicted_total_time'] = np.nan\n",
    "    end_marker_indices = props_df[props_df['sourceID'] == 10].groupby('SeqOrder')['Step'].idxmin()\n",
    "\n",
    "    for seq_order, idx in end_marker_indices.items():\n",
    "        if seq_order in seq_order_to_time:\n",
    "            props_df.loc[idx, 'predicted_total_time'] = seq_order_to_time[seq_order]\n",
    "\n",
    "    props_df.to_csv(output_predictions_file, index=False)\n",
    "    print(f\"✅ Final predictions with total time saved to '{output_predictions_file}'\")\n",
    "\n",
    "    # --- Create Results DataFrame for Visualization ---\n",
    "    true_total_times_all = scaler_y.inverse_transform(y).flatten()\n",
    "    results_df = pd.DataFrame({\n",
    "        'SeqOrder': props_df['SeqOrder'].unique(),\n",
    "        'true_total_time': true_total_times_all,\n",
    "        'predicted_total_time': predicted_times\n",
    "    })\n",
    "    \n",
    "    create_visualizations(results_df)\n",
    "    create_advanced_visualizations(results_df)\n",
    "\n",
    "    print(\"\\n--- Sample of Final Predictions ---\")\n",
    "    # Display results for a sequence to verify the output format\n",
    "    if not props_df.empty:\n",
    "        first_seq_order = props_df['SeqOrder'].iloc[0]\n",
    "        print(props_df[props_df['SeqOrder'] == first_seq_order])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 223 sequences.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 227\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Define input shapes\u001b[39;00m\n\u001b[0;32m    226\u001b[0m sequence_shape, scalar_shape, stats_shape \u001b[38;5;241m=\u001b[39m X_seq_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:], (\u001b[38;5;241m1\u001b[39m,), X_stats_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 227\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_residual_attention_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    230\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m),\n\u001b[0;32m    231\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mHuber(),\n\u001b[0;32m    232\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    233\u001b[0m )\n\u001b[0;32m    234\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[35], line 100\u001b[0m, in \u001b[0;36mbuild_residual_attention_model\u001b[1;34m(sequence_shape, scalar_shape, stats_shape)\u001b[0m\n\u001b[0;32m     98\u001b[0m masked_sequence \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mMasking(mask_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m)(sequence_input)\n\u001b[0;32m     99\u001b[0m gru_out \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mBidirectional(layers\u001b[38;5;241m.\u001b[39mGRU(\u001b[38;5;241m64\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))(masked_sequence)\n\u001b[1;32m--> 100\u001b[0m attention_out \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgru_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgru_out\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m context_vector \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mGlobalAveragePooling1D()(attention_out)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# The specialist branch also gets the non-sequential features to have full context\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
