{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Constants and Configuration ---\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "# --- UPDATED FEATURES ---\n",
    "# Give the model more context by including the sourceID and step number.\n",
    "FEATURE_COLUMNS = ['predicted_proportion', 'sourceID', 'Step']\n",
    "\n",
    "# --- 2. Data Loading and Preparation for LSTM ---\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads proportion data, determines the true total time for each sequence\n",
    "    using the final cumulative timediff, and prepares data for the LSTM.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"❌ Error: Predictions file not found at '{file_path}'\")\n",
    "        return None, None, None\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['true_total_time'] = df.groupby('SeqOrder')['timediff'].transform('max')\n",
    "\n",
    "    grouped = df.groupby('SeqOrder')\n",
    "    sequences = []\n",
    "    total_times = []\n",
    "    \n",
    "    print(f\"Processing {len(grouped)} sequences for the LSTM model...\")\n",
    "    for _, group in grouped:\n",
    "        sequences.append(group[FEATURE_COLUMNS].values)\n",
    "        total_times.append(group['true_total_time'].iloc[0])\n",
    "\n",
    "    return sequences, np.array(total_times), df\n",
    "\n",
    "# --- 3. LSTM Model Architecture (Improved) ---\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Builds a more powerful LSTM model for total time prediction.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(None, input_shape[-1]))\n",
    "    masking = layers.Masking(mask_value=0.0)(inputs)\n",
    "    # Increased complexity to capture non-linear patterns\n",
    "    lstm1 = layers.LSTM(64, return_sequences=True)(masking)\n",
    "    lstm2 = layers.LSTM(32, return_sequences=False)(lstm1)\n",
    "    # Dropout helps prevent overfitting\n",
    "    dropout = layers.Dropout(0.2)(lstm2)\n",
    "    dense1 = layers.Dense(16, activation='relu')(dropout)\n",
    "    outputs = layers.Dense(1)(dense1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# --- 4. Visualization Function ---\n",
    "\n",
    "def create_visualizations(history, results_df):\n",
    "    \"\"\"\n",
    "    Generates and saves plots for model analysis for ALL sequences.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Visualizations ---\")\n",
    "    \n",
    "    output_plot_dir = 'sequence_plots'\n",
    "    os.makedirs(output_plot_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot 1: Training & Validation Loss (Global)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_loss_plot.png')\n",
    "    print(\"✅ Saved training loss plot.\")\n",
    "    plt.close()\n",
    "\n",
    "    all_seq_orders = results_df['SeqOrder'].unique()\n",
    "    print(f\"Generating comparison plots for {len(all_seq_orders)} sequences...\")\n",
    "\n",
    "    for seq_order in all_seq_orders:\n",
    "        sample_df = results_df[results_df['SeqOrder'] == seq_order]\n",
    "\n",
    "        # Plot 2: Cumulative Time Comparison\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.plot(sample_df['Step'], sample_df['timediff'], label='True Cumulative Time', marker='o')\n",
    "        plt.plot(sample_df['Step'], sample_df['predicted_cumulative_time'], label='Predicted Cumulative Time', marker='x', linestyle='--')\n",
    "        plt.title(f'Cumulative Time Comparison for Sequence {seq_order}')\n",
    "        plt.xlabel('Step in Sequence')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_plot_dir, f'cumulative_time_comparison_seq_{seq_order}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot 3: Time Increment Comparison\n",
    "        true_increments = sample_df['timediff'].diff().fillna(sample_df['timediff'].iloc[0])\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.plot(sample_df['Step'], true_increments, label='True Time Increment', marker='o')\n",
    "        plt.plot(sample_df['Step'], sample_df['predicted_time_increment'], label='Predicted Time Increment', marker='x', linestyle='--')\n",
    "        plt.title(f'Time Increment Comparison for Sequence {seq_order}')\n",
    "        plt.xlabel('Step in Sequence')\n",
    "        plt.ylabel('Time (seconds)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_plot_dir, f'time_increment_comparison_seq_{seq_order}.png'))\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ Saved individual sequence plots to the '{output_plot_dir}' directory.\")\n",
    "\n",
    "    # Plot 4: Total Time Prediction Analysis (Global)\n",
    "    total_time_analysis = results_df[['SeqOrder', 'true_total_time', 'predicted_total_time']].drop_duplicates()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(total_time_analysis['true_total_time'], total_time_analysis['predicted_total_time'], alpha=0.6, label='Predictions')\n",
    "    plt.plot([0, total_time_analysis['true_total_time'].max()], [0, total_time_analysis['true_total_time'].max()], color='red', linestyle='--', label='Perfect Prediction Line')\n",
    "    plt.title('True vs. Predicted Total Time')\n",
    "    plt.xlabel('True Total Time (s)')\n",
    "    plt.ylabel('Predicted Total Time (s)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.savefig('total_time_prediction_analysis.png')\n",
    "    print(\"✅ Saved total time prediction analysis plot.\")\n",
    "    plt.close()\n",
    "\n",
    "# --- 5. Main Orchestration ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the data processing, training, and prediction.\"\"\"\n",
    "    \n",
    "    proportions_file = 'prediction_176401_proportions.csv'\n",
    "    output_file = 'predictions_total_time_176401.csv'\n",
    "    \n",
    "    sequences, total_times, original_df = load_and_prepare_data(proportions_file)\n",
    "    if sequences is None: return\n",
    "\n",
    "    indices = np.arange(len(sequences))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_unpadded = [sequences[i] for i in train_indices]\n",
    "    y_train_raw = total_times[train_indices]\n",
    "    X_val_unpadded = [sequences[i] for i in val_indices]\n",
    "    y_val_raw = total_times[val_indices]\n",
    "    \n",
    "    # --- SCALING INPUT AND OUTPUT ---\n",
    "    # Scale the input features (X)\n",
    "    scaler_X = StandardScaler()\n",
    "    # Fit the scaler ONLY on the training data to avoid data leakage\n",
    "    X_train_combined = np.vstack(X_train_unpadded)\n",
    "    scaler_X.fit(X_train_combined)\n",
    "    \n",
    "    # Transform both training and validation sets\n",
    "    X_train_scaled = [scaler_X.transform(seq) for seq in X_train_unpadded]\n",
    "    X_val_scaled = [scaler_X.transform(seq) for seq in X_val_unpadded]\n",
    "\n",
    "    # Scale the output/target variable (y)\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train = scaler_y.fit_transform(y_train_raw.reshape(-1, 1))\n",
    "    y_val = scaler_y.transform(y_val_raw.reshape(-1, 1))\n",
    "\n",
    "    X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train_scaled, maxlen=MAX_SEQ_LEN, padding='post', dtype='float32')\n",
    "    X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val_scaled, maxlen=MAX_SEQ_LEN, padding='post', dtype='float32')\n",
    "\n",
    "    model = build_lstm_model(X_train.shape)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\n--- Starting LSTM Model Training ---\")\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        callbacks=[early_stopping, reduce_lr_on_plateau]\n",
    "    )\n",
    "    print(\"--- LSTM Model Training Finished ---\\n\")\n",
    "\n",
    "    print(\"--- Generating total time predictions for all sequences ---\")\n",
    "    # Scale and pad all sequences for the final prediction\n",
    "    all_sequences_scaled = [scaler_X.transform(seq) for seq in sequences]\n",
    "    X_all_padded = tf.keras.preprocessing.sequence.pad_sequences(all_sequences_scaled, maxlen=MAX_SEQ_LEN, padding='post', dtype='float32')\n",
    "    \n",
    "    scaled_predictions = model.predict(X_all_padded)\n",
    "    predicted_total_times = scaler_y.inverse_transform(scaled_predictions).flatten()\n",
    "\n",
    "    seq_order_to_time = {seq_order: time for seq_order, time in zip(original_df['SeqOrder'].unique(), predicted_total_times)}\n",
    "    \n",
    "    results_df = original_df.copy()\n",
    "    results_df['predicted_total_time'] = results_df['SeqOrder'].map(seq_order_to_time)\n",
    "    results_df['predicted_time_increment'] = results_df['predicted_proportion'] * results_df['predicted_total_time']\n",
    "    results_df['predicted_cumulative_time'] = results_df.groupby('SeqOrder')['predicted_time_increment'].cumsum()\n",
    "\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Final predictions saved to '{output_file}'\")\n",
    "    \n",
    "    create_visualizations(history, results_df)\n",
    "    \n",
    "    print(\"\\n--- Sample of Final Output ---\")\n",
    "    print(results_df[['SeqOrder', 'Step', 'timediff', 'true_total_time', 'predicted_total_time', 'predicted_cumulative_time']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Step'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17036\\272064486.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Note: To run this, you need to have the data file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 'data/176401/encoded_176401_condensed.csv' available in the correct path.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# If the file is not found, the script will print an error and exit.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# You may need to create the 'data' directory structure or update the path in main().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17036\\1974352058.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;31m# Make sure this file exists or update the path.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[0mdata_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data/176401/encoded_176401_condensed.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0moutput_predictions_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'prediction_176401_proportions_corrected.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproportions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequences\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17036\\1974352058.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# --- Correctly Calculate Step Durations and Proportions ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Sort by sequence and step to ensure correct diff calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SeqOrder'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# 'timediff' is cumulative. Calculate individual step duration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# The first step's duration is its own timediff value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7168\u001b[0m                 \u001b[1;34mf\"Length of ascending ({len(ascending)})\"\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7169\u001b[0m                 \u001b[1;34mf\" != length of by ({len(by)})\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7170\u001b[0m             )\n\u001b[0;32m   7171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7172\u001b[1;33m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7174\u001b[0m             \u001b[1;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m-> 7172\u001b[1;33m         \u001b[1;33m...\u001b[0m     \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_natsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Step'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Note: To run this, you need to have the data file \n",
    "    # 'data/176401/encoded_176401_condensed.csv' available in the correct path.\n",
    "    # If the file is not found, the script will print an error and exit.\n",
    "    # You may need to create the 'data' directory structure or update the path in main().\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
