{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import os # For checking file existence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # For target scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute increments and cumulative times from proportions and total time\n",
    "def calculate_times_from_proportions(proportions_per_step, total_time_for_sequence, mask_per_step):\n",
    "    \"\"\"\n",
    "    Calculates time increments and cumulative times from step-wise proportions and a total time.\n",
    "    \"\"\"\n",
    "    proportions_tf = tf.cast(proportions_per_step, tf.float32)\n",
    "    total_time_tf = tf.cast(total_time_for_sequence, tf.float32)\n",
    "    mask_tf = tf.cast(mask_per_step, tf.float32)\n",
    "\n",
    "    if len(tf.shape(total_time_tf)) == 1:\n",
    "        total_time_tf = tf.expand_dims(total_time_tf, axis=-1)\n",
    "\n",
    "    masked_proportions = proportions_tf * mask_tf\n",
    "    row_sums = tf.reduce_sum(masked_proportions, axis=1, keepdims=True)\n",
    "    row_sums = tf.where(tf.equal(row_sums, 0), tf.ones_like(row_sums), row_sums) \n",
    "    normalized_proportions = masked_proportions / row_sums\n",
    "    increments = normalized_proportions * total_time_tf \n",
    "    cumulative_times = tf.cumsum(increments, axis=1)\n",
    "    \n",
    "    increments *= mask_tf\n",
    "    cumulative_times *= mask_tf\n",
    "    normalized_proportions *= mask_tf\n",
    "\n",
    "    return normalized_proportions, increments, cumulative_times\n",
    "\n",
    "# %%\n",
    "class TotalTimeLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Two-stream model:\n",
    "    1. A BiLSTM with Attention for sequential features.\n",
    "    2. An MLP for global/static features.\n",
    "    The outputs are then combined for a final prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=256, \n",
    "                 dense_units_1=128, \n",
    "                 dense_units_2=64, \n",
    "                 dropout_rate=0.4):\n",
    "        super(TotalTimeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.dense_units_1 = dense_units_1\n",
    "        self.dense_units_2 = dense_units_2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.bilstm_output_dim = hidden_units * 2\n",
    "\n",
    "        # --- Stream 1: Sequential Features ---\n",
    "        self.bi_lstm_layer = layers.Bidirectional(\n",
    "            layers.LSTM(self.hidden_units, return_sequences=True, dropout=self.dropout_rate, recurrent_dropout=0.25),\n",
    "            name=\"bidirectional_lstm_v21\"\n",
    "        )\n",
    "        self.W1 = layers.Dense(self.bilstm_output_dim, name=\"attention_dense_W1\")\n",
    "        self.W2 = layers.Dense(self.bilstm_output_dim, name=\"attention_dense_W2\")\n",
    "        self.V = layers.Dense(1, name=\"attention_dense_V\")\n",
    "        \n",
    "        # --- Stream 2: Global Features ---\n",
    "        self.global_dense_1 = layers.Dense(64, activation='relu', name=\"global_dense_1\")\n",
    "        self.global_dropout_1 = layers.Dropout(self.dropout_rate, name=\"global_dropout_1\")\n",
    "        self.global_dense_2 = layers.Dense(32, activation='relu', name=\"global_dense_2\")\n",
    "\n",
    "        # --- Combination and Final Prediction Head ---\n",
    "        self.concat_layer = layers.Concatenate(name=\"concatenate_features_v21\")\n",
    "        self.final_dense_1 = layers.Dense(\n",
    "            self.dense_units_1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), name=\"final_dense_1_v21\"\n",
    "        )\n",
    "        self.final_dropout_1 = layers.Dropout(self.dropout_rate, name=\"final_dropout_1_v21\")\n",
    "        self.final_dense_2 = layers.Dense(\n",
    "            self.dense_units_2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), name=\"final_dense_2_v21\"\n",
    "        )\n",
    "        self.final_dropout_2 = layers.Dropout(self.dropout_rate, name=\"final_dropout_2_v21\")\n",
    "        self.total_time_head = layers.Dense(1, activation='linear', name=\"total_time_dense_v21\") \n",
    "    \n",
    "    def call(self, inputs, training=False): \n",
    "        sequence_input, global_features_input = inputs \n",
    "        mask_bool_seq = tf.reduce_any(tf.not_equal(sequence_input, 0.0), axis=-1)\n",
    "        \n",
    "        # --- Stream 1: Process Sequential Data ---\n",
    "        lstm_output = self.bi_lstm_layer(sequence_input, mask=mask_bool_seq, training=training)\n",
    "        query_summary = tf.reduce_mean(lstm_output, axis=1)\n",
    "        query_with_time_axis = tf.expand_dims(query_summary, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(lstm_output) + self.W2(query_with_time_axis)))\n",
    "        mask_for_scores = tf.expand_dims(tf.cast(mask_bool_seq, tf.float32), -1)\n",
    "        masked_score = score - (1. - mask_for_scores) * 1e9\n",
    "        attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * lstm_output, axis=1)\n",
    "        \n",
    "        # --- Stream 2: Process Global Data ---\n",
    "        global_x = self.global_dense_1(global_features_input)\n",
    "        global_x = self.global_dropout_1(global_x, training=training)\n",
    "        global_embedding = self.global_dense_2(global_x)\n",
    "\n",
    "        # --- Combine and Predict ---\n",
    "        combined_features = self.concat_layer([context_vector, global_embedding])\n",
    "        \n",
    "        x = self.final_dense_1(combined_features)\n",
    "        x = self.final_dropout_1(x, training=training)\n",
    "        x = self.final_dense_2(x)\n",
    "        x = self.final_dropout_2(x, training=training)\n",
    "        total_time_pred = self.total_time_head(x)\n",
    "        \n",
    "        return total_time_pred\n",
    "\n",
    "# %%\n",
    "def process_input_data_for_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process the transformer predictions CSV to prepare data for LSTM training,\n",
    "    incorporating new sequential and global features.\n",
    "    \"\"\"\n",
    "    print(f\"Processing data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "    df = pd.read_csv(transformer_predictions_file)\n",
    "    \n",
    "    required_cols = ['Predicted_Proportion', 'GroundTruth_Cumulative', 'GroundTruth_Increment', 'Sequence', 'Step']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns: raise ValueError(f\"CSV must contain '{col}' column.\")\n",
    "\n",
    "    sequences = df['Sequence'].unique()\n",
    "    \n",
    "    X_sequential_data_list = []; X_global_features_list = []\n",
    "    y_total_times_list = []; original_dfs_list = [] \n",
    "    transformer_proportions_list = []; ground_truth_increments_list = []\n",
    "    ground_truth_cumulative_list = [] \n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_df = df[df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        if seq_df.empty:\n",
    "            original_dfs_list.append(seq_df); continue\n",
    "        original_dfs_list.append(seq_df) \n",
    "\n",
    "        actual_sequence_length = float(len(seq_df))\n",
    "        \n",
    "        # --- Sequential Features ---\n",
    "        props_for_seq = seq_df['Predicted_Proportion'].values\n",
    "        \n",
    "        current_max_steps = seq_df['Step'].max()\n",
    "        if current_max_steps == 0: current_max_steps = 1 \n",
    "        \n",
    "        max_timediff = seq_df['timediff'].max()\n",
    "        if max_timediff == 0: max_timediff = 1\n",
    "\n",
    "        sequential_features = np.column_stack([\n",
    "            props_for_seq, \n",
    "            seq_df['Step'].values / current_max_steps,\n",
    "            seq_df['timediff'].values / max_timediff, # Normalized timediff\n",
    "            seq_df['PTAB'].values # PTAB added directly\n",
    "        ])\n",
    "        X_sequential_data_list.append(sequential_features)\n",
    "        \n",
    "        # --- Global Features ---\n",
    "        sum_props = np.sum(props_for_seq)\n",
    "        mean_props = np.mean(props_for_seq) if actual_sequence_length > 0 else 0.0\n",
    "        std_props = np.std(props_for_seq) if actual_sequence_length > 1 else 0.0\n",
    "        max_prop = np.max(props_for_seq) if actual_sequence_length > 0 else 0.0\n",
    "        \n",
    "        first_row = seq_df.iloc[0]\n",
    "        global_features_for_seq = np.array([\n",
    "            sum_props, mean_props, std_props, max_prop,\n",
    "            first_row['patient_height'], first_row['patient_weight'],\n",
    "            first_row['patient_age'], first_row['patient_gender']\n",
    "        ], dtype=np.float32)\n",
    "        X_global_features_list.append(global_features_for_seq)\n",
    "        \n",
    "        gt_cumulative_for_seq = seq_df['GroundTruth_Cumulative'].values\n",
    "        total_time_for_seq = np.max(gt_cumulative_for_seq) if len(gt_cumulative_for_seq) > 0 else 0.0\n",
    "        y_total_times_list.append(total_time_for_seq)\n",
    "        \n",
    "        transformer_proportions_list.append(props_for_seq)\n",
    "        ground_truth_increments_list.append(seq_df['GroundTruth_Increment'].values)\n",
    "        ground_truth_cumulative_list.append(gt_cumulative_for_seq) \n",
    "\n",
    "    if not X_sequential_data_list: raise ValueError(\"No valid sequences processed.\")\n",
    "\n",
    "    y_total_times_array_unpadded = np.array(y_total_times_list)\n",
    "    print(f\"\\nStatistics for TARGET y_total_times_list (max GT_Cumulative per seq, {len(y_total_times_array_unpadded)} sequences):\")\n",
    "    print(f\"  Mean: {np.mean(y_total_times_array_unpadded):.4f}, Std Dev: {np.std(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Min: {np.min(y_total_times_array_unpadded):.4f}, Max: {np.max(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Number of zeros (<=1e-6): {np.sum(y_total_times_array_unpadded <= 1e-6)}\\n\")\n",
    "\n",
    "    max_length_sequential = max(len(x) for x in X_sequential_data_list) if X_sequential_data_list else 0\n",
    "    if max_length_sequential == 0: raise ValueError(\"Max length for sequential features is 0.\")\n",
    "    num_sequential_features = X_sequential_data_list[0].shape[1]\n",
    "    num_global_features = X_global_features_list[0].shape[0]\n",
    "\n",
    "    X_sequential_padded = np.zeros((len(X_sequential_data_list), max_length_sequential, num_sequential_features), dtype=np.float32)\n",
    "    masks_padded_float = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32) \n",
    "    transformer_proportions_padded = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "    gt_increments_padded_original = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "    gt_cumulative_padded_original = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "\n",
    "    for i, seq_data in enumerate(X_sequential_data_list):\n",
    "        seq_len = len(seq_data)\n",
    "        if seq_len > 0:\n",
    "            X_sequential_padded[i, :seq_len, :] = seq_data\n",
    "            masks_padded_float[i, :seq_len] = 1.0 \n",
    "            transformer_proportions_padded[i, :seq_len] = transformer_proportions_list[i]\n",
    "            gt_increments_padded_original[i, :seq_len] = ground_truth_increments_list[i]\n",
    "            gt_cumulative_padded_original[i, :seq_len] = ground_truth_cumulative_list[i]\n",
    "        \n",
    "    y_total_times_np = np.array(y_total_times_list, dtype=np.float32)\n",
    "    X_global_features_np = np.array(X_global_features_list, dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        'X_sequential_input': X_sequential_padded, \n",
    "        'X_global_features_input': X_global_features_np, \n",
    "        'y_lstm_target_total_times': y_total_times_np,\n",
    "        'masks_for_calc': masks_padded_float, \n",
    "        'sequences_ids': sequences, \n",
    "        'original_dfs': original_dfs_list, \n",
    "        'transformer_proportions_padded': transformer_proportions_padded, \n",
    "        'gt_increments_padded_original': gt_increments_padded_original,\n",
    "        'gt_cumulative_padded_original': gt_cumulative_padded_original,\n",
    "        'max_len_sequential': max_length_sequential,\n",
    "        'num_sequential_features': num_sequential_features,\n",
    "        'num_global_features': num_global_features\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def train_total_time_lstm(transformer_predictions_file, epochs=50, batch_size=32, val_split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Train the enhanced LSTM model to predict total_time, using a log transform on the target.\n",
    "    \"\"\"\n",
    "    print(\"Processing data for LSTM training...\")\n",
    "    data_for_lstm = process_input_data_for_lstm(transformer_predictions_file)\n",
    "    \n",
    "    print(f\"Num sequential features: {data_for_lstm['num_sequential_features']}, Max seq length: {data_for_lstm['max_len_sequential']}\")\n",
    "    print(f\"Num global features: {data_for_lstm['num_global_features']}\")\n",
    "\n",
    "    lstm_model = TotalTimeLSTM(hidden_units=256, dense_units_1=128, dense_units_2=64, dropout_rate=0.4) \n",
    "    \n",
    "    X_sequential_all = data_for_lstm['X_sequential_input']\n",
    "    X_global_all = data_for_lstm['X_global_features_input']\n",
    "    \n",
    "    # --- LOG TRANSFORM TARGET VARIABLE ---\n",
    "    y_targets_all = data_for_lstm['y_lstm_target_total_times']\n",
    "    y_targets_log = np.log1p(y_targets_all) # Use log(1 + x) to handle zeros\n",
    "    \n",
    "    if len(X_sequential_all) > 0:\n",
    "        sample_seq_input_for_build = tf.convert_to_tensor(X_sequential_all[:1], dtype=tf.float32)\n",
    "        sample_glob_input_for_build = tf.convert_to_tensor(X_global_all[:1], dtype=tf.float32)\n",
    "        _ = lstm_model((sample_seq_input_for_build, sample_glob_input_for_build)) \n",
    "        print(\"\\nEnhanced LSTM Model Summary (v21 - after sample call):\")\n",
    "        lstm_model.summary(expand_nested=True) \n",
    "    else: print(\"Warning: No data to build model with sample call.\")\n",
    "\n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse' ) \n",
    "    \n",
    "    global_feature_scaler = StandardScaler()\n",
    "    indices = np.arange(len(X_sequential_all))\n",
    "\n",
    "    if len(X_sequential_all) < 10: \n",
    "        print(\"Warning: Very few samples (<10), using all for training.\")\n",
    "        X_train_seq = X_sequential_all\n",
    "        X_train_glob_scaled = global_feature_scaler.fit_transform(X_global_all)\n",
    "        y_train_log = y_targets_log\n",
    "        validation_data_for_fit = None\n",
    "    else:\n",
    "        train_indices, val_indices = train_test_split(indices, test_size=val_split_ratio, random_state=42, shuffle=True)\n",
    "        X_train_seq = X_sequential_all[train_indices]; X_val_seq = X_sequential_all[val_indices]\n",
    "        X_train_glob = X_global_all[train_indices]; X_val_glob = X_global_all[val_indices]\n",
    "        X_train_glob_scaled = global_feature_scaler.fit_transform(X_train_glob) \n",
    "        X_val_glob_scaled = global_feature_scaler.transform(X_val_glob)     \n",
    "        \n",
    "        y_train_log = y_targets_log[train_indices]\n",
    "        y_val_log = y_targets_log[val_indices]\n",
    "        \n",
    "        validation_data_for_fit = ([X_val_seq, X_val_glob_scaled], y_val_log) \n",
    "        print(f\"\\nManually split data: {len(X_train_seq)} train, {len(X_val_seq)} validation samples.\")\n",
    "        print(f\"Training target stats (log scale): Mean={np.mean(y_train_log):.2f}, Std={np.std(y_train_log):.2f}\")\n",
    "        print(f\"Validation target stats (log scale): Mean={np.mean(y_val_log):.2f}, Std={np.std(y_val_log):.2f}\\n\")\n",
    "\n",
    "    callbacks_list = [\n",
    "        EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1), \n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=1e-7, verbose=1) \n",
    "    ]\n",
    "    \n",
    "    print(\"Starting LSTM model training (with LOG-TRANSFORMED targets)...\")\n",
    "    history = lstm_model.fit(\n",
    "        [X_train_seq, X_train_glob_scaled], y_train_log,          \n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        validation_data=validation_data_for_fit, \n",
    "        callbacks=callbacks_list, verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM training finished.\")\n",
    "    data_for_lstm['global_feature_scaler'] = global_feature_scaler \n",
    "    return lstm_model, history, data_for_lstm\n",
    "\n",
    "# %%\n",
    "def generate_refined_predictions_with_lstm(lstm_model, processed_data):\n",
    "    \"\"\"\n",
    "    Generate refined time predictions, including additional specified columns from the original input.\n",
    "    Reverses the log transform on the predictions.\n",
    "    \"\"\"\n",
    "    print(\"Generating refined predictions using LSTM's total time...\")\n",
    "    \n",
    "    COLUMNS_TO_KEEP = ['timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to', 'PatientID_from', 'PatientID_to',\n",
    "                       'patient_height', 'patient_weight', 'patient_age', 'patient_gender']\n",
    "\n",
    "    X_sequential_input_all = processed_data['X_sequential_input']\n",
    "    X_global_features_input_all_unscaled = processed_data['X_global_features_input']\n",
    "    \n",
    "    global_feature_scaler = processed_data['global_feature_scaler']\n",
    "    X_global_features_input_all_scaled = global_feature_scaler.transform(X_global_features_input_all_unscaled)\n",
    "\n",
    "    # Model now predicts the LOG of the total time\n",
    "    lstm_predicted_log_total_times = lstm_model.predict(\n",
    "        [X_sequential_input_all, X_global_features_input_all_scaled] \n",
    "    ) \n",
    "    \n",
    "    # --- REVERSE THE LOG TRANSFORM ---\n",
    "    lstm_predicted_total_times_original_scale = np.expm1(lstm_predicted_log_total_times)\n",
    "    lstm_predicted_total_times_original_scale = np.squeeze(lstm_predicted_total_times_original_scale)\n",
    "    lstm_predicted_total_times_original_scale = np.maximum(0, lstm_predicted_total_times_original_scale) \n",
    "\n",
    "    transformer_step_proportions = processed_data['transformer_proportions_padded'] \n",
    "    masks_for_calc = processed_data['masks_for_calc'] \n",
    "\n",
    "    _, lstm_refined_increments, lstm_refined_cumulative = calculate_times_from_proportions(\n",
    "        transformer_step_proportions,\n",
    "        lstm_predicted_total_times_original_scale, \n",
    "        masks_for_calc \n",
    "    )\n",
    "\n",
    "    lstm_refined_increments_np = lstm_refined_increments.numpy()\n",
    "    lstm_refined_cumulative_np = lstm_refined_cumulative.numpy()\n",
    "\n",
    "    results_list_df = []\n",
    "    original_dfs_from_processing = processed_data['original_dfs'] \n",
    "    gt_total_times_all = processed_data['y_lstm_target_total_times'] \n",
    "\n",
    "    for i, seq_id in enumerate(processed_data['sequences_ids']):\n",
    "        if i >= len(original_dfs_from_processing): continue\n",
    "        original_seq_df = original_dfs_from_processing[i]\n",
    "        seq_len = len(original_seq_df)\n",
    "        if seq_len == 0:\n",
    "            if original_seq_df.empty: results_list_df.append(original_seq_df) \n",
    "            continue\n",
    "        if i >= len(lstm_predicted_total_times_original_scale): continue\n",
    "\n",
    "        output_dict = {\n",
    "            'Sequence': seq_id, 'Step': np.arange(1, seq_len + 1),\n",
    "            'SourceID': original_seq_df['SourceID'].values,\n",
    "            'GroundTruth_Increment': processed_data['gt_increments_padded_original'][i, :seq_len],\n",
    "            'GroundTruth_Cumulative': processed_data['gt_cumulative_padded_original'][i, :seq_len],\n",
    "            'LSTM_Predicted_Increment': lstm_refined_increments_np[i, :seq_len],\n",
    "            'LSTM_Predicted_Cumulative': lstm_refined_cumulative_np[i, :seq_len],\n",
    "            'LSTM_Predicted_TotalTime': np.full(seq_len, np.nan, dtype=np.float32),\n",
    "            'TotalTime_Difference': np.full(seq_len, np.nan, dtype=np.float32) \n",
    "        }\n",
    "        \n",
    "        for col in COLUMNS_TO_KEEP:\n",
    "            if col in original_seq_df.columns:\n",
    "                output_dict[col] = original_seq_df[col].values\n",
    "        \n",
    "        predicted_total = lstm_predicted_total_times_original_scale[i]\n",
    "        gt_total = gt_total_times_all[i]\n",
    "        \n",
    "        output_dict['LSTM_Predicted_TotalTime'][-1] = predicted_total\n",
    "        output_dict['TotalTime_Difference'][-1] = gt_total - predicted_total \n",
    "\n",
    "        transformer_pred_increment = original_seq_df['Predicted_Increment'].fillna(0).values\n",
    "        transformer_pred_cumulative = original_seq_df['Predicted_Cumulative'].fillna(0).values\n",
    "        diff_transformer_inc = np.abs(output_dict['GroundTruth_Increment'] - transformer_pred_increment)\n",
    "        diff_lstm_inc = np.abs(output_dict['GroundTruth_Increment'] - output_dict['LSTM_Predicted_Increment'])\n",
    "        output_dict['Increment_Improvement_Pct'] = np.where(diff_transformer_inc > 1e-6, (diff_transformer_inc - diff_lstm_inc) / diff_transformer_inc * 100, 0 )\n",
    "        diff_transformer_cum = np.abs(output_dict['GroundTruth_Cumulative'] - transformer_pred_cumulative)\n",
    "        diff_lstm_cum = np.abs(output_dict['GroundTruth_Cumulative'] - output_dict['LSTM_Predicted_Cumulative'])\n",
    "        output_dict['Cumulative_Improvement_Pct'] = np.where(diff_transformer_cum > 1e-6, (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100, 0 )\n",
    "        \n",
    "        clean_seq_df = pd.DataFrame(output_dict)\n",
    "        results_list_df.append(clean_seq_df)\n",
    "\n",
    "    if not results_list_df: return pd.DataFrame()\n",
    "    final_results_df = pd.concat(results_list_df, ignore_index=True)\n",
    "    \n",
    "    output_filename = 'predictions_lstm_refined_total_time_v21.csv' # Changed filename\n",
    "    final_results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Combined and refined predictions saved to {output_filename}\")\n",
    "    \n",
    "    if not final_results_df.empty:\n",
    "        original_df_full = pd.concat(original_dfs_from_processing, ignore_index=True)\n",
    "        merged_for_summary = pd.merge(final_results_df, original_df_full[['Sequence', 'Step', 'Predicted_Increment', 'Predicted_Cumulative']], on=['Sequence', 'Step'])\n",
    "        if 'Predicted_Increment' in merged_for_summary.columns and 'LSTM_Predicted_Increment' in merged_for_summary.columns:\n",
    "            mae_transformer = np.mean(np.abs(merged_for_summary['GroundTruth_Increment'] - merged_for_summary['Predicted_Increment']))\n",
    "            mae_lstm = np.mean(np.abs(merged_for_summary['GroundTruth_Increment'] - merged_for_summary['LSTM_Predicted_Increment']))\n",
    "            print(f\"\\nTransformer MAE (Increments): {mae_transformer:.4f}, LSTM-Refined MAE (Increments): {mae_lstm:.4f}\")\n",
    "            if mae_transformer > 1e-6: print(f\"Improvement (Increments): {(mae_transformer - mae_lstm) / mae_transformer * 100:.2f}%\")\n",
    "        if 'Predicted_Cumulative' in merged_for_summary.columns and 'LSTM_Predicted_Cumulative' in merged_for_summary.columns:\n",
    "            mae_transformer_cum = np.mean(np.abs(merged_for_summary['GroundTruth_Cumulative'] - merged_for_summary['Predicted_Cumulative']))\n",
    "            mae_lstm_cum = np.mean(np.abs(merged_for_summary['GroundTruth_Cumulative'] - merged_for_summary['LSTM_Predicted_Cumulative']))\n",
    "            print(f\"Transformer MAE (Cumulative): {mae_transformer_cum:.4f}, LSTM-Refined MAE (Cumulative): {mae_lstm_cum:.4f}\")\n",
    "            if mae_transformer_cum > 1e-6: print(f\"Improvement (Cumulative): {(mae_transformer_cum - mae_lstm_cum) / mae_transformer_cum * 100:.2f}%\")\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data['y_lstm_target_total_times'] \n",
    "    if len(lstm_predicted_total_times_original_scale) == len(gt_total_times_for_lstm_training):\n",
    "        mae_total_time_lstm = np.mean(np.abs(gt_total_times_for_lstm_training - lstm_predicted_total_times_original_scale))\n",
    "        print(f\"\\nLSTM MAE for Total Time (vs Max GT Cumulative): {mae_total_time_lstm:.4f}\")\n",
    "\n",
    "    return final_results_df\n",
    "\n",
    "# %%\n",
    "def visualize_lstm_results(results_df, processed_data, lstm_model, training_history):\n",
    "    if results_df.empty: print(\"Results DataFrame is empty, skipping visualizations.\"); return\n",
    "    print(\"Generating visualizations for LSTM results...\")\n",
    "    plt.style.use('ggplot')\n",
    "    if training_history and hasattr(training_history, 'history'):\n",
    "        if 'loss' in training_history.history and 'val_loss' in training_history.history:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "            plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "            if 'lr' in training_history.history:\n",
    "                ax2 = plt.gca().twinx(); ax2.plot(training_history.history['lr'], label='Learning Rate', color='g', linestyle='--')\n",
    "                ax2.set_ylabel('Learning Rate'); ax2.legend(loc='upper center')\n",
    "            plt.title('LSTM Model Loss (Predicting Scaled Total Time)') \n",
    "            plt.xlabel('Epoch'); plt.ylabel('Mean Squared Error (Scaled Loss)'); plt.legend(loc='upper left'); plt.tight_layout()\n",
    "            plt.savefig('lstm_total_time_training_loss_v21.png'); print(\"Saved LSTM training loss plot.\"); plt.close() # v21\n",
    "    else: print(\"Warning: Training history not available or malformed.\")\n",
    "\n",
    "    sample_sequence_ids = results_df['Sequence'].unique()\n",
    "    if len(sample_sequence_ids) == 0 : print(\"No sequences in results_df for plotting.\"); return\n",
    "    sample_sequence_ids = sample_sequence_ids[:min(5, len(sample_sequence_ids))]\n",
    "    \n",
    "    original_df_full = pd.concat(processed_data['original_dfs'], ignore_index=True)\n",
    "    plot_df = pd.merge(results_df, original_df_full[['Sequence', 'Step', 'Predicted_Cumulative', 'Predicted_Increment']], on=['Sequence', 'Step'], how='left')\n",
    "    \n",
    "    if len(sample_sequence_ids) > 0:\n",
    "        num_plots = len(sample_sequence_ids); fig_height = max(8, 3 * num_plots) \n",
    "        plt.figure(figsize=(15, fig_height)) # Cumulative Plot\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = plot_df[plot_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Cumulative'], 'o-', label='GT Cumul.', ms=4)\n",
    "            if 'Predicted_Cumulative' in plot_df.columns and 'Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Cumulative'], 's--', label='Transformer Cumul.', ms=4)\n",
    "            if 'LSTM_Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Cumulative'], '^-.', label='LSTM-Refined Cumul.', ms=4)\n",
    "            lstm_total_time_for_seq = seq_data_plot['LSTM_Predicted_TotalTime'].dropna().unique()\n",
    "            if len(lstm_total_time_for_seq) == 1: plt.axhline(y=lstm_total_time_for_seq[0], color='purple', linestyle=':', label=f'LSTM Total Pred: {lstm_total_time_for_seq[0]:.2f}')\n",
    "            plt.title(f'Cumulative Times: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Cumulative Time'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_cumulative_time_comparison_v21.png'); print(\"Saved cumulative time comparison plot.\"); plt.close() # v21\n",
    "\n",
    "        plt.figure(figsize=(15, fig_height)) # Increment Plot\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = plot_df[plot_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Increment'], 'o-', label='GT Incr.', ms=4)\n",
    "            if 'Predicted_Increment' in plot_df.columns and 'Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Increment'], 's--', label='Transformer Incr.', ms=4)\n",
    "            if 'LSTM_Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Increment'], '^-.', label='LSTM-Refined Incr.', ms=4)\n",
    "            plt.title(f'Time Increments: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Time Increment'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_increment_comparison_v21.png'); print(\"Saved increment comparison plot.\"); plt.close() # v21\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data.get('y_lstm_target_total_times', np.array([])) \n",
    "    if lstm_model is not None and 'X_sequential_input' in processed_data and 'X_global_features_input' in processed_data:\n",
    "        X_seq_tensor = tf.convert_to_tensor(processed_data['X_sequential_input'], dtype=tf.float32)\n",
    "        X_glob_unscaled = processed_data['X_global_features_input']\n",
    "        X_glob_scaled_for_plot = processed_data['global_feature_scaler'].transform(X_glob_unscaled)\n",
    "        X_glob_tensor = tf.convert_to_tensor(X_glob_scaled_for_plot, dtype=tf.float32)\n",
    "        \n",
    "        lstm_pred_log_total_t = lstm_model.predict([X_seq_tensor, X_glob_tensor])\n",
    "        lstm_pred_original_scale_total_t = np.expm1(lstm_pred_log_total_t).squeeze()\n",
    "        \n",
    "        if lstm_pred_original_scale_total_t.ndim == 0: lstm_pred_original_scale_total_t = np.array([lstm_pred_original_scale_total_t])\n",
    "            \n",
    "        if gt_total_times_for_lstm_training.size > 0 and lstm_pred_original_scale_total_t.size > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1); \n",
    "            plt.hist(gt_total_times_for_lstm_training, bins=30, alpha=0.7, label='GT Total Times (Max Cumul.)')\n",
    "            plt.hist(lstm_pred_original_scale_total_t, bins=30, alpha=0.7, label='LSTM Pred Total Times (Original Scale)')\n",
    "            plt.xlabel('Total Time'); plt.ylabel('Frequency'); plt.title('Distribution of Total Times'); plt.legend()\n",
    "            plt.subplot(1, 2, 2); \n",
    "            if len(gt_total_times_for_lstm_training) == len(lstm_pred_original_scale_total_t):\n",
    "                errors_total_time = gt_total_times_for_lstm_training - lstm_pred_original_scale_total_t\n",
    "                plt.hist(errors_total_time, bins=30, alpha=0.7, color='red')\n",
    "                plt.xlabel('Prediction Error (GT Max Cumul. - Pred)'); plt.ylabel('Frequency'); plt.title('LSTM Total Time Prediction Errors')\n",
    "                if errors_total_time.size > 0: \n",
    "                    mean_error_val = errors_total_time.mean(); plt.axvline(mean_error_val, color='k', ls='--', lw=1, label=f'Mean Error: {mean_error_val:.2f}')\n",
    "                plt.legend()\n",
    "            else: print(\"Warning: Mismatch length GT total times and predictions for error histogram.\")\n",
    "            plt.tight_layout(); plt.savefig('lstm_total_time_prediction_analysis_v21.png'); print(\"Saved total time prediction analysis plot.\"); plt.close() # v21\n",
    "        else: print(\"Warning: Not enough data for total time distribution plots.\")\n",
    "    else: print(\"Warning: LSTM model, input data, or scaler missing for total time prediction plot.\")\n",
    "    print(\"Visualizations for LSTM completed!\")\n",
    "\n",
    "# %%\n",
    "def main_lstm_total_time_flow():\n",
    "    try:\n",
    "        transformer_predictions_file = \"predictions_transformer_175651_with_details.csv\" \n",
    "        if not os.path.exists(transformer_predictions_file):\n",
    "            print(f\"Error: Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "            print(\"Creating DUMMY CSV for testing flow.\")\n",
    "            dummy_data = []\n",
    "            for seq_idx in range(300): \n",
    "                num_steps = np.random.randint(10, 60) \n",
    "                steps = np.arange(1, num_steps + 1)\n",
    "                gt_increments = np.random.lognormal(mean=2.0, sigma=0.7, size=num_steps) + 0.1 \n",
    "                gt_increments = np.maximum(gt_increments, 0.01) \n",
    "                gt_cumulative = np.cumsum(gt_increments)\n",
    "                \n",
    "                raw_props = np.random.rand(num_steps) + 0.05 \n",
    "                pred_proportions = raw_props / raw_props.sum() \n",
    "                \n",
    "                actual_sequence_total_time = gt_cumulative[-1] if num_steps > 0 else 1.0\n",
    "                actual_sequence_total_time = max(actual_sequence_total_time, 1.0) \n",
    "\n",
    "                dummy_transformer_effective_total_time = actual_sequence_total_time * np.random.normal(loc=1.0, scale=0.4) \n",
    "                dummy_transformer_effective_total_time = max(dummy_transformer_effective_total_time, 0.1)\n",
    "\n",
    "                pred_increments_from_transformer = pred_proportions * dummy_transformer_effective_total_time\n",
    "                pred_cumulative_from_transformer = np.cumsum(pred_increments_from_transformer)\n",
    "\n",
    "                for s_idx in range(num_steps):\n",
    "                    dummy_data.append({\n",
    "                        'Sequence': seq_idx, 'Step': steps[s_idx], 'SourceID': f'MRI_DUMMY_{s_idx%5 +1}',\n",
    "                        'Predicted_Proportion': pred_proportions[s_idx], \n",
    "                        'Predicted_Increment': pred_increments_from_transformer[s_idx],\n",
    "                        'Predicted_Cumulative': pred_cumulative_from_transformer[s_idx], \n",
    "                        'GroundTruth_Increment': gt_increments[s_idx], \n",
    "                        'GroundTruth_Cumulative': gt_cumulative[s_idx],\n",
    "                        'timediff': np.random.randint(100, 1000), 'PTAB': np.random.randint(-2000000, -100000),\n",
    "                        'BodyGroup_from': 'CHEST', 'BodyGroup_to': 'ABDOMEN',\n",
    "                        'PatientID_from': f'PAT_{seq_idx}', 'PatientID_to': f'PAT_{seq_idx}',\n",
    "                        'patient_height': np.random.randint(150, 200), 'patient_weight': np.random.randint(50, 100),\n",
    "                        'patient_age': np.random.randint(20, 80), 'patient_gender': np.random.randint(0, 2)\n",
    "                    })\n",
    "            if not dummy_data: \n",
    "                 dummy_data.append({ 'Sequence': 0, 'Step': 1, 'SourceID': 'MRI_DUMMY_0', 'Predicted_Proportion': 1.0, \n",
    "                                     'Predicted_Increment': 10.0, 'Predicted_Cumulative': 10.0, \n",
    "                                     'GroundTruth_Increment': 10.0, 'GroundTruth_Cumulative': 10.0,\n",
    "                                     'timediff': 500, 'PTAB': -1000000, 'BodyGroup_from': 'CHEST', 'BodyGroup_to': 'ABDOMEN',\n",
    "                                     'PatientID_from': 'PAT_0', 'PatientID_to': 'PAT_0',\n",
    "                                     'patient_height': 175, 'patient_weight': 75, 'patient_age': 50, 'patient_gender': 1})\n",
    "            dummy_df = pd.DataFrame(dummy_data)\n",
    "            dummy_df.to_csv(transformer_predictions_file, index=False)\n",
    "            print(f\"Dummy '{transformer_predictions_file}' created with {len(dummy_df)} rows and {len(dummy_df['Sequence'].unique())} sequences.\")\n",
    "        \n",
    "        lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
    "            transformer_predictions_file, epochs=200, batch_size=32 ) \n",
    "        \n",
    "        if lstm_model is None or processed_lstm_data is None:\n",
    "            print(\"LSTM training failed or returned None. Exiting.\"); return\n",
    "\n",
    "        refined_results_df = generate_refined_predictions_with_lstm(lstm_model, processed_lstm_data)\n",
    "        if not refined_results_df.empty:\n",
    "            print(\"\\nSample of Refined Predictions (LSTM Total Time Approach - v21):\")\n",
    "            display_cols = [ 'Sequence', 'Step', 'SourceID', 'timediff', 'PTAB', 'BodyGroup_from', 'PatientID_from',\n",
    "                             'LSTM_Predicted_Increment', 'GroundTruth_Increment', \n",
    "                             'LSTM_Predicted_Cumulative', 'GroundTruth_Cumulative',\n",
    "                             'LSTM_Predicted_TotalTime', 'TotalTime_Difference']\n",
    "            actual_display_cols = [col for col in display_cols if col in refined_results_df.columns]\n",
    "            print(refined_results_df[actual_display_cols].head(20))\n",
    "            print(\"\\nGenerating visualizations for LSTM (total time approach - v21)...\")\n",
    "            visualize_lstm_results(refined_results_df, processed_lstm_data, lstm_model, lstm_history)\n",
    "        else: print(\"No refined predictions were generated by the LSTM flow.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM (total time) main function: {e}\"); import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for LSTM training...\n",
      "Processing data from: predictions_transformer_175651_with_details.csv\n",
      "\n",
      "Statistics for TARGET y_total_times_list (max GT_Cumulative per seq, 101 sequences):\n",
      "  Mean: 809.5545, Std Dev: 869.1292\n",
      "  Min: 0.0000, Max: 4850.0000\n",
      "  Number of zeros (<=1e-6): 2\n",
      "\n",
      "Num sequential features: 4, Max seq length: 53\n",
      "Num global features: 8\n",
      "\n",
      "Enhanced LSTM Model Summary (v21 - after sample call):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"total_time_lstm_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"total_time_lstm_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_lstm_v21          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">534,528</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_V (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate_features_v21        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                   │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ final_dense_1_v21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │        <span style=\"color: #00af00; text-decoration-color: #00af00\">69,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ final_dropout_1_v21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ final_dense_2_v21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ final_dropout_2_v21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_v21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_lstm_v21          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m534,528\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W1 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W2 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_V (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │           \u001b[38;5;34m513\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_dense_1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_dropout_1 (\u001b[38;5;33mDropout\u001b[0m)      │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_dense_2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate_features_v21        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m544\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)                   │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ final_dense_1_v21 (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │        \u001b[38;5;34m69,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ final_dropout_1_v21 (\u001b[38;5;33mDropout\u001b[0m)   │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ final_dense_2_v21 (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ final_dropout_2_v21 (\u001b[38;5;33mDropout\u001b[0m)   │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_v21 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,141,090</span> (4.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,141,090\u001b[0m (4.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,141,090</span> (4.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,141,090\u001b[0m (4.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manually split data: 80 train, 21 validation samples.\n",
      "Training target stats (log scale): Mean=5.95, Std=1.51\n",
      "Validation target stats (log scale): Mean=6.64, Std=1.18\n",
      "\n",
      "Starting LSTM model training (with LOG-TRANSFORMED targets)...\n",
      "Epoch 1/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 335ms/step - loss: 29.6702 - val_loss: 8.5495 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 14.6084 - val_loss: 1.4009 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 10.3295 - val_loss: 3.4554 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 7.8910 - val_loss: 1.0913 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 6.1662 - val_loss: 2.8076 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 5.6008 - val_loss: 4.5758 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 4.8477 - val_loss: 4.8858 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 4.4882 - val_loss: 4.0634 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 3.7800 - val_loss: 2.5855 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 3.6716 - val_loss: 1.7645 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 4.3774 - val_loss: 1.6995 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 3.6765 - val_loss: 1.9588 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 4.1594 - val_loss: 2.7210 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 3.7836 - val_loss: 3.1308 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 4.0457 - val_loss: 2.9180 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 3.0886 - val_loss: 2.3629 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 3.6706 - val_loss: 2.2311 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 4.2076 - val_loss: 2.5431 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 3.5051\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 3.5189 - val_loss: 2.5374 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 3.1560 - val_loss: 2.3866 - learning_rate: 2.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 3.7804 - val_loss: 2.2100 - learning_rate: 2.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 3.3830 - val_loss: 2.0732 - learning_rate: 2.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 2.9023 - val_loss: 1.8902 - learning_rate: 2.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 3.0590 - val_loss: 1.8467 - learning_rate: 2.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 3.0750 - val_loss: 1.9848 - learning_rate: 2.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 3.4759 - val_loss: 2.1231 - learning_rate: 2.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 3.7222 - val_loss: 2.2270 - learning_rate: 2.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 2.9262 - val_loss: 2.2534 - learning_rate: 2.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 2.6903 - val_loss: 2.3204 - learning_rate: 2.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 2.8201 - val_loss: 2.3638 - learning_rate: 2.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 3.5577 - val_loss: 2.4125 - learning_rate: 2.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 3.0879 - val_loss: 2.3787 - learning_rate: 2.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 3.0796 - val_loss: 2.4008 - learning_rate: 2.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 2.5606\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 2.5822 - val_loss: 2.4113 - learning_rate: 2.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 2.5441 - val_loss: 2.4106 - learning_rate: 4.0000e-05\n",
      "Epoch 36/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 2.7481 - val_loss: 2.4135 - learning_rate: 4.0000e-05\n",
      "Epoch 37/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 2.7698 - val_loss: 2.4052 - learning_rate: 4.0000e-05\n",
      "Epoch 38/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 2.8519 - val_loss: 2.3944 - learning_rate: 4.0000e-05\n",
      "Epoch 39/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 3.6928 - val_loss: 2.3780 - learning_rate: 4.0000e-05\n",
      "Epoch 40/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 3.4253 - val_loss: 2.3858 - learning_rate: 4.0000e-05\n",
      "Epoch 41/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 3.1014 - val_loss: 2.4287 - learning_rate: 4.0000e-05\n",
      "Epoch 42/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 3.1463 - val_loss: 2.4592 - learning_rate: 4.0000e-05\n",
      "Epoch 43/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 2.9255 - val_loss: 2.5033 - learning_rate: 4.0000e-05\n",
      "Epoch 44/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 3.1639 - val_loss: 2.5246 - learning_rate: 4.0000e-05\n",
      "Epoch 44: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "LSTM training finished.\n",
      "Generating refined predictions using LSTM's total time...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step\n",
      "Combined and refined predictions saved to predictions_lstm_refined_total_time_v21.csv\n",
      "\n",
      "Transformer MAE (Increments): 95.1243, LSTM-Refined MAE (Increments): 98.5522\n",
      "Improvement (Increments): -3.60%\n",
      "Transformer MAE (Cumulative): 248.8449, LSTM-Refined MAE (Cumulative): 432.7517\n",
      "Improvement (Cumulative): -73.90%\n",
      "\n",
      "LSTM MAE for Total Time (vs Max GT Cumulative): 580.5671\n",
      "\n",
      "Sample of Refined Predictions (LSTM Total Time Approach - v21):\n",
      "    Sequence  Step      SourceID  timediff     PTAB BodyGroup_from  \\\n",
      "0          0     1   MRI_MSR_104       0.0  -708250           KNEE   \n",
      "1          0     2   MRI_FRR_257      14.0 -1608950           KNEE   \n",
      "2          0     3   MRI_FRR_264      24.0 -1608950           KNEE   \n",
      "3          0     4  MRI_MPT_1005      33.0 -1608950           KNEE   \n",
      "4          0     5    MRI_CCS_11      57.0 -1608950           KNEE   \n",
      "5          0     6   MRI_FRR_264      63.0 -1608950           KNEE   \n",
      "6          0     7   MRI_FRR_257     179.0      550           KNEE   \n",
      "7          0     8   MRI_FRR_264     181.0      550           KNEE   \n",
      "8          0     9   MRI_FRR_264     194.0      550           KNEE   \n",
      "9          0    10    MRI_CCS_11     210.0      550           KNEE   \n",
      "10         0    11   MRI_FRR_257     251.0     -550           KNEE   \n",
      "11         0    12   MRI_FRR_264     256.0     -550           KNEE   \n",
      "12         0    13   MRI_FRR_257     256.0  -591200           KNEE   \n",
      "13         0    14   MRI_FRR_264     257.0  -591200           KNEE   \n",
      "14         0    15   MRI_FRR_257     257.0  -579800           KNEE   \n",
      "15         0    16   MRI_FRR_264     261.0  -579800           KNEE   \n",
      "16         0    17   MRI_FRR_257     261.0  -473400           KNEE   \n",
      "17         0    18   MRI_FRR_264     262.0  -473400           KNEE   \n",
      "18         0    19    MRI_FRR_34     263.0  -473400           KNEE   \n",
      "19         0    20   MRI_FRR_257     263.0  -448700           KNEE   \n",
      "\n",
      "                              PatientID_from  LSTM_Predicted_Increment  \\\n",
      "0   8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "1   8a29d06a4cc1203e141061958fac39a8ad96b36c                 42.150982   \n",
      "2   8a29d06a4cc1203e141061958fac39a8ad96b36c                 25.290592   \n",
      "3   8a29d06a4cc1203e141061958fac39a8ad96b36c                 42.150982   \n",
      "4   8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "5   8a29d06a4cc1203e141061958fac39a8ad96b36c                 75.871765   \n",
      "6   8a29d06a4cc1203e141061958fac39a8ad96b36c                193.894531   \n",
      "7   8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "8   8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "9   8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "10  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "11  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "12  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "13  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "14  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "15  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "16  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "17  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "18  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "19  8a29d06a4cc1203e141061958fac39a8ad96b36c                  8.430197   \n",
      "\n",
      "    GroundTruth_Increment  LSTM_Predicted_Cumulative  GroundTruth_Cumulative  \\\n",
      "0                    14.0                   8.430197                    14.0   \n",
      "1                    10.0                  50.581177                    24.0   \n",
      "2                     9.0                  75.871765                    33.0   \n",
      "3                    24.0                 118.022751                    57.0   \n",
      "4                     6.0                 126.452950                    63.0   \n",
      "5                   116.0                 202.324707                   179.0   \n",
      "6                     2.0                 396.219238                   181.0   \n",
      "7                    13.0                 404.649445                   194.0   \n",
      "8                    16.0                 413.079651                   210.0   \n",
      "9                    41.0                 421.509857                   251.0   \n",
      "10                    5.0                 429.940063                   256.0   \n",
      "11                    0.0                 438.370270                   256.0   \n",
      "12                    1.0                 446.800476                   257.0   \n",
      "13                    0.0                 455.230682                   257.0   \n",
      "14                    4.0                 463.660889                   261.0   \n",
      "15                    0.0                 472.091095                   261.0   \n",
      "16                    1.0                 480.521301                   262.0   \n",
      "17                    1.0                 488.951508                   263.0   \n",
      "18                    0.0                 497.381714                   263.0   \n",
      "19                    6.0                 505.811920                   269.0   \n",
      "\n",
      "    LSTM_Predicted_TotalTime  TotalTime_Difference  \n",
      "0                        NaN                   NaN  \n",
      "1                        NaN                   NaN  \n",
      "2                        NaN                   NaN  \n",
      "3                        NaN                   NaN  \n",
      "4                        NaN                   NaN  \n",
      "5                        NaN                   NaN  \n",
      "6                        NaN                   NaN  \n",
      "7                        NaN                   NaN  \n",
      "8                        NaN                   NaN  \n",
      "9                        NaN                   NaN  \n",
      "10                       NaN                   NaN  \n",
      "11                       NaN                   NaN  \n",
      "12                       NaN                   NaN  \n",
      "13                       NaN                   NaN  \n",
      "14                       NaN                   NaN  \n",
      "15                       NaN                   NaN  \n",
      "16                       NaN                   NaN  \n",
      "17                       NaN                   NaN  \n",
      "18                       NaN                   NaN  \n",
      "19                       NaN                   NaN  \n",
      "\n",
      "Generating visualizations for LSTM (total time approach - v21)...\n",
      "Generating visualizations for LSTM results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_35688\\3519000422.py:365: RuntimeWarning: divide by zero encountered in divide\n",
      "  output_dict['Increment_Improvement_Pct'] = np.where(diff_transformer_inc > 1e-6, (diff_transformer_inc - diff_lstm_inc) / diff_transformer_inc * 100, 0 )\n",
      "C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_35688\\3519000422.py:368: RuntimeWarning: divide by zero encountered in divide\n",
      "  output_dict['Cumulative_Improvement_Pct'] = np.where(diff_transformer_cum > 1e-6, (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100, 0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LSTM training loss plot.\n",
      "Saved cumulative time comparison plot.\n",
      "Saved increment comparison plot.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Saved total time prediction analysis plot.\n",
      "Visualizations for LSTM completed!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main_lstm_total_time_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
