{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import os # For checking file existence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # For target scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute increments and cumulative times from proportions and total time\n",
    "def calculate_times_from_proportions(proportions_per_step, total_time_for_sequence, mask_per_step):\n",
    "    \"\"\"\n",
    "    Calculates time increments and cumulative times from step-wise proportions and a total time.\n",
    "    \"\"\"\n",
    "    proportions_tf = tf.cast(proportions_per_step, tf.float32)\n",
    "    total_time_tf = tf.cast(total_time_for_sequence, tf.float32)\n",
    "    mask_tf = tf.cast(mask_per_step, tf.float32)\n",
    "\n",
    "    if len(tf.shape(total_time_tf)) == 1:\n",
    "        total_time_tf = tf.expand_dims(total_time_tf, axis=-1)\n",
    "\n",
    "    masked_proportions = proportions_tf * mask_tf\n",
    "    row_sums = tf.reduce_sum(masked_proportions, axis=1, keepdims=True)\n",
    "    row_sums = tf.where(tf.equal(row_sums, 0), tf.ones_like(row_sums), row_sums) \n",
    "    normalized_proportions = masked_proportions / row_sums\n",
    "    increments = normalized_proportions * total_time_tf \n",
    "    cumulative_times = tf.cumsum(increments, axis=1)\n",
    "    \n",
    "    increments *= mask_tf\n",
    "    cumulative_times *= mask_tf\n",
    "    normalized_proportions *= mask_tf\n",
    "\n",
    "    return normalized_proportions, increments, cumulative_times\n",
    "\n",
    "# %%\n",
    "class TotalTimeLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Enhanced LSTM model with a manual Bahdanau-style Attention to predict the total time.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=256, \n",
    "                 dense_units_1=128, \n",
    "                 dense_units_2=64, \n",
    "                 dropout_rate=0.4):\n",
    "        super(TotalTimeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.dense_units_1 = dense_units_1\n",
    "        self.dense_units_2 = dense_units_2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.bilstm_output_dim = hidden_units * 2\n",
    "\n",
    "        # --- Layers for sequential input ---\n",
    "        self.bi_lstm_layer = layers.Bidirectional(\n",
    "            layers.LSTM(self.hidden_units, return_sequences=True, dropout=self.dropout_rate, recurrent_dropout=0.25),\n",
    "            name=\"bidirectional_lstm_v18\"\n",
    "        )\n",
    "        \n",
    "        # --- Manual Attention Layers ---\n",
    "        self.W1 = layers.Dense(self.bilstm_output_dim, name=\"attention_dense_W1\")\n",
    "        self.W2 = layers.Dense(self.bilstm_output_dim, name=\"attention_dense_W2\")\n",
    "        self.V = layers.Dense(1, name=\"attention_dense_V\")\n",
    "        \n",
    "        # --- Layers for combined features ---\n",
    "        self.concat_layer = layers.Concatenate(name=\"concatenate_features_v18\")\n",
    "        self.dense_1 = layers.Dense(\n",
    "            self.dense_units_1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), name=\"dense_1_v18\"\n",
    "        )\n",
    "        self.dropout_1 = layers.Dropout(self.dropout_rate, name=\"dropout_1_v18\")\n",
    "        self.dense_2 = layers.Dense(\n",
    "            self.dense_units_2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), name=\"dense_2_v18\"\n",
    "        )\n",
    "        self.dropout_2 = layers.Dropout(self.dropout_rate, name=\"dropout_2_v18\")\n",
    "        self.total_time_head = layers.Dense(1, activation='linear', name=\"total_time_dense_v18\") \n",
    "    \n",
    "    def call(self, inputs, training=False): \n",
    "        sequence_input, global_features_input = inputs \n",
    "        mask_bool_seq = tf.reduce_any(tf.not_equal(sequence_input, 0.0), axis=-1)\n",
    "        \n",
    "        lstm_output = self.bi_lstm_layer(sequence_input, mask=mask_bool_seq, training=training)\n",
    "        \n",
    "        query_summary = tf.reduce_mean(lstm_output, axis=1)\n",
    "        query_with_time_axis = tf.expand_dims(query_summary, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(lstm_output) + self.W2(query_with_time_axis)))\n",
    "\n",
    "        mask_for_scores = tf.expand_dims(tf.cast(mask_bool_seq, tf.float32), -1)\n",
    "        masked_score = score - (1. - mask_for_scores) * 1e9\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "\n",
    "        context_vector = tf.reduce_sum(attention_weights * lstm_output, axis=1)\n",
    "        \n",
    "        combined_features = self.concat_layer([context_vector, global_features_input])\n",
    "        \n",
    "        x = self.dense_1(combined_features)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        total_time_pred = self.total_time_head(x)\n",
    "        \n",
    "        return total_time_pred\n",
    "\n",
    "# %%\n",
    "def process_input_data_for_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process the transformer predictions CSV to prepare data for LSTM training.\n",
    "    Global features now exclude sequence length.\n",
    "    \"\"\"\n",
    "    print(f\"Processing data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "    df = pd.read_csv(transformer_predictions_file)\n",
    "    \n",
    "    required_cols = ['Predicted_Proportion', 'GroundTruth_Cumulative', 'GroundTruth_Increment', 'Sequence', 'Step']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns: raise ValueError(f\"CSV must contain '{col}' column.\")\n",
    "\n",
    "    sequences = df['Sequence'].unique()\n",
    "    \n",
    "    X_sequential_data_list = []; X_global_features_list = []\n",
    "    y_total_times_list = []; original_dfs_list = [] \n",
    "    transformer_proportions_list = []; ground_truth_increments_list = []\n",
    "    ground_truth_cumulative_list = [] \n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_df = df[df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        if seq_df.empty:\n",
    "            original_dfs_list.append(seq_df); continue\n",
    "        original_dfs_list.append(seq_df) \n",
    "\n",
    "        actual_sequence_length = float(len(seq_df))\n",
    "        props_for_seq = seq_df['Predicted_Proportion'].values\n",
    "        \n",
    "        sum_props = np.sum(props_for_seq)\n",
    "        mean_props = np.mean(props_for_seq) if actual_sequence_length > 0 else 0.0\n",
    "        std_props = np.std(props_for_seq) if actual_sequence_length > 1 else 0.0\n",
    "        max_prop = np.max(props_for_seq) if actual_sequence_length > 0 else 0.0\n",
    "\n",
    "        current_max_steps = seq_df['Step'].max()\n",
    "        if current_max_steps == 0: current_max_steps = 1 \n",
    "        \n",
    "        sequential_features = np.column_stack([props_for_seq, seq_df['Step'].values / current_max_steps])\n",
    "        X_sequential_data_list.append(sequential_features)\n",
    "        \n",
    "        global_features_for_seq = np.array([\n",
    "            sum_props,\n",
    "            mean_props,\n",
    "            std_props,\n",
    "            max_prop\n",
    "        ], dtype=np.float32)\n",
    "        X_global_features_list.append(global_features_for_seq)\n",
    "        \n",
    "        gt_cumulative_for_seq = seq_df['GroundTruth_Cumulative'].values\n",
    "        total_time_for_seq = np.max(gt_cumulative_for_seq) if len(gt_cumulative_for_seq) > 0 else 0.0\n",
    "        y_total_times_list.append(total_time_for_seq)\n",
    "        \n",
    "        transformer_proportions_list.append(props_for_seq)\n",
    "        ground_truth_increments_list.append(seq_df['GroundTruth_Increment'].values)\n",
    "        ground_truth_cumulative_list.append(gt_cumulative_for_seq) \n",
    "\n",
    "    if not X_sequential_data_list: raise ValueError(\"No valid sequences processed.\")\n",
    "\n",
    "    y_total_times_array_unpadded = np.array(y_total_times_list)\n",
    "    print(f\"\\nStatistics for TARGET y_total_times_list (max GT_Cumulative per seq, {len(y_total_times_array_unpadded)} sequences):\")\n",
    "    print(f\"  Mean: {np.mean(y_total_times_array_unpadded):.4f}, Std Dev: {np.std(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Min: {np.min(y_total_times_array_unpadded):.4f}, Max: {np.max(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Number of zeros (<=1e-6): {np.sum(y_total_times_array_unpadded <= 1e-6)}\\n\")\n",
    "\n",
    "    max_length_sequential = max(len(x) for x in X_sequential_data_list) if X_sequential_data_list else 0\n",
    "    if max_length_sequential == 0: raise ValueError(\"Max length for sequential features is 0.\")\n",
    "    num_sequential_features = X_sequential_data_list[0].shape[1]\n",
    "    num_global_features = X_global_features_list[0].shape[0]\n",
    "\n",
    "    X_sequential_padded = np.zeros((len(X_sequential_data_list), max_length_sequential, num_sequential_features), dtype=np.float32)\n",
    "    masks_padded_float = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32) \n",
    "    transformer_proportions_padded = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "    gt_increments_padded_original = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "    gt_cumulative_padded_original = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "\n",
    "    for i, seq_data in enumerate(X_sequential_data_list):\n",
    "        seq_len = len(seq_data)\n",
    "        if seq_len > 0:\n",
    "            X_sequential_padded[i, :seq_len, :] = seq_data\n",
    "            masks_padded_float[i, :seq_len] = 1.0 \n",
    "            transformer_proportions_padded[i, :seq_len] = transformer_proportions_list[i]\n",
    "            gt_increments_padded_original[i, :seq_len] = ground_truth_increments_list[i]\n",
    "            gt_cumulative_padded_original[i, :seq_len] = ground_truth_cumulative_list[i]\n",
    "        \n",
    "    y_total_times_np = np.array(y_total_times_list, dtype=np.float32)\n",
    "    X_global_features_np = np.array(X_global_features_list, dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        'X_sequential_input': X_sequential_padded, \n",
    "        'X_global_features_input': X_global_features_np, \n",
    "        'y_lstm_target_total_times': y_total_times_np,\n",
    "        'masks_for_calc': masks_padded_float, \n",
    "        'sequences_ids': sequences, \n",
    "        'original_dfs': original_dfs_list, \n",
    "        'transformer_proportions_padded': transformer_proportions_padded, \n",
    "        'gt_increments_padded_original': gt_increments_padded_original,\n",
    "        'gt_cumulative_padded_original': gt_cumulative_padded_original,\n",
    "        'max_len_sequential': max_length_sequential,\n",
    "        'num_sequential_features': num_sequential_features,\n",
    "        'num_global_features': num_global_features\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def train_total_time_lstm(transformer_predictions_file, epochs=50, batch_size=32, val_split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Train the enhanced LSTM model to predict total_time.\n",
    "    \"\"\"\n",
    "    print(\"Processing data for LSTM training...\")\n",
    "    data_for_lstm = process_input_data_for_lstm(transformer_predictions_file)\n",
    "    \n",
    "    print(f\"Num sequential features: {data_for_lstm['num_sequential_features']}, Max seq length: {data_for_lstm['max_len_sequential']}\")\n",
    "    print(f\"Num global features: {data_for_lstm['num_global_features']}\")\n",
    "\n",
    "    lstm_model = TotalTimeLSTM(hidden_units=256, dense_units_1=128, dense_units_2=64, dropout_rate=0.4) \n",
    "    \n",
    "    X_sequential_all = data_for_lstm['X_sequential_input']\n",
    "    X_global_all = data_for_lstm['X_global_features_input']\n",
    "    y_targets_all = data_for_lstm['y_lstm_target_total_times']\n",
    "    \n",
    "    if len(X_sequential_all) > 0:\n",
    "        sample_seq_input_for_build = tf.convert_to_tensor(X_sequential_all[:1], dtype=tf.float32)\n",
    "        sample_glob_input_for_build = tf.convert_to_tensor(X_global_all[:1], dtype=tf.float32)\n",
    "        _ = lstm_model((sample_seq_input_for_build, sample_glob_input_for_build)) \n",
    "        print(\"\\nEnhanced LSTM Model Summary (v18 - after sample call):\")\n",
    "        lstm_model.summary(expand_nested=True) \n",
    "    else: print(\"Warning: No data to build model with sample call.\")\n",
    "\n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse' ) \n",
    "    \n",
    "    if np.any(np.isnan(X_sequential_all)) or np.any(np.isinf(X_sequential_all)): print(\"CRITICAL WARNING: NaN/Inf in X_sequential_all.\")\n",
    "    if np.any(np.isnan(X_global_all)) or np.any(np.isinf(X_global_all)): print(\"CRITICAL WARNING: NaN/Inf in X_global_all.\")\n",
    "    if np.any(np.isnan(y_targets_all)) or np.any(np.isinf(y_targets_all)): print(\"CRITICAL WARNING: NaN/Inf in y_targets_all.\")\n",
    "    if len(y_targets_all) > 0 and np.all(np.abs(y_targets_all) <= 1e-6) : print(\"CRITICAL WARNING: All target total times are near zero.\")\n",
    "\n",
    "    target_scaler = StandardScaler()\n",
    "    y_targets_all_reshaped = y_targets_all.reshape(-1, 1)\n",
    "    global_feature_scaler = StandardScaler()\n",
    "    indices = np.arange(len(X_sequential_all))\n",
    "\n",
    "    if len(X_sequential_all) < 10: \n",
    "        print(\"Warning: Very few samples (<10), using all for training.\")\n",
    "        X_train_seq = X_sequential_all\n",
    "        X_train_glob_scaled = global_feature_scaler.fit_transform(X_global_all)\n",
    "        y_train_scaled = target_scaler.fit_transform(y_targets_all_reshaped)\n",
    "        validation_data_for_fit = None\n",
    "    else:\n",
    "        train_indices, val_indices = train_test_split(indices, test_size=val_split_ratio, random_state=42, shuffle=True)\n",
    "        X_train_seq = X_sequential_all[train_indices]; X_val_seq = X_sequential_all[val_indices]\n",
    "        X_train_glob = X_global_all[train_indices]; X_val_glob = X_global_all[val_indices]\n",
    "        X_train_glob_scaled = global_feature_scaler.fit_transform(X_train_glob) \n",
    "        X_val_glob_scaled = global_feature_scaler.transform(X_val_glob)     \n",
    "        y_train_orig_reshaped = y_targets_all_reshaped[train_indices]; y_val_orig_reshaped = y_targets_all_reshaped[val_indices]\n",
    "        y_train_scaled = target_scaler.fit_transform(y_train_orig_reshaped) \n",
    "        y_val_scaled = target_scaler.transform(y_val_orig_reshaped)         \n",
    "        validation_data_for_fit = ([X_val_seq, X_val_glob_scaled], y_val_scaled) \n",
    "        print(f\"\\nManually split data: {len(X_train_seq)} train, {len(X_val_seq)} validation samples.\")\n",
    "        print(f\"Training target stats (orig scale): Mean={np.mean(y_train_orig_reshaped):.2f}, Std={np.std(y_train_orig_reshaped):.2f}\")\n",
    "        print(f\"Validation target stats (orig scale): Mean={np.mean(y_val_orig_reshaped):.2f}, Std={np.std(y_val_orig_reshaped):.2f}\\n\")\n",
    "\n",
    "    callbacks_list = [\n",
    "        EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1), \n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=1e-7, verbose=1) \n",
    "    ]\n",
    "    \n",
    "    print(\"Starting LSTM model training (with scaled targets and global features)...\")\n",
    "    history = lstm_model.fit(\n",
    "        [X_train_seq, X_train_glob_scaled], y_train_scaled,          \n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        validation_data=validation_data_for_fit, \n",
    "        callbacks=callbacks_list, verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM training finished.\")\n",
    "    data_for_lstm['target_scaler'] = target_scaler\n",
    "    data_for_lstm['global_feature_scaler'] = global_feature_scaler \n",
    "    return lstm_model, history, data_for_lstm\n",
    "\n",
    "# %%\n",
    "def generate_refined_predictions_with_lstm(lstm_model, processed_data):\n",
    "    \"\"\"\n",
    "    Generate refined time predictions, including additional specified columns from the original input.\n",
    "    Output CSV changed to _v18.\n",
    "    \"\"\"\n",
    "    print(\"Generating refined predictions using LSTM's total time...\")\n",
    "    \n",
    "    # Define the columns from the original data to keep in the final output\n",
    "    COLUMNS_TO_KEEP = ['timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to', 'PatientID_from', 'PatientID_to']\n",
    "\n",
    "    X_sequential_input_all = processed_data['X_sequential_input']\n",
    "    X_global_features_input_all_unscaled = processed_data['X_global_features_input']\n",
    "    \n",
    "    global_feature_scaler = processed_data['global_feature_scaler']\n",
    "    X_global_features_input_all_scaled = global_feature_scaler.transform(X_global_features_input_all_unscaled)\n",
    "\n",
    "    lstm_predicted_scaled_total_times = lstm_model.predict(\n",
    "        [X_sequential_input_all, X_global_features_input_all_scaled] \n",
    "    ) \n",
    "    \n",
    "    target_scaler = processed_data['target_scaler']\n",
    "    lstm_predicted_total_times_original_scale = target_scaler.inverse_transform(lstm_predicted_scaled_total_times)\n",
    "    lstm_predicted_total_times_original_scale = np.squeeze(lstm_predicted_total_times_original_scale)\n",
    "    lstm_predicted_total_times_original_scale = np.maximum(0, lstm_predicted_total_times_original_scale) \n",
    "\n",
    "    transformer_step_proportions = processed_data['transformer_proportions_padded'] \n",
    "    masks_for_calc = processed_data['masks_for_calc'] \n",
    "\n",
    "    _, lstm_refined_increments, lstm_refined_cumulative = calculate_times_from_proportions(\n",
    "        transformer_step_proportions,\n",
    "        lstm_predicted_total_times_original_scale, \n",
    "        masks_for_calc \n",
    "    )\n",
    "\n",
    "    lstm_refined_increments_np = lstm_refined_increments.numpy()\n",
    "    lstm_refined_cumulative_np = lstm_refined_cumulative.numpy()\n",
    "\n",
    "    results_list_df = []\n",
    "    original_dfs_from_processing = processed_data['original_dfs'] \n",
    "    gt_total_times_all = processed_data['y_lstm_target_total_times'] \n",
    "\n",
    "    for i, seq_id in enumerate(processed_data['sequences_ids']):\n",
    "        if i >= len(original_dfs_from_processing): continue\n",
    "        original_seq_df = original_dfs_from_processing[i]\n",
    "        seq_len = len(original_seq_df)\n",
    "        if seq_len == 0:\n",
    "            if original_seq_df.empty: results_list_df.append(original_seq_df) \n",
    "            continue\n",
    "        if i >= len(lstm_predicted_total_times_original_scale): continue\n",
    "\n",
    "        # --- Build the new, clean DataFrame ---\n",
    "        output_dict = {\n",
    "            'Sequence': seq_id, \n",
    "            'Step': np.arange(1, seq_len + 1),\n",
    "            'SourceID': original_seq_df['SourceID'].values, # Added SourceID back\n",
    "            'GroundTruth_Increment': processed_data['gt_increments_padded_original'][i, :seq_len],\n",
    "            'GroundTruth_Cumulative': processed_data['gt_cumulative_padded_original'][i, :seq_len],\n",
    "            'LSTM_Predicted_Increment': lstm_refined_increments_np[i, :seq_len],\n",
    "            'LSTM_Predicted_Cumulative': lstm_refined_cumulative_np[i, :seq_len],\n",
    "            'LSTM_Predicted_TotalTime': np.full(seq_len, np.nan, dtype=np.float32),\n",
    "            'TotalTime_Difference': np.full(seq_len, np.nan, dtype=np.float32) \n",
    "        }\n",
    "        \n",
    "        # Add the additional columns from the original dataframe\n",
    "        for col in COLUMNS_TO_KEEP:\n",
    "            if col in original_seq_df.columns:\n",
    "                output_dict[col] = original_seq_df[col].values\n",
    "        \n",
    "        predicted_total = lstm_predicted_total_times_original_scale[i]\n",
    "        gt_total = gt_total_times_all[i]\n",
    "        \n",
    "        output_dict['LSTM_Predicted_TotalTime'][-1] = predicted_total\n",
    "        output_dict['TotalTime_Difference'][-1] = gt_total - predicted_total \n",
    "\n",
    "        transformer_pred_increment = original_seq_df['Predicted_Increment'].fillna(0).values\n",
    "        transformer_pred_cumulative = original_seq_df['Predicted_Cumulative'].fillna(0).values\n",
    "        diff_transformer_inc = np.abs(output_dict['GroundTruth_Increment'] - transformer_pred_increment)\n",
    "        diff_lstm_inc = np.abs(output_dict['GroundTruth_Increment'] - output_dict['LSTM_Predicted_Increment'])\n",
    "        output_dict['Increment_Improvement_Pct'] = np.where(diff_transformer_inc > 1e-6, (diff_transformer_inc - diff_lstm_inc) / diff_transformer_inc * 100, 0 )\n",
    "        diff_transformer_cum = np.abs(output_dict['GroundTruth_Cumulative'] - transformer_pred_cumulative)\n",
    "        diff_lstm_cum = np.abs(output_dict['GroundTruth_Cumulative'] - output_dict['LSTM_Predicted_Cumulative'])\n",
    "        output_dict['Cumulative_Improvement_Pct'] = np.where(diff_transformer_cum > 1e-6, (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100, 0 )\n",
    "        \n",
    "        clean_seq_df = pd.DataFrame(output_dict)\n",
    "        results_list_df.append(clean_seq_df)\n",
    "\n",
    "    if not results_list_df: return pd.DataFrame()\n",
    "    final_results_df = pd.concat(results_list_df, ignore_index=True)\n",
    "    \n",
    "    output_filename = 'predictions_lstm_refined_total_time_v18.csv' # Changed filename\n",
    "    final_results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Combined and refined predictions saved to {output_filename}\")\n",
    "    \n",
    "    if not final_results_df.empty:\n",
    "        original_df_full = pd.concat(original_dfs_from_processing, ignore_index=True)\n",
    "        merged_for_summary = pd.merge(final_results_df, original_df_full[['Sequence', 'Step', 'Predicted_Increment', 'Predicted_Cumulative']], on=['Sequence', 'Step'])\n",
    "        if 'Predicted_Increment' in merged_for_summary.columns and 'LSTM_Predicted_Increment' in merged_for_summary.columns:\n",
    "            mae_transformer = np.mean(np.abs(merged_for_summary['GroundTruth_Increment'] - merged_for_summary['Predicted_Increment']))\n",
    "            mae_lstm = np.mean(np.abs(merged_for_summary['GroundTruth_Increment'] - merged_for_summary['LSTM_Predicted_Increment']))\n",
    "            print(f\"\\nTransformer MAE (Increments): {mae_transformer:.4f}, LSTM-Refined MAE (Increments): {mae_lstm:.4f}\")\n",
    "            if mae_transformer > 1e-6: print(f\"Improvement (Increments): {(mae_transformer - mae_lstm) / mae_transformer * 100:.2f}%\")\n",
    "        if 'Predicted_Cumulative' in merged_for_summary.columns and 'LSTM_Predicted_Cumulative' in merged_for_summary.columns:\n",
    "            mae_transformer_cum = np.mean(np.abs(merged_for_summary['GroundTruth_Cumulative'] - merged_for_summary['Predicted_Cumulative']))\n",
    "            mae_lstm_cum = np.mean(np.abs(merged_for_summary['GroundTruth_Cumulative'] - merged_for_summary['LSTM_Predicted_Cumulative']))\n",
    "            print(f\"Transformer MAE (Cumulative): {mae_transformer_cum:.4f}, LSTM-Refined MAE (Cumulative): {mae_lstm_cum:.4f}\")\n",
    "            if mae_transformer_cum > 1e-6: print(f\"Improvement (Cumulative): {(mae_transformer_cum - mae_lstm_cum) / mae_transformer_cum * 100:.2f}%\")\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data['y_lstm_target_total_times'] \n",
    "    if len(lstm_predicted_total_times_original_scale) == len(gt_total_times_for_lstm_training):\n",
    "        mae_total_time_lstm = np.mean(np.abs(gt_total_times_for_lstm_training - lstm_predicted_total_times_original_scale))\n",
    "        print(f\"\\nLSTM MAE for Total Time (vs Max GT Cumulative): {mae_total_time_lstm:.4f}\")\n",
    "\n",
    "    return final_results_df\n",
    "\n",
    "# %%\n",
    "def visualize_lstm_results(results_df, processed_data, lstm_model, training_history):\n",
    "    if results_df.empty: print(\"Results DataFrame is empty, skipping visualizations.\"); return\n",
    "    print(\"Generating visualizations for LSTM results...\")\n",
    "    plt.style.use('ggplot')\n",
    "    if training_history and hasattr(training_history, 'history'):\n",
    "        if 'loss' in training_history.history and 'val_loss' in training_history.history:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "            plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "            if 'lr' in training_history.history:\n",
    "                ax2 = plt.gca().twinx(); ax2.plot(training_history.history['lr'], label='Learning Rate', color='g', linestyle='--')\n",
    "                ax2.set_ylabel('Learning Rate'); ax2.legend(loc='upper center')\n",
    "            plt.title('LSTM Model Loss (Predicting Scaled Total Time)') \n",
    "            plt.xlabel('Epoch'); plt.ylabel('Mean Squared Error (Scaled Loss)'); plt.legend(loc='upper left'); plt.tight_layout()\n",
    "            plt.savefig('lstm_total_time_training_loss_v18.png'); print(\"Saved LSTM training loss plot.\"); plt.close() # v18\n",
    "    else: print(\"Warning: Training history not available or malformed.\")\n",
    "\n",
    "    sample_sequence_ids = results_df['Sequence'].unique()\n",
    "    if len(sample_sequence_ids) == 0 : print(\"No sequences in results_df for plotting.\"); return\n",
    "    sample_sequence_ids = sample_sequence_ids[:min(5, len(sample_sequence_ids))]\n",
    "    \n",
    "    original_df_full = pd.concat(processed_data['original_dfs'], ignore_index=True)\n",
    "    plot_df = pd.merge(results_df, original_df_full[['Sequence', 'Step', 'Predicted_Cumulative', 'Predicted_Increment']], on=['Sequence', 'Step'], how='left')\n",
    "    \n",
    "    if len(sample_sequence_ids) > 0:\n",
    "        num_plots = len(sample_sequence_ids); fig_height = max(8, 3 * num_plots) \n",
    "        plt.figure(figsize=(15, fig_height)) # Cumulative Plot\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = plot_df[plot_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Cumulative'], 'o-', label='GT Cumul.', ms=4)\n",
    "            if 'Predicted_Cumulative' in plot_df.columns and 'Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Cumulative'], 's--', label='Transformer Cumul.', ms=4)\n",
    "            if 'LSTM_Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Cumulative'], '^-.', label='LSTM-Refined Cumul.', ms=4)\n",
    "            lstm_total_time_for_seq = seq_data_plot['LSTM_Predicted_TotalTime'].dropna().unique()\n",
    "            if len(lstm_total_time_for_seq) == 1: plt.axhline(y=lstm_total_time_for_seq[0], color='purple', linestyle=':', label=f'LSTM Total Pred: {lstm_total_time_for_seq[0]:.2f}')\n",
    "            plt.title(f'Cumulative Times: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Cumulative Time'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_cumulative_time_comparison_v18.png'); print(\"Saved cumulative time comparison plot.\"); plt.close() # v18\n",
    "\n",
    "        plt.figure(figsize=(15, fig_height)) # Increment Plot\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = plot_df[plot_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Increment'], 'o-', label='GT Incr.', ms=4)\n",
    "            if 'Predicted_Increment' in plot_df.columns and 'Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Increment'], 's--', label='Transformer Incr.', ms=4)\n",
    "            if 'LSTM_Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Increment'], '^-.', label='LSTM-Refined Incr.', ms=4)\n",
    "            plt.title(f'Time Increments: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Time Increment'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_increment_comparison_v18.png'); print(\"Saved increment comparison plot.\"); plt.close() # v18\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data.get('y_lstm_target_total_times', np.array([])) \n",
    "    if lstm_model is not None and 'X_sequential_input' in processed_data and 'X_global_features_input' in processed_data and 'target_scaler' in processed_data:\n",
    "        X_seq_tensor = tf.convert_to_tensor(processed_data['X_sequential_input'], dtype=tf.float32)\n",
    "        X_glob_unscaled = processed_data['X_global_features_input']\n",
    "        X_glob_scaled_for_plot = processed_data['global_feature_scaler'].transform(X_glob_unscaled)\n",
    "        X_glob_tensor = tf.convert_to_tensor(X_glob_scaled_for_plot, dtype=tf.float32)\n",
    "        \n",
    "        lstm_pred_scaled_total_t = lstm_model.predict([X_seq_tensor, X_glob_tensor])\n",
    "        lstm_pred_original_scale_total_t = processed_data['target_scaler'].inverse_transform(lstm_pred_scaled_total_t).squeeze()\n",
    "        \n",
    "        if lstm_pred_original_scale_total_t.ndim == 0: lstm_pred_original_scale_total_t = np.array([lstm_pred_original_scale_total_t])\n",
    "            \n",
    "        if gt_total_times_for_lstm_training.size > 0 and lstm_pred_original_scale_total_t.size > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1); \n",
    "            plt.hist(gt_total_times_for_lstm_training, bins=30, alpha=0.7, label='GT Total Times (Max Cumul.)')\n",
    "            plt.hist(lstm_pred_original_scale_total_t, bins=30, alpha=0.7, label='LSTM Pred Total Times (Original Scale)')\n",
    "            plt.xlabel('Total Time'); plt.ylabel('Frequency'); plt.title('Distribution of Total Times'); plt.legend()\n",
    "            plt.subplot(1, 2, 2); \n",
    "            if len(gt_total_times_for_lstm_training) == len(lstm_pred_original_scale_total_t):\n",
    "                errors_total_time = gt_total_times_for_lstm_training - lstm_pred_original_scale_total_t\n",
    "                plt.hist(errors_total_time, bins=30, alpha=0.7, color='red')\n",
    "                plt.xlabel('Prediction Error (GT Max Cumul. - Pred)'); plt.ylabel('Frequency'); plt.title('LSTM Total Time Prediction Errors')\n",
    "                if errors_total_time.size > 0: \n",
    "                    mean_error_val = errors_total_time.mean(); plt.axvline(mean_error_val, color='k', ls='--', lw=1, label=f'Mean Error: {mean_error_val:.2f}')\n",
    "                plt.legend()\n",
    "            else: print(\"Warning: Mismatch length GT total times and predictions for error histogram.\")\n",
    "            plt.tight_layout(); plt.savefig('lstm_total_time_prediction_analysis_v18.png'); print(\"Saved total time prediction analysis plot.\"); plt.close() # v18\n",
    "        else: print(\"Warning: Not enough data for total time distribution plots.\")\n",
    "    else: print(\"Warning: LSTM model, input data, or scaler missing for total time prediction plot.\")\n",
    "    print(\"Visualizations for LSTM completed!\")\n",
    "\n",
    "# %%\n",
    "def main_lstm_total_time_flow():\n",
    "    try:\n",
    "        transformer_predictions_file = \"predictions_transformer_182625_with_details.csv\" \n",
    "        if not os.path.exists(transformer_predictions_file):\n",
    "            print(f\"Error: Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "            print(\"Creating DUMMY CSV for testing flow.\")\n",
    "            dummy_data = []\n",
    "            for seq_idx in range(300): \n",
    "                num_steps = np.random.randint(10, 60) \n",
    "                steps = np.arange(1, num_steps + 1)\n",
    "                gt_increments = np.random.lognormal(mean=2.0, sigma=0.7, size=num_steps) + 0.1 \n",
    "                gt_increments = np.maximum(gt_increments, 0.01) \n",
    "                gt_cumulative = np.cumsum(gt_increments)\n",
    "                \n",
    "                raw_props = np.random.rand(num_steps) + 0.05 \n",
    "                pred_proportions = raw_props / raw_props.sum() \n",
    "                \n",
    "                actual_sequence_total_time = gt_cumulative[-1] if num_steps > 0 else 1.0\n",
    "                actual_sequence_total_time = max(actual_sequence_total_time, 1.0) \n",
    "\n",
    "                dummy_transformer_effective_total_time = actual_sequence_total_time * np.random.normal(loc=1.0, scale=0.4) \n",
    "                dummy_transformer_effective_total_time = max(dummy_transformer_effective_total_time, 0.1)\n",
    "\n",
    "                pred_increments_from_transformer = pred_proportions * dummy_transformer_effective_total_time\n",
    "                pred_cumulative_from_transformer = np.cumsum(pred_increments_from_transformer)\n",
    "\n",
    "                for s_idx in range(num_steps):\n",
    "                    dummy_data.append({\n",
    "                        'Sequence': seq_idx, 'Step': steps[s_idx], 'SourceID': f'MRI_DUMMY_{s_idx%5 +1}',\n",
    "                        'Predicted_Proportion': pred_proportions[s_idx], \n",
    "                        'Predicted_Increment': pred_increments_from_transformer[s_idx],\n",
    "                        'Predicted_Cumulative': pred_cumulative_from_transformer[s_idx], \n",
    "                        'GroundTruth_Increment': gt_increments[s_idx], \n",
    "                        'GroundTruth_Cumulative': gt_cumulative[s_idx],\n",
    "                        # Add dummy data for the new columns\n",
    "                        'timediff': np.random.randint(100, 1000),\n",
    "                        'PTAB': f'PTAB_{seq_idx}',\n",
    "                        'BodyGroup_from': 'CHEST',\n",
    "                        'BodyGroup_to': 'ABDOMEN',\n",
    "                        'PatientID_from': f'PAT_{seq_idx}',\n",
    "                        'PatientID_to': f'PAT_{seq_idx}'\n",
    "                    })\n",
    "            if not dummy_data: \n",
    "                 dummy_data.append({ 'Sequence': 0, 'Step': 1, 'SourceID': 'MRI_DUMMY_0', 'Predicted_Proportion': 1.0, \n",
    "                                     'Predicted_Increment': 10.0, 'Predicted_Cumulative': 10.0, \n",
    "                                     'GroundTruth_Increment': 10.0, 'GroundTruth_Cumulative': 10.0,\n",
    "                                     'timediff': 500, 'PTAB': 'PTAB_0', 'BodyGroup_from': 'CHEST', 'BodyGroup_to': 'ABDOMEN',\n",
    "                                     'PatientID_from': 'PAT_0', 'PatientID_to': 'PAT_0'})\n",
    "            dummy_df = pd.DataFrame(dummy_data)\n",
    "            dummy_df.to_csv(transformer_predictions_file, index=False)\n",
    "            print(f\"Dummy '{transformer_predictions_file}' created with {len(dummy_df)} rows and {len(dummy_df['Sequence'].unique())} sequences.\")\n",
    "        \n",
    "        lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
    "            transformer_predictions_file, epochs=200, batch_size=32 ) \n",
    "        \n",
    "        if lstm_model is None or processed_lstm_data is None:\n",
    "            print(\"LSTM training failed or returned None. Exiting.\"); return\n",
    "\n",
    "        refined_results_df = generate_refined_predictions_with_lstm(lstm_model, processed_lstm_data)\n",
    "        if not refined_results_df.empty:\n",
    "            print(\"\\nSample of Refined Predictions (LSTM Total Time Approach - v18):\")\n",
    "            # Display the new columns as well\n",
    "            display_cols = [ 'Sequence', 'Step', 'SourceID', 'timediff', 'PTAB', 'BodyGroup_from', 'PatientID_from',\n",
    "                             'LSTM_Predicted_Increment', 'GroundTruth_Increment', \n",
    "                             'LSTM_Predicted_Cumulative', 'GroundTruth_Cumulative',\n",
    "                             'LSTM_Predicted_TotalTime', 'TotalTime_Difference']\n",
    "            actual_display_cols = [col for col in display_cols if col in refined_results_df.columns]\n",
    "            print(refined_results_df[actual_display_cols].head(20))\n",
    "            print(\"\\nGenerating visualizations for LSTM (total time approach - v18)...\")\n",
    "            visualize_lstm_results(refined_results_df, processed_lstm_data, lstm_model, lstm_history)\n",
    "        else: print(\"No refined predictions were generated by the LSTM flow.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM (total time) main function: {e}\"); import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for LSTM training...\n",
      "Processing data from: predictions_transformer_182625_with_details.csv\n",
      "\n",
      "Statistics for TARGET y_total_times_list (max GT_Cumulative per seq, 186 sequences):\n",
      "  Mean: 374.8065, Std Dev: 348.4868\n",
      "  Min: 0.0000, Max: 2900.0000\n",
      "  Number of zeros (<=1e-6): 5\n",
      "\n",
      "Num sequential features: 2, Max seq length: 42\n",
      "Num global features: 4\n",
      "\n",
      "Enhanced LSTM Model Summary (v18 - after sample call):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"total_time_lstm_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"total_time_lstm_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_lstm_v18          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">530,432</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_V (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate_features_v18        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                   │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_lstm_v18          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m530,432\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W1 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W2 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_V (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │           \u001b[38;5;34m513\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate_features_v18        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m516\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)                   │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1_v18 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │        \u001b[38;5;34m66,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1_v18 (\u001b[38;5;33mDropout\u001b[0m)         │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2_v18 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2_v18 (\u001b[38;5;33mDropout\u001b[0m)         │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_v18 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,130,754</span> (4.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,130,754\u001b[0m (4.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,130,754</span> (4.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,130,754\u001b[0m (4.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manually split data: 148 train, 38 validation samples.\n",
      "Training target stats (orig scale): Mean=390.04, Std=362.56\n",
      "Validation target stats (orig scale): Mean=315.47, Std=279.35\n",
      "\n",
      "Starting LSTM model training (with scaled targets and global features)...\n",
      "Epoch 1/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - loss: 0.9226 - val_loss: 0.8173 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 1.5399 - val_loss: 0.8068 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.9563 - val_loss: 0.7967 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 1.3969 - val_loss: 0.8048 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.1061 - val_loss: 0.7797 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.6285 - val_loss: 0.7727 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.2713 - val_loss: 0.7624 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.1867 - val_loss: 0.7458 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.2911 - val_loss: 0.7339 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.4845 - val_loss: 0.7371 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.1627 - val_loss: 0.7280 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.1241 - val_loss: 0.7240 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.1044 - val_loss: 0.7196 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1.0005 - val_loss: 0.7140 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.4246 - val_loss: 0.7274 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.9809 - val_loss: 0.7254 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.1174 - val_loss: 0.7148 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.9257 - val_loss: 0.7080 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.8684 - val_loss: 0.7079 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.8229 - val_loss: 0.7063 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.9815 - val_loss: 0.7083 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 1.0170 - val_loss: 0.6958 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - loss: 1.0397 - val_loss: 0.6808 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - loss: 1.2085 - val_loss: 0.6674 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.1790 - val_loss: 0.6792 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9187 - val_loss: 0.6931 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.2111 - val_loss: 0.6896 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - loss: 0.9466 - val_loss: 0.6780 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8262 - val_loss: 0.6776 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.8418 - val_loss: 0.7009 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9749 - val_loss: 0.7123 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.8645 - val_loss: 0.6778 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 1.0057 - val_loss: 0.6558 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.1458 - val_loss: 0.6716 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7731 - val_loss: 0.6776 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.0888 - val_loss: 0.6820 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8142 - val_loss: 0.6640 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.8923 - val_loss: 0.6559 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.7336 - val_loss: 0.6463 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 1.0576 - val_loss: 0.6473 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - loss: 1.1643 - val_loss: 0.6623 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1.1571 - val_loss: 0.6537 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8224 - val_loss: 0.6443 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.2906 - val_loss: 0.6437 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - loss: 0.7282 - val_loss: 0.6411 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1.0792 - val_loss: 0.6553 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - loss: 1.2525 - val_loss: 0.6623 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - loss: 0.8048 - val_loss: 0.6487 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - loss: 0.8571 - val_loss: 0.6501 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - loss: 0.7991 - val_loss: 0.6509 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1.0007 - val_loss: 0.6549 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 1.1475 - val_loss: 0.6547 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1281 - val_loss: 0.6521 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8794 - val_loss: 0.6478 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 1.1196 - val_loss: 0.6594 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.9082 - val_loss: 0.6455 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - loss: 0.8915 - val_loss: 0.6448 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.4366 - val_loss: 0.6563 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.8223 - val_loss: 0.6487 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.5937\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.3927 - val_loss: 0.6459 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8426 - val_loss: 0.6454 - learning_rate: 2.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.7217 - val_loss: 0.6436 - learning_rate: 2.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - loss: 1.2799 - val_loss: 0.6445 - learning_rate: 2.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.0072 - val_loss: 0.6471 - learning_rate: 2.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.1371 - val_loss: 0.6496 - learning_rate: 2.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.6669 - val_loss: 0.6489 - learning_rate: 2.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.0743 - val_loss: 0.6473 - learning_rate: 2.0000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8895 - val_loss: 0.6461 - learning_rate: 2.0000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - loss: 0.7908 - val_loss: 0.6440 - learning_rate: 2.0000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - loss: 0.9301 - val_loss: 0.6432 - learning_rate: 2.0000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - loss: 0.9662 - val_loss: 0.6412 - learning_rate: 2.0000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - loss: 0.7312 - val_loss: 0.6389 - learning_rate: 2.0000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 308ms/step - loss: 0.7539 - val_loss: 0.6381 - learning_rate: 2.0000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - loss: 1.0647 - val_loss: 0.6371 - learning_rate: 2.0000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 0.9345 - val_loss: 0.6358 - learning_rate: 2.0000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - loss: 1.0047 - val_loss: 0.6350 - learning_rate: 2.0000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - loss: 0.8661 - val_loss: 0.6345 - learning_rate: 2.0000e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - loss: 0.9508 - val_loss: 0.6367 - learning_rate: 2.0000e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.9926 - val_loss: 0.6366 - learning_rate: 2.0000e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.8237 - val_loss: 0.6352 - learning_rate: 2.0000e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.7320 - val_loss: 0.6388 - learning_rate: 2.0000e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.3588 - val_loss: 0.6477 - learning_rate: 2.0000e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.6445 - val_loss: 0.6515 - learning_rate: 2.0000e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.8315 - val_loss: 0.6552 - learning_rate: 2.0000e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1.0317 - val_loss: 0.6594 - learning_rate: 2.0000e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.7683 - val_loss: 0.6622 - learning_rate: 2.0000e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.9322 - val_loss: 0.6622 - learning_rate: 2.0000e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.9429 - val_loss: 0.6594 - learning_rate: 2.0000e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.0714 - val_loss: 0.6567 - learning_rate: 2.0000e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.6954 - val_loss: 0.6516 - learning_rate: 2.0000e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.1565 - val_loss: 0.6526 - learning_rate: 2.0000e-04\n",
      "Epoch 92/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.8960\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.8966 - val_loss: 0.6538 - learning_rate: 2.0000e-04\n",
      "Epoch 93/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.2045 - val_loss: 0.6540 - learning_rate: 4.0000e-05\n",
      "Epoch 94/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.0419 - val_loss: 0.6540 - learning_rate: 4.0000e-05\n",
      "Epoch 95/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.8807 - val_loss: 0.6542 - learning_rate: 4.0000e-05\n",
      "Epoch 96/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.7632 - val_loss: 0.6542 - learning_rate: 4.0000e-05\n",
      "Epoch 97/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.9440 - val_loss: 0.6540 - learning_rate: 4.0000e-05\n",
      "Epoch 98/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.9108 - val_loss: 0.6540 - learning_rate: 4.0000e-05\n",
      "Epoch 99/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.9032 - val_loss: 0.6540 - learning_rate: 4.0000e-05\n",
      "Epoch 100/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9287 - val_loss: 0.6540 - learning_rate: 4.0000e-05\n",
      "Epoch 101/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 1.0419 - val_loss: 0.6542 - learning_rate: 4.0000e-05\n",
      "Epoch 102/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7808 - val_loss: 0.6541 - learning_rate: 4.0000e-05\n",
      "Epoch 103/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.1359 - val_loss: 0.6542 - learning_rate: 4.0000e-05\n",
      "Epoch 104/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.8082 - val_loss: 0.6539 - learning_rate: 4.0000e-05\n",
      "Epoch 105/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.8314 - val_loss: 0.6536 - learning_rate: 4.0000e-05\n",
      "Epoch 106/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.7238 - val_loss: 0.6531 - learning_rate: 4.0000e-05\n",
      "Epoch 107/200\n",
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.2501\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 1.1421 - val_loss: 0.6527 - learning_rate: 4.0000e-05\n",
      "Epoch 108/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.2594 - val_loss: 0.6527 - learning_rate: 8.0000e-06\n",
      "Epoch 109/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.1897 - val_loss: 0.6526 - learning_rate: 8.0000e-06\n",
      "Epoch 110/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.6572 - val_loss: 0.6526 - learning_rate: 8.0000e-06\n",
      "Epoch 111/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 1.1791 - val_loss: 0.6526 - learning_rate: 8.0000e-06\n",
      "Epoch 112/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.0644 - val_loss: 0.6525 - learning_rate: 8.0000e-06\n",
      "Epoch 113/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.7521 - val_loss: 0.6525 - learning_rate: 8.0000e-06\n",
      "Epoch 114/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.7394 - val_loss: 0.6525 - learning_rate: 8.0000e-06\n",
      "Epoch 115/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 1.0264 - val_loss: 0.6526 - learning_rate: 8.0000e-06\n",
      "Epoch 116/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 1.0370 - val_loss: 0.6527 - learning_rate: 8.0000e-06\n",
      "Epoch 117/200\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.9756 - val_loss: 0.6526 - learning_rate: 8.0000e-06\n",
      "Epoch 117: early stopping\n",
      "Restoring model weights from the end of the best epoch: 77.\n",
      "LSTM training finished.\n",
      "Generating refined predictions using LSTM's total time...\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step\n",
      "Combined and refined predictions saved to predictions_lstm_refined_total_time_v18.csv\n",
      "\n",
      "Transformer MAE (Increments): 45.2859, LSTM-Refined MAE (Increments): 45.3131\n",
      "Improvement (Increments): -0.06%\n",
      "Transformer MAE (Cumulative): 66.3642, LSTM-Refined MAE (Cumulative): 145.3391\n",
      "Improvement (Cumulative): -119.00%\n",
      "\n",
      "LSTM MAE for Total Time (vs Max GT Cumulative): 188.4787\n",
      "\n",
      "Sample of Refined Predictions (LSTM Total Time Approach - v18):\n",
      "    Sequence  Step      SourceID  timediff     PTAB BodyGroup_from  \\\n",
      "0          0     1   MRI_MSR_104       0.0 -1182200       SHOULDER   \n",
      "1          0     2     MRI_FRR_2      40.0 -1182200       SHOULDER   \n",
      "2          0     3   MRI_FRR_257      45.0 -1181050       SHOULDER   \n",
      "3          0     4   MRI_FRR_264      52.0 -1181050       SHOULDER   \n",
      "4          0     5   MRI_FRR_264      68.0 -1181050       SHOULDER   \n",
      "5          0     6    MRI_CCS_11      77.0 -1181050       SHOULDER   \n",
      "6          0     7    MRI_CCS_11      83.0 -1181050       SHOULDER   \n",
      "7          0     8   MRI_FRR_257     213.0      700       SHOULDER   \n",
      "8          0     9   MRI_FRR_264     214.0      700       SHOULDER   \n",
      "9          0    10   MRI_FRR_264     224.0      700       SHOULDER   \n",
      "10         0    11   MRI_FRR_257     226.0      150       SHOULDER   \n",
      "11         0    12   MRI_FRR_264     233.0      150       SHOULDER   \n",
      "12         0    13     MRI_FRR_3     235.0      150       SHOULDER   \n",
      "13         0    14  MRI_MPT_1005     281.0      150       SHOULDER   \n",
      "14         0    15   MRI_FRR_257     292.0 -1182150       SHOULDER   \n",
      "15         0    16   MRI_FRR_264     293.0 -1182150       SHOULDER   \n",
      "16         0    17    MRI_EXU_95     307.0 -1182150       SHOULDER   \n",
      "17         0    18   MRI_MSR_100     307.0 -1182150       SHOULDER   \n",
      "18         1     1   MRI_MSR_104       0.0 -1182150       SHOULDER   \n",
      "19         1     2     MRI_FRR_2      26.0 -1182150       SHOULDER   \n",
      "\n",
      "                              PatientID_from  LSTM_Predicted_Increment  \\\n",
      "0   222141929e178baebc35086b2e09016803a31622                  2.128489   \n",
      "1   222141929e178baebc35086b2e09016803a31622                 31.927340   \n",
      "2   222141929e178baebc35086b2e09016803a31622                 19.156403   \n",
      "3   222141929e178baebc35086b2e09016803a31622                 23.413383   \n",
      "4   222141929e178baebc35086b2e09016803a31622                 48.955254   \n",
      "5   222141929e178baebc35086b2e09016803a31622                 53.212234   \n",
      "6   222141929e178baebc35086b2e09016803a31622                  6.385468   \n",
      "7   222141929e178baebc35086b2e09016803a31622                 27.670361   \n",
      "8   222141929e178baebc35086b2e09016803a31622                  2.128489   \n",
      "9   222141929e178baebc35086b2e09016803a31622                 27.670361   \n",
      "10  222141929e178baebc35086b2e09016803a31622                  2.128489   \n",
      "11  222141929e178baebc35086b2e09016803a31622                  2.128489   \n",
      "12  222141929e178baebc35086b2e09016803a31622                  2.128489   \n",
      "13  222141929e178baebc35086b2e09016803a31622                 27.670361   \n",
      "14  222141929e178baebc35086b2e09016803a31622                 19.156403   \n",
      "15  222141929e178baebc35086b2e09016803a31622                  6.385468   \n",
      "16  222141929e178baebc35086b2e09016803a31622                 48.955254   \n",
      "17  222141929e178baebc35086b2e09016803a31622                  2.128489   \n",
      "18  bab3a095d6a3db0905d1755af829c90031c7a3ce                  2.128489   \n",
      "19  bab3a095d6a3db0905d1755af829c90031c7a3ce                 31.927340   \n",
      "\n",
      "    GroundTruth_Increment  LSTM_Predicted_Cumulative  GroundTruth_Cumulative  \\\n",
      "0                    40.0                   2.128489                    40.0   \n",
      "1                     5.0                  34.055828                    45.0   \n",
      "2                     7.0                  53.212231                    52.0   \n",
      "3                    16.0                  76.625610                    68.0   \n",
      "4                     9.0                 125.580864                    77.0   \n",
      "5                     6.0                 178.793091                    83.0   \n",
      "6                   130.0                 185.178558                   213.0   \n",
      "7                     1.0                 212.848923                   214.0   \n",
      "8                    10.0                 214.977417                   224.0   \n",
      "9                     2.0                 242.647781                   226.0   \n",
      "10                    7.0                 244.776276                   233.0   \n",
      "11                    2.0                 246.904770                   235.0   \n",
      "12                   46.0                 249.033264                   281.0   \n",
      "13                   11.0                 276.703613                   292.0   \n",
      "14                    1.0                 295.860016                   293.0   \n",
      "15                   14.0                 302.245483                   307.0   \n",
      "16                    0.0                 351.200745                   307.0   \n",
      "17                 -307.0                 353.329224                     0.0   \n",
      "18                   26.0                   2.128489                    26.0   \n",
      "19                    5.0                  34.055828                    31.0   \n",
      "\n",
      "    LSTM_Predicted_TotalTime  TotalTime_Difference  \n",
      "0                        NaN                   NaN  \n",
      "1                        NaN                   NaN  \n",
      "2                        NaN                   NaN  \n",
      "3                        NaN                   NaN  \n",
      "4                        NaN                   NaN  \n",
      "5                        NaN                   NaN  \n",
      "6                        NaN                   NaN  \n",
      "7                        NaN                   NaN  \n",
      "8                        NaN                   NaN  \n",
      "9                        NaN                   NaN  \n",
      "10                       NaN                   NaN  \n",
      "11                       NaN                   NaN  \n",
      "12                       NaN                   NaN  \n",
      "13                       NaN                   NaN  \n",
      "14                       NaN                   NaN  \n",
      "15                       NaN                   NaN  \n",
      "16                       NaN                   NaN  \n",
      "17                353.329224            -46.329224  \n",
      "18                       NaN                   NaN  \n",
      "19                       NaN                   NaN  \n",
      "\n",
      "Generating visualizations for LSTM (total time approach - v18)...\n",
      "Generating visualizations for LSTM results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_35688\\624772111.py:352: RuntimeWarning: divide by zero encountered in divide\n",
      "  output_dict['Increment_Improvement_Pct'] = np.where(diff_transformer_inc > 1e-6, (diff_transformer_inc - diff_lstm_inc) / diff_transformer_inc * 100, 0 )\n",
      "C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_35688\\624772111.py:355: RuntimeWarning: divide by zero encountered in divide\n",
      "  output_dict['Cumulative_Improvement_Pct'] = np.where(diff_transformer_cum > 1e-6, (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100, 0 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LSTM training loss plot.\n",
      "Saved cumulative time comparison plot.\n",
      "Saved increment comparison plot.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Saved total time prediction analysis plot.\n",
      "Visualizations for LSTM completed!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main_lstm_total_time_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
