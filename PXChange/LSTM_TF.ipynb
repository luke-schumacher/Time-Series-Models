{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Assuming pad_sequences is available if needed, though direct use might change.\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Seaborn was commented out in original, keeping it that way\n",
    "#import test train split sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import standard scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute increments and cumulative times from proportions and total time\n",
    "def calculate_times_from_proportions(proportions_per_step, total_time_for_sequence, mask_per_step):\n",
    "    \"\"\"\n",
    "    Calculates time increments and cumulative times from step-wise proportions and a total time.\n",
    "    \"\"\"\n",
    "    proportions_tf = tf.cast(proportions_per_step, tf.float32)\n",
    "    total_time_tf = tf.cast(total_time_for_sequence, tf.float32)\n",
    "    mask_tf = tf.cast(mask_per_step, tf.float32)\n",
    "\n",
    "    if len(tf.shape(total_time_tf)) == 1:\n",
    "        total_time_tf = tf.expand_dims(total_time_tf, axis=-1)\n",
    "\n",
    "    masked_proportions = proportions_tf * mask_tf\n",
    "    row_sums = tf.reduce_sum(masked_proportions, axis=1, keepdims=True)\n",
    "    row_sums = tf.where(tf.equal(row_sums, 0), tf.ones_like(row_sums), row_sums) \n",
    "    normalized_proportions = masked_proportions / row_sums\n",
    "    increments = normalized_proportions * total_time_tf \n",
    "    cumulative_times = tf.cumsum(increments, axis=1)\n",
    "    \n",
    "    increments *= mask_tf\n",
    "    cumulative_times *= mask_tf\n",
    "    normalized_proportions *= mask_tf\n",
    "\n",
    "    return normalized_proportions, increments, cumulative_times\n",
    "\n",
    "# %%\n",
    "class TotalTimeLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Simplified LSTM model to predict only the total time of a sequence.\n",
    "    Architecture: LSTM -> GlobalAveragePooling1D -> Dense\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=64, dropout_rate=0.2):\n",
    "        super(TotalTimeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.lstm_layer = layers.LSTM(self.hidden_units, \n",
    "                                      return_sequences=True, \n",
    "                                      dropout=self.dropout_rate,\n",
    "                                      recurrent_dropout=self.dropout_rate,\n",
    "                                      name=\"lstm_simplified\")\n",
    "        \n",
    "        self.global_avg_pool = layers.GlobalAveragePooling1D(name=\"global_avg_pooling_simplified\")\n",
    "        self.total_time_head = layers.Dense(1, activation='linear', name=\"total_time_dense_simplified\") \n",
    "        \n",
    "    def call(self, inputs, training=False): \n",
    "        mask_bool = tf.reduce_any(tf.not_equal(inputs, 0.0), axis=-1)\n",
    "        x = self.lstm_layer(inputs, mask=mask_bool, training=training)\n",
    "        x = self.global_avg_pool(x, mask=mask_bool)\n",
    "        total_time_pred = self.total_time_head(x)\n",
    "        return total_time_pred\n",
    "\n",
    "# %%\n",
    "def process_input_data_for_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process the transformer predictions CSV to prepare data for LSTM training.\n",
    "    Target total time is the max GroundTruth_Cumulative for each sequence.\n",
    "    \"\"\"\n",
    "    print(f\"Processing data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "    df = pd.read_csv(transformer_predictions_file)\n",
    "    \n",
    "    required_cols = ['Predicted_Proportion', 'GroundTruth_Cumulative', 'GroundTruth_Increment', 'Sequence', 'Step']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"CSV must contain '{col}' column.\")\n",
    "\n",
    "    sequences = df['Sequence'].unique()\n",
    "    \n",
    "    X_data_list = []\n",
    "    y_total_times_list = [] \n",
    "    original_dfs_list = [] \n",
    "    transformer_proportions_list = [] \n",
    "    ground_truth_increments_list = []\n",
    "    ground_truth_cumulative_list = [] \n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_df = df[df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        if seq_df.empty:\n",
    "            original_dfs_list.append(seq_df) \n",
    "            continue\n",
    "        original_dfs_list.append(seq_df) \n",
    "\n",
    "        current_max_steps = seq_df['Step'].max()\n",
    "        if current_max_steps == 0: current_max_steps = 1 \n",
    "        \n",
    "        features = np.column_stack([\n",
    "            seq_df['Predicted_Proportion'].values,\n",
    "            seq_df['Step'].values / current_max_steps \n",
    "        ])\n",
    "        X_data_list.append(features)\n",
    "        \n",
    "        gt_cumulative_for_seq = seq_df['GroundTruth_Cumulative'].values\n",
    "        total_time_for_seq = np.max(gt_cumulative_for_seq) if len(gt_cumulative_for_seq) > 0 else 0.0\n",
    "        y_total_times_list.append(total_time_for_seq)\n",
    "        \n",
    "        transformer_proportions_list.append(seq_df['Predicted_Proportion'].values)\n",
    "        ground_truth_increments_list.append(seq_df['GroundTruth_Increment'].values)\n",
    "        ground_truth_cumulative_list.append(gt_cumulative_for_seq) \n",
    "\n",
    "    if not X_data_list:\n",
    "        raise ValueError(\"No valid sequences processed. Check CSV content or processing logic.\")\n",
    "\n",
    "    y_total_times_array_unpadded = np.array(y_total_times_list)\n",
    "    print(f\"\\nStatistics for TARGET y_total_times_list (max GT_Cumulative per seq, {len(y_total_times_array_unpadded)} sequences):\")\n",
    "    print(f\"  Mean: {np.mean(y_total_times_array_unpadded):.4f}, Std Dev: {np.std(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Min: {np.min(y_total_times_array_unpadded):.4f}, Max: {np.max(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Number of zeros (<=1e-6): {np.sum(y_total_times_array_unpadded <= 1e-6)}\")\n",
    "    print(f\"  Number non-positive (<=0): {np.sum(y_total_times_array_unpadded <= 0)}\\n\")\n",
    "\n",
    "    max_length = max(len(x) for x in X_data_list) if X_data_list else 0\n",
    "    if max_length == 0: raise ValueError(\"Max length is 0 after processing sequences.\")\n",
    "    num_features = X_data_list[0].shape[1] if X_data_list and X_data_list[0].shape[0] > 0 else 0\n",
    "    if num_features == 0: raise ValueError(\"Number of features is 0 after processing sequences.\")\n",
    "\n",
    "\n",
    "    X_padded = np.zeros((len(X_data_list), max_length, num_features), dtype=np.float32)\n",
    "    masks_padded_float = np.zeros((len(X_data_list), max_length), dtype=np.float32) \n",
    "    transformer_proportions_padded = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "    gt_increments_padded_original = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "    gt_cumulative_padded_original = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(X_data_list)):\n",
    "        seq_len = len(X_data_list[i])\n",
    "        if seq_len > 0:\n",
    "            X_padded[i, :seq_len, :] = X_data_list[i]\n",
    "            masks_padded_float[i, :seq_len] = 1.0 \n",
    "            transformer_proportions_padded[i, :seq_len] = transformer_proportions_list[i]\n",
    "            gt_increments_padded_original[i, :seq_len] = ground_truth_increments_list[i]\n",
    "            gt_cumulative_padded_original[i, :seq_len] = ground_truth_cumulative_list[i]\n",
    "        \n",
    "    y_total_times_np = np.array(y_total_times_list, dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        'X_lstm_input': X_padded,\n",
    "        'y_lstm_target_total_times': y_total_times_np,\n",
    "        'masks_for_calc': masks_padded_float, \n",
    "        'sequences_ids': sequences, \n",
    "        'original_dfs': original_dfs_list, \n",
    "        'transformer_proportions_padded': transformer_proportions_padded, \n",
    "        'gt_increments_padded_original': gt_increments_padded_original,\n",
    "        'gt_cumulative_padded_original': gt_cumulative_padded_original,\n",
    "        'max_len': max_length,\n",
    "        'num_features': num_features\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def train_total_time_lstm(transformer_predictions_file, epochs=50, batch_size=32, val_split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Train the simplified LSTM model to predict total_time, with target scaling.\n",
    "    \"\"\"\n",
    "    print(\"Processing data for LSTM training...\")\n",
    "    data_for_lstm = process_input_data_for_lstm(transformer_predictions_file)\n",
    "    \n",
    "    print(f\"Number of features for LSTM input: {data_for_lstm['num_features']}\")\n",
    "    print(f\"Max sequence length for LSTM input: {data_for_lstm['max_len']}\")\n",
    "\n",
    "    lstm_model = TotalTimeLSTM(hidden_units=64, dropout_rate=0.2) \n",
    "    \n",
    "    y_targets_all = data_for_lstm['y_lstm_target_total_times']\n",
    "    X_inputs_all = data_for_lstm['X_lstm_input']\n",
    "    \n",
    "    # --- Explicit model call to build layers before summary and compile ---\n",
    "    if len(X_inputs_all) > 0:\n",
    "        sample_batch_for_build = tf.convert_to_tensor(X_inputs_all[:1], dtype=tf.float32)\n",
    "        _ = lstm_model(sample_batch_for_build) # This call should build the layers\n",
    "        print(\"\\nSimplified LSTM Model Summary (after sample call):\")\n",
    "        lstm_model.summary() \n",
    "    else:\n",
    "        # Fallback if X_inputs_all is empty, though process_input_data_for_lstm should raise error earlier\n",
    "        input_shape_for_build = (None, data_for_lstm['max_len'], data_for_lstm['num_features'])\n",
    "        lstm_model.build(input_shape=input_shape_for_build)\n",
    "        print(\"\\nSimplified LSTM Model Summary (after .build()):\")\n",
    "        lstm_model.summary()\n",
    "\n",
    "\n",
    "    lstm_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001), \n",
    "        loss='mse' \n",
    "    )\n",
    "    \n",
    "    if np.any(np.isnan(X_inputs_all)) or np.any(np.isinf(X_inputs_all)):\n",
    "        print(\"CRITICAL WARNING: NaN or Inf found in X_inputs_all.\")\n",
    "    if np.any(np.isnan(y_targets_all)) or np.any(np.isinf(y_targets_all)):\n",
    "        print(\"CRITICAL WARNING: NaN or Inf found in y_targets_all.\")\n",
    "    if len(y_targets_all) > 0 and np.all(np.abs(y_targets_all) <= 1e-6) : \n",
    "        print(\"CRITICAL WARNING: All target total times are near zero. Model training will be ineffective.\")\n",
    "\n",
    "    # --- Target Scaling ---\n",
    "    target_scaler = StandardScaler()\n",
    "    # Reshape y_targets_all to 2D for scaler\n",
    "    y_targets_all_reshaped = y_targets_all.reshape(-1, 1)\n",
    "\n",
    "    if len(X_inputs_all) < 5: \n",
    "        print(\"Warning: Very few samples (<5), using all for training and no validation during fit.\")\n",
    "        X_train, y_train_scaled = X_inputs_all, target_scaler.fit_transform(y_targets_all_reshaped)\n",
    "        validation_data_for_fit = None\n",
    "    else:\n",
    "        X_train, X_val, y_train_orig, y_val_orig = train_test_split(\n",
    "            X_inputs_all, y_targets_all_reshaped, test_size=val_split_ratio, random_state=42, shuffle=True\n",
    "        )\n",
    "        # Fit scaler ONLY on training data, then transform both train and val\n",
    "        y_train_scaled = target_scaler.fit_transform(y_train_orig)\n",
    "        y_val_scaled = target_scaler.transform(y_val_orig)\n",
    "        validation_data_for_fit = (X_val, y_val_scaled)\n",
    "        \n",
    "        print(f\"\\nManually split data: {len(X_train)} train, {len(X_val)} validation samples.\")\n",
    "        # Print stats for original scale before scaling for clarity\n",
    "        print(\"Training target statistics (y_train - original scale):\")\n",
    "        print(f\"  Mean: {np.mean(y_train_orig.flatten()):.4f}, Std: {np.std(y_train_orig.flatten()):.4f}\")\n",
    "        print(\"Validation target statistics (y_val - original scale):\")\n",
    "        print(f\"  Mean: {np.mean(y_val_orig.flatten()):.4f}, Std: {np.std(y_val_orig.flatten()):.4f}\\n\")\n",
    "        if np.all(np.abs(y_val_orig) <= 1e-6): \n",
    "            print(\"CRITICAL WARNING: All original validation targets (y_val_orig) are effectively zero.\")\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=20, restore_best_weights=True, verbose=1 )\n",
    "    \n",
    "    print(\"Starting LSTM model training (with scaled targets)...\")\n",
    "    history = lstm_model.fit(\n",
    "        X_train, y_train_scaled, # Train on scaled targets         \n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        validation_data=validation_data_for_fit, \n",
    "        callbacks=[early_stopping], verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM training finished.\")\n",
    "    # Store the scaler in data_for_lstm to use for inverse transform during prediction\n",
    "    data_for_lstm['target_scaler'] = target_scaler\n",
    "    return lstm_model, history, data_for_lstm\n",
    "\n",
    "# %%\n",
    "def generate_refined_predictions_with_lstm(lstm_model, processed_data):\n",
    "    \"\"\"\n",
    "    Generate refined time predictions using LSTM's total_time.\n",
    "    Inverse transforms scaled predictions. Output CSV changed to _v6.\n",
    "    \"\"\"\n",
    "    print(\"Generating refined predictions using LSTM's total time...\")\n",
    "    \n",
    "    X_input_for_prediction = processed_data['X_lstm_input']\n",
    "    # LSTM predicts SCALED total times\n",
    "    lstm_predicted_scaled_total_times = lstm_model.predict(X_input_for_prediction) \n",
    "    \n",
    "    # Inverse transform the predictions to original scale\n",
    "    target_scaler = processed_data['target_scaler']\n",
    "    lstm_predicted_total_times_original_scale = target_scaler.inverse_transform(lstm_predicted_scaled_total_times)\n",
    "    lstm_predicted_total_times_original_scale = np.squeeze(lstm_predicted_total_times_original_scale)\n",
    "    lstm_predicted_total_times_original_scale = np.maximum(0, lstm_predicted_total_times_original_scale) # Ensure non-negative\n",
    "\n",
    "    transformer_step_proportions = processed_data['transformer_proportions_padded'] \n",
    "    masks_for_calc = processed_data['masks_for_calc'] \n",
    "\n",
    "    _, lstm_refined_increments, lstm_refined_cumulative = calculate_times_from_proportions(\n",
    "        transformer_step_proportions,\n",
    "        lstm_predicted_total_times_original_scale, # Use inverse_transformed predictions\n",
    "        masks_for_calc \n",
    "    )\n",
    "\n",
    "    lstm_refined_increments_np = lstm_refined_increments.numpy()\n",
    "    lstm_refined_cumulative_np = lstm_refined_cumulative.numpy()\n",
    "\n",
    "    results_list_df = []\n",
    "    original_dfs_from_processing = processed_data['original_dfs'] \n",
    "\n",
    "    for i, seq_id in enumerate(processed_data['sequences_ids']):\n",
    "        if i >= len(original_dfs_from_processing): continue\n",
    "        current_seq_df_base = original_dfs_from_processing[i].copy()\n",
    "        seq_len = len(current_seq_df_base)\n",
    "        if seq_len == 0:\n",
    "            if current_seq_df_base.empty: results_list_df.append(current_seq_df_base) \n",
    "            continue\n",
    "        if i >= len(lstm_predicted_total_times_original_scale): continue\n",
    "\n",
    "        current_seq_df_base['LSTM_Predicted_TotalTime'] = lstm_predicted_total_times_original_scale[i]\n",
    "        current_seq_df_base['LSTM_Predicted_Increment'] = lstm_refined_increments_np[i, :seq_len]\n",
    "        current_seq_df_base['LSTM_Predicted_Cumulative'] = lstm_refined_cumulative_np[i, :seq_len]\n",
    "        \n",
    "        if 'GroundTruth_Increment' in current_seq_df_base.columns and 'Predicted_Increment' in current_seq_df_base.columns:\n",
    "            gt_increment = current_seq_df_base['GroundTruth_Increment'].fillna(0) \n",
    "            transformer_pred_increment = current_seq_df_base['Predicted_Increment'].fillna(0) \n",
    "            lstm_pred_increment = current_seq_df_base['LSTM_Predicted_Increment'].fillna(0)\n",
    "            diff_transformer = np.abs(gt_increment - transformer_pred_increment)\n",
    "            diff_lstm = np.abs(gt_increment - lstm_pred_increment)\n",
    "            current_seq_df_base['Increment_MAE_Transformer'] = diff_transformer\n",
    "            current_seq_df_base['Increment_MAE_LSTM'] = diff_lstm\n",
    "            current_seq_df_base['Increment_Improvement_Pct'] = np.where(\n",
    "                diff_transformer > 1e-6, (diff_transformer - diff_lstm) / diff_transformer * 100, 0 )\n",
    "        \n",
    "        if 'GroundTruth_Cumulative' in current_seq_df_base.columns and 'Predicted_Cumulative' in current_seq_df_base.columns:\n",
    "            gt_cumulative = current_seq_df_base['GroundTruth_Cumulative'].fillna(0)\n",
    "            transformer_pred_cumulative = current_seq_df_base['Predicted_Cumulative'].fillna(0) \n",
    "            lstm_pred_cumulative = current_seq_df_base['LSTM_Predicted_Cumulative'].fillna(0)\n",
    "            diff_transformer_cum = np.abs(gt_cumulative - transformer_pred_cumulative)\n",
    "            diff_lstm_cum = np.abs(gt_cumulative - lstm_pred_cumulative)\n",
    "            current_seq_df_base['Cumulative_MAE_Transformer'] = diff_transformer_cum\n",
    "            current_seq_df_base['Cumulative_MAE_LSTM'] = diff_lstm_cum\n",
    "            current_seq_df_base['Cumulative_Improvement_Pct'] = np.where(\n",
    "                diff_transformer_cum > 1e-6, (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100, 0 )\n",
    "        results_list_df.append(current_seq_df_base)\n",
    "    \n",
    "    if not results_list_df: return pd.DataFrame()\n",
    "    final_results_df = pd.concat(results_list_df, ignore_index=True)\n",
    "    \n",
    "    output_filename = 'predictions_lstm_refined_total_time_v6.csv' # Changed filename\n",
    "    final_results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Combined and refined predictions saved to {output_filename}\")\n",
    "    \n",
    "    if not final_results_df.empty:\n",
    "        if 'Increment_MAE_Transformer' in final_results_df.columns and 'Increment_MAE_LSTM' in final_results_df.columns:\n",
    "            valid_inc_mae_transformer = final_results_df['Increment_MAE_Transformer'].dropna()\n",
    "            valid_inc_mae_lstm = final_results_df['Increment_MAE_LSTM'].dropna()\n",
    "            if not valid_inc_mae_transformer.empty and not valid_inc_mae_lstm.empty:\n",
    "                avg_transformer_inc_mae = valid_inc_mae_transformer.mean(); avg_lstm_inc_mae = valid_inc_mae_lstm.mean()\n",
    "                print(\"\\n--- Mean Absolute Error for Increments ---\")\n",
    "                print(f\"Transformer MAE (Increments): {avg_transformer_inc_mae:.4f}\")\n",
    "                print(f\"LSTM-Refined MAE (Increments): {avg_lstm_inc_mae:.4f}\")\n",
    "                if avg_transformer_inc_mae > 1e-6: \n",
    "                    print(f\"Improvement (Increments): {(avg_transformer_inc_mae - avg_lstm_inc_mae) / avg_transformer_inc_mae * 100:.2f}%\")\n",
    "\n",
    "        if 'Cumulative_MAE_Transformer' in final_results_df.columns and 'Cumulative_MAE_LSTM' in final_results_df.columns:\n",
    "            valid_cum_mae_transformer = final_results_df['Cumulative_MAE_Transformer'].dropna()\n",
    "            valid_cum_mae_lstm = final_results_df['Cumulative_MAE_LSTM'].dropna()\n",
    "            if not valid_cum_mae_transformer.empty and not valid_cum_mae_lstm.empty:\n",
    "                avg_transformer_cum_mae = valid_cum_mae_transformer.mean(); avg_lstm_cum_mae = valid_cum_mae_lstm.mean()\n",
    "                print(\"\\n--- Mean Absolute Error for Cumulative Times ---\")\n",
    "                print(f\"Transformer MAE (Cumulative): {avg_transformer_cum_mae:.4f}\")\n",
    "                print(f\"LSTM-Refined MAE (Cumulative): {avg_lstm_cum_mae:.4f}\")\n",
    "                if avg_transformer_cum_mae > 1e-6: \n",
    "                    print(f\"Improvement (Cumulative): {(avg_transformer_cum_mae - avg_lstm_cum_mae) / avg_transformer_cum_mae * 100:.2f}%\")\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data['y_lstm_target_total_times'] \n",
    "    if len(lstm_predicted_total_times_original_scale) == len(gt_total_times_for_lstm_training):\n",
    "        mae_total_time_lstm = np.mean(np.abs(gt_total_times_for_lstm_training - lstm_predicted_total_times_original_scale))\n",
    "        print(\"\\n--- LSTM Total Time Prediction Performance (vs Max GT Cumulative) ---\")\n",
    "        print(f\"MAE for LSTM Predicted Total Time: {mae_total_time_lstm:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nWarning: Mismatch in lengths for GT total times and LSTM predicted total times for MAE calculation.\")\n",
    "    return final_results_df\n",
    "\n",
    "# %%\n",
    "def visualize_lstm_results(results_df, processed_data, lstm_model, training_history):\n",
    "    if results_df.empty: print(\"Results DataFrame is empty, skipping visualizations.\"); return\n",
    "    print(\"Generating visualizations for LSTM results...\")\n",
    "    plt.style.use('ggplot')\n",
    "    if training_history and hasattr(training_history, 'history'):\n",
    "        if 'loss' in training_history.history and 'val_loss' in training_history.history:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "            plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "            plt.title('LSTM Model Loss (Predicting Scaled Total Time)') # Note: Loss is on scaled values\n",
    "            plt.xlabel('Epoch'); plt.ylabel('Mean Squared Error (Scaled Loss)'); plt.legend(); plt.tight_layout()\n",
    "            plt.savefig('lstm_total_time_training_loss_v6.png'); print(\"Saved LSTM training loss plot.\"); plt.close()\n",
    "    else: print(\"Warning: Training history not available or malformed.\")\n",
    "\n",
    "    sample_sequence_ids = results_df['Sequence'].unique()\n",
    "    if len(sample_sequence_ids) == 0 : print(\"No sequences in results_df for plotting.\"); return\n",
    "    sample_sequence_ids = sample_sequence_ids[:min(5, len(sample_sequence_ids))]\n",
    "    \n",
    "    if len(sample_sequence_ids) > 0:\n",
    "        num_plots = len(sample_sequence_ids); fig_height = max(8, 3 * num_plots) \n",
    "        plt.figure(figsize=(15, fig_height))\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Cumulative'], 'o-', label='GT Cumul.', ms=4)\n",
    "            if 'Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Cumulative'], 's--', label='Transformer Cumul.', ms=4)\n",
    "            if 'LSTM_Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Cumulative'], '^-.', label='LSTM-Refined Cumul.', ms=4)\n",
    "            plt.title(f'Cumulative Times: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Cumulative Time'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_cumulative_time_comparison_v6.png'); print(\"Saved cumulative time comparison plot.\"); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(15, fig_height))\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Increment'], 'o-', label='GT Incr.', ms=4)\n",
    "            if 'Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Increment'], 's--', label='Transformer Incr.', ms=4)\n",
    "            if 'LSTM_Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Increment'], '^-.', label='LSTM-Refined Incr.', ms=4)\n",
    "            plt.title(f'Time Increments: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Time Increment'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_increment_comparison_v6.png'); print(\"Saved increment comparison plot.\"); plt.close()\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data.get('y_lstm_target_total_times', np.array([])) # Original scale\n",
    "    if lstm_model is not None and 'X_lstm_input' in processed_data and 'target_scaler' in processed_data:\n",
    "        X_input_tensor = tf.convert_to_tensor(processed_data['X_lstm_input'], dtype=tf.float32)\n",
    "        lstm_pred_scaled_total_t = lstm_model.predict(X_input_tensor)\n",
    "        lstm_pred_original_scale_total_t = processed_data['target_scaler'].inverse_transform(lstm_pred_scaled_total_t).squeeze()\n",
    "        \n",
    "        if lstm_pred_original_scale_total_t.ndim == 0: lstm_pred_original_scale_total_t = np.array([lstm_pred_original_scale_total_t])\n",
    "            \n",
    "        if gt_total_times_for_lstm_training.size > 0 and lstm_pred_original_scale_total_t.size > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1); \n",
    "            plt.hist(gt_total_times_for_lstm_training, bins=30, alpha=0.7, label='GT Total Times (Max Cumul.)')\n",
    "            plt.hist(lstm_pred_original_scale_total_t, bins=30, alpha=0.7, label='LSTM Pred Total Times (Original Scale)')\n",
    "            plt.xlabel('Total Time'); plt.ylabel('Frequency'); plt.title('Distribution of Total Times'); plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2); \n",
    "            if len(gt_total_times_for_lstm_training) == len(lstm_pred_original_scale_total_t):\n",
    "                errors_total_time = gt_total_times_for_lstm_training - lstm_pred_original_scale_total_t\n",
    "                plt.hist(errors_total_time, bins=30, alpha=0.7, color='red')\n",
    "                plt.xlabel('Prediction Error (GT Max Cumul. - Pred)'); plt.ylabel('Frequency'); plt.title('LSTM Total Time Prediction Errors')\n",
    "                if errors_total_time.size > 0: \n",
    "                    mean_error_val = errors_total_time.mean(); plt.axvline(mean_error_val, color='k', ls='--', lw=1, label=f'Mean Error: {mean_error_val:.2f}')\n",
    "                plt.legend()\n",
    "            else: print(\"Warning: Mismatch length GT total times and predictions for error histogram.\")\n",
    "            plt.tight_layout(); plt.savefig('lstm_total_time_prediction_analysis_v6.png'); print(\"Saved total time prediction analysis plot.\"); plt.close()\n",
    "        else: print(\"Warning: Not enough data for total time distribution plots.\")\n",
    "    else: print(\"Warning: LSTM model, input data, or target_scaler missing for total time prediction plot.\")\n",
    "    print(\"Visualizations for LSTM completed!\")\n",
    "\n",
    "# %%\n",
    "def main_lstm_total_time_flow():\n",
    "    try:\n",
    "        transformer_predictions_file = \"predictions_transformer_182625.csv\" \n",
    "        if not os.path.exists(transformer_predictions_file):\n",
    "            print(f\"Error: Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "            print(\"Creating DUMMY CSV for testing flow.\")\n",
    "            dummy_data = []\n",
    "            for seq_idx in range(250): # More dummy data\n",
    "                num_steps = np.random.randint(8, 50) # Ensure decent sequence lengths\n",
    "                steps = np.arange(1, num_steps + 1)\n",
    "                # Increments that lead to a good range of total times\n",
    "                gt_increments = np.random.lognormal(mean=1.5, sigma=0.8, size=num_steps) + 0.5 \n",
    "                gt_increments = np.maximum(gt_increments, 0.01) \n",
    "                gt_cumulative = np.cumsum(gt_increments)\n",
    "                \n",
    "                raw_props = np.random.rand(num_steps) + 0.1 # Ensure non-zero proportions\n",
    "                pred_proportions = raw_props / raw_props.sum() \n",
    "                \n",
    "                actual_sequence_total_time = gt_cumulative[-1] if num_steps > 0 else 1.0\n",
    "                actual_sequence_total_time = max(actual_sequence_total_time, 1.0) \n",
    "\n",
    "                # Transformer's \"prediction\" for its proportions (can be different from actual total time)\n",
    "                dummy_transformer_effective_total_time = actual_sequence_total_time * np.random.normal(loc=1.0, scale=0.3)\n",
    "                dummy_transformer_effective_total_time = max(dummy_transformer_effective_total_time, 0.1)\n",
    "\n",
    "                pred_increments_from_transformer = pred_proportions * dummy_transformer_effective_total_time\n",
    "                pred_cumulative_from_transformer = np.cumsum(pred_increments_from_transformer)\n",
    "\n",
    "                for s_idx in range(num_steps):\n",
    "                    dummy_data.append({\n",
    "                        'Sequence': seq_idx, 'Step': steps[s_idx], 'SourceID': f'MRI_DUMMY_{s_idx%5 +1}',\n",
    "                        'Predicted_Proportion': pred_proportions[s_idx], \n",
    "                        'Predicted_Increment': pred_increments_from_transformer[s_idx],\n",
    "                        'Predicted_Cumulative': pred_cumulative_from_transformer[s_idx], \n",
    "                        'GroundTruth_Increment': gt_increments[s_idx], \n",
    "                        'GroundTruth_Cumulative': gt_cumulative[s_idx]  })\n",
    "            if not dummy_data: \n",
    "                 dummy_data.append({ 'Sequence': 0, 'Step': 1, 'SourceID': 'MRI_DUMMY_0', 'Predicted_Proportion': 1.0, \n",
    "                                     'Predicted_Increment': 10.0, 'Predicted_Cumulative': 10.0, \n",
    "                                     'GroundTruth_Increment': 10.0, 'GroundTruth_Cumulative': 10.0})\n",
    "            dummy_df = pd.DataFrame(dummy_data)\n",
    "            dummy_df.to_csv(transformer_predictions_file, index=False)\n",
    "            print(f\"Dummy '{transformer_predictions_file}' created with {len(dummy_df)} rows and {len(dummy_df['Sequence'].unique())} sequences.\")\n",
    "        \n",
    "        lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
    "            transformer_predictions_file, epochs=150, batch_size=32 ) # batch_size back to 32\n",
    "        \n",
    "        if lstm_model is None or processed_lstm_data is None:\n",
    "            print(\"LSTM training failed or returned None. Exiting.\"); return\n",
    "\n",
    "        refined_results_df = generate_refined_predictions_with_lstm(lstm_model, processed_lstm_data)\n",
    "        if not refined_results_df.empty:\n",
    "            print(\"\\nSample of Refined Predictions (LSTM Total Time Approach - v6):\")\n",
    "            display_cols = [ 'Sequence', 'Step', 'SourceID', \n",
    "                             'Predicted_Increment', 'LSTM_Predicted_Increment', 'GroundTruth_Increment', \n",
    "                             'Predicted_Cumulative', 'LSTM_Predicted_Cumulative', 'GroundTruth_Cumulative',\n",
    "                             'LSTM_Predicted_TotalTime', 'Increment_Improvement_Pct']\n",
    "            actual_display_cols = [col for col in display_cols if col in refined_results_df.columns]\n",
    "            print(refined_results_df[actual_display_cols].head(10))\n",
    "            print(\"\\nGenerating visualizations for LSTM (total time approach - v6)...\")\n",
    "            visualize_lstm_results(refined_results_df, processed_lstm_data, lstm_model, lstm_history)\n",
    "        else: print(\"No refined predictions were generated by the LSTM flow.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM (total time) main function: {e}\"); import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for LSTM training...\n",
      "Processing data from: predictions_transformer_182625.csv\n",
      "\n",
      "Statistics for TARGET y_total_times_list (max GT_Cumulative per seq, 186 sequences):\n",
      "  Mean: 374.8065, Std Dev: 348.4868\n",
      "  Min: 0.0000, Max: 2900.0000\n",
      "  Number of zeros (<=1e-6): 5\n",
      "  Number non-positive (<=0): 5\n",
      "\n",
      "Number of features for LSTM input: 2\n",
      "Max sequence length for LSTM input: 42\n",
      "\n",
      "Simplified LSTM Model Summary (after sample call):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"total_time_lstm_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"total_time_lstm_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_simplified (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pooling_simplified   │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_simplified     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                         │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_simplified (\u001b[38;5;33mLSTM\u001b[0m)          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │        \u001b[38;5;34m17,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pooling_simplified   │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_simplified     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │            \u001b[38;5;34m65\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)                         │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,217</span> (67.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,217\u001b[0m (67.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,217</span> (67.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,217\u001b[0m (67.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manually split data: 148 train, 38 validation samples.\n",
      "Training target statistics (y_train - original scale):\n",
      "  Mean: 390.0405, Std: 362.5637\n",
      "Validation target statistics (y_val - original scale):\n",
      "  Mean: 315.4737, Std: 279.3477\n",
      "\n",
      "Starting LSTM model training (with scaled targets)...\n",
      "Epoch 1/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 1.2112 - val_loss: 0.6538\n",
      "Epoch 2/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.3508 - val_loss: 0.6462\n",
      "Epoch 3/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.2826 - val_loss: 0.6417\n",
      "Epoch 4/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1398 - val_loss: 0.6318\n",
      "Epoch 5/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.8268 - val_loss: 0.6302\n",
      "Epoch 6/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.8439 - val_loss: 0.6316\n",
      "Epoch 7/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.9842 - val_loss: 0.6326\n",
      "Epoch 8/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.7746 - val_loss: 0.6336\n",
      "Epoch 9/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.8341 - val_loss: 0.6393\n",
      "Epoch 10/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.9688 - val_loss: 0.6473\n",
      "Epoch 11/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.8778 - val_loss: 0.6529\n",
      "Epoch 12/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0579 - val_loss: 0.6565\n",
      "Epoch 13/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3727 - val_loss: 0.6489\n",
      "Epoch 14/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.9659 - val_loss: 0.6400\n",
      "Epoch 15/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.8655 - val_loss: 0.6361\n",
      "Epoch 16/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.8585 - val_loss: 0.6327\n",
      "Epoch 17/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.7164 - val_loss: 0.6359\n",
      "Epoch 18/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0032 - val_loss: 0.6400\n",
      "Epoch 19/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7665 - val_loss: 0.6336\n",
      "Epoch 20/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1529 - val_loss: 0.6361\n",
      "Epoch 21/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.8942 - val_loss: 0.6397\n",
      "Epoch 22/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3624 - val_loss: 0.6430\n",
      "Epoch 23/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.0125 - val_loss: 0.6362\n",
      "Epoch 24/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7986 - val_loss: 0.6310\n",
      "Epoch 25/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.3548 - val_loss: 0.6388\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "LSTM training finished.\n",
      "Generating refined predictions using LSTM's total time...\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Combined and refined predictions saved to predictions_lstm_refined_total_time_v6.csv\n",
      "\n",
      "--- Mean Absolute Error for Increments ---\n",
      "Transformer MAE (Increments): 46.1613\n",
      "LSTM-Refined MAE (Increments): 46.8303\n",
      "Improvement (Increments): -1.45%\n",
      "\n",
      "--- Mean Absolute Error for Cumulative Times ---\n",
      "Transformer MAE (Cumulative): 62.9617\n",
      "LSTM-Refined MAE (Cumulative): 144.3640\n",
      "Improvement (Cumulative): -129.29%\n",
      "\n",
      "--- LSTM Total Time Prediction Performance (vs Max GT Cumulative) ---\n",
      "MAE for LSTM Predicted Total Time: 211.7170\n",
      "\n",
      "Sample of Refined Predictions (LSTM Total Time Approach - v6):\n",
      "   Sequence  Step     SourceID  Predicted_Increment  LSTM_Predicted_Increment  \\\n",
      "0         0     1  MRI_MSR_104             1.574359                  1.979804   \n",
      "1         0     2    MRI_FRR_2            23.615385                 29.697058   \n",
      "2         0     3  MRI_FRR_257             7.871795                  9.899020   \n",
      "3         0     4  MRI_FRR_264            17.317950                 21.777842   \n",
      "4         0     5  MRI_FRR_264            26.764105                 33.656670   \n",
      "5         0     6   MRI_CCS_11            45.656410                 57.414314   \n",
      "6         0     7   MRI_CCS_11             1.574359                  1.979804   \n",
      "7         0     8  MRI_FRR_257            39.358980                 49.495098   \n",
      "8         0     9  MRI_FRR_264             1.574359                  1.979804   \n",
      "9         0    10  MRI_FRR_264            20.466667                 25.737450   \n",
      "\n",
      "   GroundTruth_Increment  Predicted_Cumulative  LSTM_Predicted_Cumulative  \\\n",
      "0                   40.0              3.148718                   1.979804   \n",
      "1                    5.0             26.764103                  31.676861   \n",
      "2                    7.0             34.635900                  41.575882   \n",
      "3                   16.0             51.953850                  63.353722   \n",
      "4                    9.0             78.717960                  97.010391   \n",
      "5                    6.0            124.374370                 154.424713   \n",
      "6                  130.0            125.948720                 156.404510   \n",
      "7                    1.0            165.307710                 205.899612   \n",
      "8                   10.0            166.882060                 207.879410   \n",
      "9                    2.0            187.348720                 233.616852   \n",
      "\n",
      "   GroundTruth_Cumulative  LSTM_Predicted_TotalTime  Increment_Improvement_Pct  \n",
      "0                    40.0                 384.08197                   1.055141  \n",
      "1                    45.0                 384.08197                 -32.670142  \n",
      "2                    52.0                 384.08197                -232.534621  \n",
      "3                    68.0                 384.08197                -338.396113  \n",
      "4                    77.0                 384.08197                 -38.800517  \n",
      "5                    83.0                 384.08197                 -29.649442  \n",
      "6                   213.0                 384.08197                   0.315704  \n",
      "7                   214.0                 384.08197                 -26.424368  \n",
      "8                   224.0                 384.08197                   4.812036  \n",
      "9                   226.0                 384.08197                 -28.542144  \n",
      "\n",
      "Generating visualizations for LSTM (total time approach - v6)...\n",
      "Generating visualizations for LSTM results...\n",
      "Saved LSTM training loss plot.\n",
      "Saved cumulative time comparison plot.\n",
      "Saved increment comparison plot.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Saved total time prediction analysis plot.\n",
      "Visualizations for LSTM completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_lstm_total_time_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
