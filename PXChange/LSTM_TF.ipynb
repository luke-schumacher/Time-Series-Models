{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import os # For checking file existence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # For target scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute increments and cumulative times from proportions and total time\n",
    "def calculate_times_from_proportions(proportions_per_step, total_time_for_sequence, mask_per_step):\n",
    "    \"\"\"\n",
    "    Calculates time increments and cumulative times from step-wise proportions and a total time.\n",
    "    \"\"\"\n",
    "    proportions_tf = tf.cast(proportions_per_step, tf.float32)\n",
    "    total_time_tf = tf.cast(total_time_for_sequence, tf.float32)\n",
    "    mask_tf = tf.cast(mask_per_step, tf.float32)\n",
    "\n",
    "    if len(tf.shape(total_time_tf)) == 1:\n",
    "        total_time_tf = tf.expand_dims(total_time_tf, axis=-1)\n",
    "\n",
    "    masked_proportions = proportions_tf * mask_tf\n",
    "    row_sums = tf.reduce_sum(masked_proportions, axis=1, keepdims=True)\n",
    "    row_sums = tf.where(tf.equal(row_sums, 0), tf.ones_like(row_sums), row_sums) \n",
    "    normalized_proportions = masked_proportions / row_sums\n",
    "    increments = normalized_proportions * total_time_tf \n",
    "    cumulative_times = tf.cumsum(increments, axis=1)\n",
    "    \n",
    "    increments *= mask_tf\n",
    "    cumulative_times *= mask_tf\n",
    "    normalized_proportions *= mask_tf\n",
    "\n",
    "    return normalized_proportions, increments, cumulative_times\n",
    "\n",
    "# %%\n",
    "class TotalTimeLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Enhanced LSTM model with a manual Bahdanau-style Attention to predict the total time.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=256, \n",
    "                 dense_units_1=128, \n",
    "                 dense_units_2=64, \n",
    "                 dropout_rate=0.4):\n",
    "        super(TotalTimeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.dense_units_1 = dense_units_1\n",
    "        self.dense_units_2 = dense_units_2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.bilstm_output_dim = hidden_units * 2\n",
    "\n",
    "        # --- Layers for sequential input ---\n",
    "        self.bi_lstm_layer = layers.Bidirectional(\n",
    "            layers.LSTM(self.hidden_units, return_sequences=True, dropout=self.dropout_rate, recurrent_dropout=0.25),\n",
    "            name=\"bidirectional_lstm_v19\"\n",
    "        )\n",
    "        \n",
    "        # --- Manual Attention Layers ---\n",
    "        self.W1 = layers.Dense(self.bilstm_output_dim, name=\"attention_dense_W1\")\n",
    "        self.W2 = layers.Dense(self.bilstm_output_dim, name=\"attention_dense_W2\")\n",
    "        self.V = layers.Dense(1, name=\"attention_dense_V\")\n",
    "        \n",
    "        # --- Layers for combined features ---\n",
    "        self.concat_layer = layers.Concatenate(name=\"concatenate_features_v19\")\n",
    "        self.dense_1 = layers.Dense(\n",
    "            self.dense_units_1, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), name=\"dense_1_v19\"\n",
    "        )\n",
    "        self.dropout_1 = layers.Dropout(self.dropout_rate, name=\"dropout_1_v19\")\n",
    "        self.dense_2 = layers.Dense(\n",
    "            self.dense_units_2, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001), name=\"dense_2_v19\"\n",
    "        )\n",
    "        self.dropout_2 = layers.Dropout(self.dropout_rate, name=\"dropout_2_v19\")\n",
    "        self.total_time_head = layers.Dense(1, activation='linear', name=\"total_time_dense_v19\") \n",
    "    \n",
    "    def call(self, inputs, training=False): \n",
    "        sequence_input, global_features_input = inputs \n",
    "        mask_bool_seq = tf.reduce_any(tf.not_equal(sequence_input, 0.0), axis=-1)\n",
    "        \n",
    "        lstm_output = self.bi_lstm_layer(sequence_input, mask=mask_bool_seq, training=training)\n",
    "        \n",
    "        query_summary = tf.reduce_mean(lstm_output, axis=1)\n",
    "        query_with_time_axis = tf.expand_dims(query_summary, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(self.W1(lstm_output) + self.W2(query_with_time_axis)))\n",
    "\n",
    "        mask_for_scores = tf.expand_dims(tf.cast(mask_bool_seq, tf.float32), -1)\n",
    "        masked_score = score - (1. - mask_for_scores) * 1e9\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "\n",
    "        context_vector = tf.reduce_sum(attention_weights * lstm_output, axis=1)\n",
    "        \n",
    "        combined_features = self.concat_layer([context_vector, global_features_input])\n",
    "        \n",
    "        x = self.dense_1(combined_features)\n",
    "        x = self.dropout_1(x, training=training)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout_2(x, training=training)\n",
    "        total_time_pred = self.total_time_head(x)\n",
    "        \n",
    "        return total_time_pred\n",
    "\n",
    "# %%\n",
    "def process_input_data_for_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process the transformer predictions CSV to prepare data for LSTM training,\n",
    "    incorporating new sequential and global features.\n",
    "    \"\"\"\n",
    "    print(f\"Processing data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "    df = pd.read_csv(transformer_predictions_file)\n",
    "    \n",
    "    required_cols = ['Predicted_Proportion', 'GroundTruth_Cumulative', 'GroundTruth_Increment', 'Sequence', 'Step']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns: raise ValueError(f\"CSV must contain '{col}' column.\")\n",
    "\n",
    "    sequences = df['Sequence'].unique()\n",
    "    \n",
    "    X_sequential_data_list = []; X_global_features_list = []\n",
    "    y_total_times_list = []; original_dfs_list = [] \n",
    "    transformer_proportions_list = []; ground_truth_increments_list = []\n",
    "    ground_truth_cumulative_list = [] \n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_df = df[df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        if seq_df.empty:\n",
    "            original_dfs_list.append(seq_df); continue\n",
    "        original_dfs_list.append(seq_df) \n",
    "\n",
    "        actual_sequence_length = float(len(seq_df))\n",
    "        \n",
    "        # --- Sequential Features ---\n",
    "        props_for_seq = seq_df['Predicted_Proportion'].values\n",
    "        \n",
    "        current_max_steps = seq_df['Step'].max()\n",
    "        if current_max_steps == 0: current_max_steps = 1 \n",
    "        \n",
    "        max_timediff = seq_df['timediff'].max()\n",
    "        if max_timediff == 0: max_timediff = 1\n",
    "\n",
    "        sequential_features = np.column_stack([\n",
    "            props_for_seq, \n",
    "            seq_df['Step'].values / current_max_steps,\n",
    "            seq_df['timediff'].values / max_timediff, # Normalized timediff\n",
    "            seq_df['PTAB'].values # PTAB added directly\n",
    "        ])\n",
    "        X_sequential_data_list.append(sequential_features)\n",
    "        \n",
    "        # --- Global Features ---\n",
    "        sum_props = np.sum(props_for_seq)\n",
    "        mean_props = np.mean(props_for_seq) if actual_sequence_length > 0 else 0.0\n",
    "        std_props = np.std(props_for_seq) if actual_sequence_length > 1 else 0.0\n",
    "        max_prop = np.max(props_for_seq) if actual_sequence_length > 0 else 0.0\n",
    "        \n",
    "        # Take the first value for patient-level features as they are constant per sequence\n",
    "        first_row = seq_df.iloc[0]\n",
    "        global_features_for_seq = np.array([\n",
    "            sum_props, mean_props, std_props, max_prop,\n",
    "            first_row['patient_height'], first_row['patient_weight'],\n",
    "            first_row['patient_age'], first_row['patient_gender']\n",
    "        ], dtype=np.float32)\n",
    "        X_global_features_list.append(global_features_for_seq)\n",
    "        \n",
    "        gt_cumulative_for_seq = seq_df['GroundTruth_Cumulative'].values\n",
    "        total_time_for_seq = np.max(gt_cumulative_for_seq) if len(gt_cumulative_for_seq) > 0 else 0.0\n",
    "        y_total_times_list.append(total_time_for_seq)\n",
    "        \n",
    "        transformer_proportions_list.append(props_for_seq)\n",
    "        ground_truth_increments_list.append(seq_df['GroundTruth_Increment'].values)\n",
    "        ground_truth_cumulative_list.append(gt_cumulative_for_seq) \n",
    "\n",
    "    if not X_sequential_data_list: raise ValueError(\"No valid sequences processed.\")\n",
    "\n",
    "    y_total_times_array_unpadded = np.array(y_total_times_list)\n",
    "    print(f\"\\nStatistics for TARGET y_total_times_list (max GT_Cumulative per seq, {len(y_total_times_array_unpadded)} sequences):\")\n",
    "    print(f\"  Mean: {np.mean(y_total_times_array_unpadded):.4f}, Std Dev: {np.std(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Min: {np.min(y_total_times_array_unpadded):.4f}, Max: {np.max(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Number of zeros (<=1e-6): {np.sum(y_total_times_array_unpadded <= 1e-6)}\\n\")\n",
    "\n",
    "    max_length_sequential = max(len(x) for x in X_sequential_data_list) if X_sequential_data_list else 0\n",
    "    if max_length_sequential == 0: raise ValueError(\"Max length for sequential features is 0.\")\n",
    "    num_sequential_features = X_sequential_data_list[0].shape[1]\n",
    "    num_global_features = X_global_features_list[0].shape[0]\n",
    "\n",
    "    X_sequential_padded = np.zeros((len(X_sequential_data_list), max_length_sequential, num_sequential_features), dtype=np.float32)\n",
    "    masks_padded_float = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32) \n",
    "    transformer_proportions_padded = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "    gt_increments_padded_original = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "    gt_cumulative_padded_original = np.zeros((len(X_sequential_data_list), max_length_sequential), dtype=np.float32)\n",
    "\n",
    "    for i, seq_data in enumerate(X_sequential_data_list):\n",
    "        seq_len = len(seq_data)\n",
    "        if seq_len > 0:\n",
    "            X_sequential_padded[i, :seq_len, :] = seq_data\n",
    "            masks_padded_float[i, :seq_len] = 1.0 \n",
    "            transformer_proportions_padded[i, :seq_len] = transformer_proportions_list[i]\n",
    "            gt_increments_padded_original[i, :seq_len] = ground_truth_increments_list[i]\n",
    "            gt_cumulative_padded_original[i, :seq_len] = ground_truth_cumulative_list[i]\n",
    "        \n",
    "    y_total_times_np = np.array(y_total_times_list, dtype=np.float32)\n",
    "    X_global_features_np = np.array(X_global_features_list, dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        'X_sequential_input': X_sequential_padded, \n",
    "        'X_global_features_input': X_global_features_np, \n",
    "        'y_lstm_target_total_times': y_total_times_np,\n",
    "        'masks_for_calc': masks_padded_float, \n",
    "        'sequences_ids': sequences, \n",
    "        'original_dfs': original_dfs_list, \n",
    "        'transformer_proportions_padded': transformer_proportions_padded, \n",
    "        'gt_increments_padded_original': gt_increments_padded_original,\n",
    "        'gt_cumulative_padded_original': gt_cumulative_padded_original,\n",
    "        'max_len_sequential': max_length_sequential,\n",
    "        'num_sequential_features': num_sequential_features,\n",
    "        'num_global_features': num_global_features\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def train_total_time_lstm(transformer_predictions_file, epochs=50, batch_size=32, val_split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Train the enhanced LSTM model to predict total_time.\n",
    "    \"\"\"\n",
    "    print(\"Processing data for LSTM training...\")\n",
    "    data_for_lstm = process_input_data_for_lstm(transformer_predictions_file)\n",
    "    \n",
    "    print(f\"Num sequential features: {data_for_lstm['num_sequential_features']}, Max seq length: {data_for_lstm['max_len_sequential']}\")\n",
    "    print(f\"Num global features: {data_for_lstm['num_global_features']}\")\n",
    "\n",
    "    lstm_model = TotalTimeLSTM(hidden_units=256, dense_units_1=128, dense_units_2=64, dropout_rate=0.4) \n",
    "    \n",
    "    X_sequential_all = data_for_lstm['X_sequential_input']\n",
    "    X_global_all = data_for_lstm['X_global_features_input']\n",
    "    y_targets_all = data_for_lstm['y_lstm_target_total_times']\n",
    "    \n",
    "    if len(X_sequential_all) > 0:\n",
    "        sample_seq_input_for_build = tf.convert_to_tensor(X_sequential_all[:1], dtype=tf.float32)\n",
    "        sample_glob_input_for_build = tf.convert_to_tensor(X_global_all[:1], dtype=tf.float32)\n",
    "        _ = lstm_model((sample_seq_input_for_build, sample_glob_input_for_build)) \n",
    "        print(\"\\nEnhanced LSTM Model Summary (v19 - after sample call):\")\n",
    "        lstm_model.summary(expand_nested=True) \n",
    "    else: print(\"Warning: No data to build model with sample call.\")\n",
    "\n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse' ) \n",
    "    \n",
    "    if np.any(np.isnan(X_sequential_all)) or np.any(np.isinf(X_sequential_all)): print(\"CRITICAL WARNING: NaN/Inf in X_sequential_all.\")\n",
    "    if np.any(np.isnan(X_global_all)) or np.any(np.isinf(X_global_all)): print(\"CRITICAL WARNING: NaN/Inf in X_global_all.\")\n",
    "    if np.any(np.isnan(y_targets_all)) or np.any(np.isinf(y_targets_all)): print(\"CRITICAL WARNING: NaN/Inf in y_targets_all.\")\n",
    "    if len(y_targets_all) > 0 and np.all(np.abs(y_targets_all) <= 1e-6) : print(\"CRITICAL WARNING: All target total times are near zero.\")\n",
    "\n",
    "    target_scaler = StandardScaler()\n",
    "    y_targets_all_reshaped = y_targets_all.reshape(-1, 1)\n",
    "    global_feature_scaler = StandardScaler()\n",
    "    indices = np.arange(len(X_sequential_all))\n",
    "\n",
    "    if len(X_sequential_all) < 10: \n",
    "        print(\"Warning: Very few samples (<10), using all for training.\")\n",
    "        X_train_seq = X_sequential_all\n",
    "        X_train_glob_scaled = global_feature_scaler.fit_transform(X_global_all)\n",
    "        y_train_scaled = target_scaler.fit_transform(y_targets_all_reshaped)\n",
    "        validation_data_for_fit = None\n",
    "    else:\n",
    "        train_indices, val_indices = train_test_split(indices, test_size=val_split_ratio, random_state=42, shuffle=True)\n",
    "        X_train_seq = X_sequential_all[train_indices]; X_val_seq = X_sequential_all[val_indices]\n",
    "        X_train_glob = X_global_all[train_indices]; X_val_glob = X_global_all[val_indices]\n",
    "        X_train_glob_scaled = global_feature_scaler.fit_transform(X_train_glob) \n",
    "        X_val_glob_scaled = global_feature_scaler.transform(X_val_glob)     \n",
    "        y_train_orig_reshaped = y_targets_all_reshaped[train_indices]; y_val_orig_reshaped = y_targets_all_reshaped[val_indices]\n",
    "        y_train_scaled = target_scaler.fit_transform(y_train_orig_reshaped) \n",
    "        y_val_scaled = target_scaler.transform(y_val_orig_reshaped)         \n",
    "        validation_data_for_fit = ([X_val_seq, X_val_glob_scaled], y_val_scaled) \n",
    "        print(f\"\\nManually split data: {len(X_train_seq)} train, {len(X_val_seq)} validation samples.\")\n",
    "        print(f\"Training target stats (orig scale): Mean={np.mean(y_train_orig_reshaped):.2f}, Std={np.std(y_train_orig_reshaped):.2f}\")\n",
    "        print(f\"Validation target stats (orig scale): Mean={np.mean(y_val_orig_reshaped):.2f}, Std={np.std(y_val_orig_reshaped):.2f}\\n\")\n",
    "\n",
    "    callbacks_list = [\n",
    "        EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1), \n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=1e-7, verbose=1) \n",
    "    ]\n",
    "    \n",
    "    print(\"Starting LSTM model training (with scaled targets and global features)...\")\n",
    "    history = lstm_model.fit(\n",
    "        [X_train_seq, X_train_glob_scaled], y_train_scaled,          \n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        validation_data=validation_data_for_fit, \n",
    "        callbacks=callbacks_list, verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM training finished.\")\n",
    "    data_for_lstm['target_scaler'] = target_scaler\n",
    "    data_for_lstm['global_feature_scaler'] = global_feature_scaler \n",
    "    return lstm_model, history, data_for_lstm\n",
    "\n",
    "# %%\n",
    "def generate_refined_predictions_with_lstm(lstm_model, processed_data):\n",
    "    \"\"\"\n",
    "    Generate refined time predictions, including additional specified columns from the original input.\n",
    "    Output CSV changed to _v19.\n",
    "    \"\"\"\n",
    "    print(\"Generating refined predictions using LSTM's total time...\")\n",
    "    \n",
    "    # Define the columns from the original data to keep in the final output\n",
    "    COLUMNS_TO_KEEP = ['timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to', 'PatientID_from', 'PatientID_to',\n",
    "                       'patient_height', 'patient_weight', 'patient_age', 'patient_gender']\n",
    "\n",
    "    X_sequential_input_all = processed_data['X_sequential_input']\n",
    "    X_global_features_input_all_unscaled = processed_data['X_global_features_input']\n",
    "    \n",
    "    global_feature_scaler = processed_data['global_feature_scaler']\n",
    "    X_global_features_input_all_scaled = global_feature_scaler.transform(X_global_features_input_all_unscaled)\n",
    "\n",
    "    lstm_predicted_scaled_total_times = lstm_model.predict(\n",
    "        [X_sequential_input_all, X_global_features_input_all_scaled] \n",
    "    ) \n",
    "    \n",
    "    target_scaler = processed_data['target_scaler']\n",
    "    lstm_predicted_total_times_original_scale = target_scaler.inverse_transform(lstm_predicted_scaled_total_times)\n",
    "    lstm_predicted_total_times_original_scale = np.squeeze(lstm_predicted_total_times_original_scale)\n",
    "    lstm_predicted_total_times_original_scale = np.maximum(0, lstm_predicted_total_times_original_scale) \n",
    "\n",
    "    transformer_step_proportions = processed_data['transformer_proportions_padded'] \n",
    "    masks_for_calc = processed_data['masks_for_calc'] \n",
    "\n",
    "    _, lstm_refined_increments, lstm_refined_cumulative = calculate_times_from_proportions(\n",
    "        transformer_step_proportions,\n",
    "        lstm_predicted_total_times_original_scale, \n",
    "        masks_for_calc \n",
    "    )\n",
    "\n",
    "    lstm_refined_increments_np = lstm_refined_increments.numpy()\n",
    "    lstm_refined_cumulative_np = lstm_refined_cumulative.numpy()\n",
    "\n",
    "    results_list_df = []\n",
    "    original_dfs_from_processing = processed_data['original_dfs'] \n",
    "    gt_total_times_all = processed_data['y_lstm_target_total_times'] \n",
    "\n",
    "    for i, seq_id in enumerate(processed_data['sequences_ids']):\n",
    "        if i >= len(original_dfs_from_processing): continue\n",
    "        original_seq_df = original_dfs_from_processing[i]\n",
    "        seq_len = len(original_seq_df)\n",
    "        if seq_len == 0:\n",
    "            if original_seq_df.empty: results_list_df.append(original_seq_df) \n",
    "            continue\n",
    "        if i >= len(lstm_predicted_total_times_original_scale): continue\n",
    "\n",
    "        # --- Build the new, clean DataFrame ---\n",
    "        output_dict = {\n",
    "            'Sequence': seq_id, \n",
    "            'Step': np.arange(1, seq_len + 1),\n",
    "            'SourceID': original_seq_df['SourceID'].values,\n",
    "            'GroundTruth_Increment': processed_data['gt_increments_padded_original'][i, :seq_len],\n",
    "            'GroundTruth_Cumulative': processed_data['gt_cumulative_padded_original'][i, :seq_len],\n",
    "            'LSTM_Predicted_Increment': lstm_refined_increments_np[i, :seq_len],\n",
    "            'LSTM_Predicted_Cumulative': lstm_refined_cumulative_np[i, :seq_len],\n",
    "            'LSTM_Predicted_TotalTime': np.full(seq_len, np.nan, dtype=np.float32),\n",
    "            'TotalTime_Difference': np.full(seq_len, np.nan, dtype=np.float32) \n",
    "        }\n",
    "        \n",
    "        # Add the additional columns from the original dataframe\n",
    "        for col in COLUMNS_TO_KEEP:\n",
    "            if col in original_seq_df.columns:\n",
    "                output_dict[col] = original_seq_df[col].values\n",
    "        \n",
    "        predicted_total = lstm_predicted_total_times_original_scale[i]\n",
    "        gt_total = gt_total_times_all[i]\n",
    "        \n",
    "        output_dict['LSTM_Predicted_TotalTime'][-1] = predicted_total\n",
    "        output_dict['TotalTime_Difference'][-1] = gt_total - predicted_total \n",
    "\n",
    "        transformer_pred_increment = original_seq_df['Predicted_Increment'].fillna(0).values\n",
    "        transformer_pred_cumulative = original_seq_df['Predicted_Cumulative'].fillna(0).values\n",
    "        diff_transformer_inc = np.abs(output_dict['GroundTruth_Increment'] - transformer_pred_increment)\n",
    "        diff_lstm_inc = np.abs(output_dict['GroundTruth_Increment'] - output_dict['LSTM_Predicted_Increment'])\n",
    "        output_dict['Increment_Improvement_Pct'] = np.where(diff_transformer_inc > 1e-6, (diff_transformer_inc - diff_lstm_inc) / diff_transformer_inc * 100, 0 )\n",
    "        diff_transformer_cum = np.abs(output_dict['GroundTruth_Cumulative'] - transformer_pred_cumulative)\n",
    "        diff_lstm_cum = np.abs(output_dict['GroundTruth_Cumulative'] - output_dict['LSTM_Predicted_Cumulative'])\n",
    "        output_dict['Cumulative_Improvement_Pct'] = np.where(diff_transformer_cum > 1e-6, (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100, 0 )\n",
    "        \n",
    "        clean_seq_df = pd.DataFrame(output_dict)\n",
    "        results_list_df.append(clean_seq_df)\n",
    "\n",
    "    if not results_list_df: return pd.DataFrame()\n",
    "    final_results_df = pd.concat(results_list_df, ignore_index=True)\n",
    "    \n",
    "    output_filename = 'predictions_lstm_refined_total_time_v19.csv' # Changed filename\n",
    "    final_results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Combined and refined predictions saved to {output_filename}\")\n",
    "    \n",
    "    if not final_results_df.empty:\n",
    "        original_df_full = pd.concat(original_dfs_from_processing, ignore_index=True)\n",
    "        merged_for_summary = pd.merge(final_results_df, original_df_full[['Sequence', 'Step', 'Predicted_Increment', 'Predicted_Cumulative']], on=['Sequence', 'Step'])\n",
    "        if 'Predicted_Increment' in merged_for_summary.columns and 'LSTM_Predicted_Increment' in merged_for_summary.columns:\n",
    "            mae_transformer = np.mean(np.abs(merged_for_summary['GroundTruth_Increment'] - merged_for_summary['Predicted_Increment']))\n",
    "            mae_lstm = np.mean(np.abs(merged_for_summary['GroundTruth_Increment'] - merged_for_summary['LSTM_Predicted_Increment']))\n",
    "            print(f\"\\nTransformer MAE (Increments): {mae_transformer:.4f}, LSTM-Refined MAE (Increments): {mae_lstm:.4f}\")\n",
    "            if mae_transformer > 1e-6: print(f\"Improvement (Increments): {(mae_transformer - mae_lstm) / mae_transformer * 100:.2f}%\")\n",
    "        if 'Predicted_Cumulative' in merged_for_summary.columns and 'LSTM_Predicted_Cumulative' in merged_for_summary.columns:\n",
    "            mae_transformer_cum = np.mean(np.abs(merged_for_summary['GroundTruth_Cumulative'] - merged_for_summary['Predicted_Cumulative']))\n",
    "            mae_lstm_cum = np.mean(np.abs(merged_for_summary['GroundTruth_Cumulative'] - merged_for_summary['LSTM_Predicted_Cumulative']))\n",
    "            print(f\"Transformer MAE (Cumulative): {mae_transformer_cum:.4f}, LSTM-Refined MAE (Cumulative): {mae_lstm_cum:.4f}\")\n",
    "            if mae_transformer_cum > 1e-6: print(f\"Improvement (Cumulative): {(mae_transformer_cum - mae_lstm_cum) / mae_transformer_cum * 100:.2f}%\")\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data['y_lstm_target_total_times'] \n",
    "    if len(lstm_predicted_total_times_original_scale) == len(gt_total_times_for_lstm_training):\n",
    "        mae_total_time_lstm = np.mean(np.abs(gt_total_times_for_lstm_training - lstm_predicted_total_times_original_scale))\n",
    "        print(f\"\\nLSTM MAE for Total Time (vs Max GT Cumulative): {mae_total_time_lstm:.4f}\")\n",
    "\n",
    "    return final_results_df\n",
    "\n",
    "# %%\n",
    "def visualize_lstm_results(results_df, processed_data, lstm_model, training_history):\n",
    "    if results_df.empty: print(\"Results DataFrame is empty, skipping visualizations.\"); return\n",
    "    print(\"Generating visualizations for LSTM results...\")\n",
    "    plt.style.use('ggplot')\n",
    "    if training_history and hasattr(training_history, 'history'):\n",
    "        if 'loss' in training_history.history and 'val_loss' in training_history.history:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "            plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "            if 'lr' in training_history.history:\n",
    "                ax2 = plt.gca().twinx(); ax2.plot(training_history.history['lr'], label='Learning Rate', color='g', linestyle='--')\n",
    "                ax2.set_ylabel('Learning Rate'); ax2.legend(loc='upper center')\n",
    "            plt.title('LSTM Model Loss (Predicting Scaled Total Time)') \n",
    "            plt.xlabel('Epoch'); plt.ylabel('Mean Squared Error (Scaled Loss)'); plt.legend(loc='upper left'); plt.tight_layout()\n",
    "            plt.savefig('lstm_total_time_training_loss_v19.png'); print(\"Saved LSTM training loss plot.\"); plt.close() # v19\n",
    "    else: print(\"Warning: Training history not available or malformed.\")\n",
    "\n",
    "    sample_sequence_ids = results_df['Sequence'].unique()\n",
    "    if len(sample_sequence_ids) == 0 : print(\"No sequences in results_df for plotting.\"); return\n",
    "    sample_sequence_ids = sample_sequence_ids[:min(5, len(sample_sequence_ids))]\n",
    "    \n",
    "    original_df_full = pd.concat(processed_data['original_dfs'], ignore_index=True)\n",
    "    plot_df = pd.merge(results_df, original_df_full[['Sequence', 'Step', 'Predicted_Cumulative', 'Predicted_Increment']], on=['Sequence', 'Step'], how='left')\n",
    "    \n",
    "    if len(sample_sequence_ids) > 0:\n",
    "        num_plots = len(sample_sequence_ids); fig_height = max(8, 3 * num_plots) \n",
    "        plt.figure(figsize=(15, fig_height)) # Cumulative Plot\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = plot_df[plot_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Cumulative'], 'o-', label='GT Cumul.', ms=4)\n",
    "            if 'Predicted_Cumulative' in plot_df.columns and 'Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Cumulative'], 's--', label='Transformer Cumul.', ms=4)\n",
    "            if 'LSTM_Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Cumulative'], '^-.', label='LSTM-Refined Cumul.', ms=4)\n",
    "            lstm_total_time_for_seq = seq_data_plot['LSTM_Predicted_TotalTime'].dropna().unique()\n",
    "            if len(lstm_total_time_for_seq) == 1: plt.axhline(y=lstm_total_time_for_seq[0], color='purple', linestyle=':', label=f'LSTM Total Pred: {lstm_total_time_for_seq[0]:.2f}')\n",
    "            plt.title(f'Cumulative Times: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Cumulative Time'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_cumulative_time_comparison_v19.png'); print(\"Saved cumulative time comparison plot.\"); plt.close() # v19\n",
    "\n",
    "        plt.figure(figsize=(15, fig_height)) # Increment Plot\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = plot_df[plot_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Increment'], 'o-', label='GT Incr.', ms=4)\n",
    "            if 'Predicted_Increment' in plot_df.columns and 'Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Increment'], 's--', label='Transformer Incr.', ms=4)\n",
    "            if 'LSTM_Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Increment'], '^-.', label='LSTM-Refined Incr.', ms=4)\n",
    "            plt.title(f'Time Increments: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Time Increment'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_increment_comparison_v19.png'); print(\"Saved increment comparison plot.\"); plt.close() # v19\n",
    "\n",
    "    gt_total_times_for_lstm_training = processed_data.get('y_lstm_target_total_times', np.array([])) \n",
    "    if lstm_model is not None and 'X_sequential_input' in processed_data and 'X_global_features_input' in processed_data and 'target_scaler' in processed_data:\n",
    "        X_seq_tensor = tf.convert_to_tensor(processed_data['X_sequential_input'], dtype=tf.float32)\n",
    "        X_glob_unscaled = processed_data['X_global_features_input']\n",
    "        X_glob_scaled_for_plot = processed_data['global_feature_scaler'].transform(X_glob_unscaled)\n",
    "        X_glob_tensor = tf.convert_to_tensor(X_glob_scaled_for_plot, dtype=tf.float32)\n",
    "        \n",
    "        lstm_pred_scaled_total_t = lstm_model.predict([X_seq_tensor, X_glob_tensor])\n",
    "        lstm_pred_original_scale_total_t = processed_data['target_scaler'].inverse_transform(lstm_pred_scaled_total_t).squeeze()\n",
    "        \n",
    "        if lstm_pred_original_scale_total_t.ndim == 0: lstm_pred_original_scale_total_t = np.array([lstm_pred_original_scale_total_t])\n",
    "            \n",
    "        if gt_total_times_for_lstm_training.size > 0 and lstm_pred_original_scale_total_t.size > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1); \n",
    "            plt.hist(gt_total_times_for_lstm_training, bins=30, alpha=0.7, label='GT Total Times (Max Cumul.)')\n",
    "            plt.hist(lstm_pred_original_scale_total_t, bins=30, alpha=0.7, label='LSTM Pred Total Times (Original Scale)')\n",
    "            plt.xlabel('Total Time'); plt.ylabel('Frequency'); plt.title('Distribution of Total Times'); plt.legend()\n",
    "            plt.subplot(1, 2, 2); \n",
    "            if len(gt_total_times_for_lstm_training) == len(lstm_pred_original_scale_total_t):\n",
    "                errors_total_time = gt_total_times_for_lstm_training - lstm_pred_original_scale_total_t\n",
    "                plt.hist(errors_total_time, bins=30, alpha=0.7, color='red')\n",
    "                plt.xlabel('Prediction Error (GT Max Cumul. - Pred)'); plt.ylabel('Frequency'); plt.title('LSTM Total Time Prediction Errors')\n",
    "                if errors_total_time.size > 0: \n",
    "                    mean_error_val = errors_total_time.mean(); plt.axvline(mean_error_val, color='k', ls='--', lw=1, label=f'Mean Error: {mean_error_val:.2f}')\n",
    "                plt.legend()\n",
    "            else: print(\"Warning: Mismatch length GT total times and predictions for error histogram.\")\n",
    "            plt.tight_layout(); plt.savefig('lstm_total_time_prediction_analysis_v19.png'); print(\"Saved total time prediction analysis plot.\"); plt.close() # v19\n",
    "        else: print(\"Warning: Not enough data for total time distribution plots.\")\n",
    "    else: print(\"Warning: LSTM model, input data, or scaler missing for total time prediction plot.\")\n",
    "    print(\"Visualizations for LSTM completed!\")\n",
    "\n",
    "# %%\n",
    "def main_lstm_total_time_flow():\n",
    "    try:\n",
    "        transformer_predictions_file = \"PXChange/data/175651/encoded_175651_condensed_with_dummy_data.csv\" \n",
    "        if not os.path.exists(transformer_predictions_file):\n",
    "            print(f\"Error: Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "            print(\"Creating DUMMY CSV for testing flow.\")\n",
    "            dummy_data = []\n",
    "            for seq_idx in range(300): \n",
    "                num_steps = np.random.randint(10, 60) \n",
    "                steps = np.arange(1, num_steps + 1)\n",
    "                gt_increments = np.random.lognormal(mean=2.0, sigma=0.7, size=num_steps) + 0.1 \n",
    "                gt_increments = np.maximum(gt_increments, 0.01) \n",
    "                gt_cumulative = np.cumsum(gt_increments)\n",
    "                \n",
    "                raw_props = np.random.rand(num_steps) + 0.05 \n",
    "                pred_proportions = raw_props / raw_props.sum() \n",
    "                \n",
    "                actual_sequence_total_time = gt_cumulative[-1] if num_steps > 0 else 1.0\n",
    "                actual_sequence_total_time = max(actual_sequence_total_time, 1.0) \n",
    "\n",
    "                dummy_transformer_effective_total_time = actual_sequence_total_time * np.random.normal(loc=1.0, scale=0.4) \n",
    "                dummy_transformer_effective_total_time = max(dummy_transformer_effective_total_time, 0.1)\n",
    "\n",
    "                pred_increments_from_transformer = pred_proportions * dummy_transformer_effective_total_time\n",
    "                pred_cumulative_from_transformer = np.cumsum(pred_increments_from_transformer)\n",
    "\n",
    "                for s_idx in range(num_steps):\n",
    "                    dummy_data.append({\n",
    "                        'Sequence': seq_idx, 'Step': steps[s_idx], 'SourceID': f'MRI_DUMMY_{s_idx%5 +1}',\n",
    "                        'Predicted_Proportion': pred_proportions[s_idx], \n",
    "                        'Predicted_Increment': pred_increments_from_transformer[s_idx],\n",
    "                        'Predicted_Cumulative': pred_cumulative_from_transformer[s_idx], \n",
    "                        'GroundTruth_Increment': gt_increments[s_idx], \n",
    "                        'GroundTruth_Cumulative': gt_cumulative[s_idx],\n",
    "                        'timediff': np.random.randint(100, 1000), 'PTAB': np.random.randint(-2000000, -100000),\n",
    "                        'BodyGroup_from': 'CHEST', 'BodyGroup_to': 'ABDOMEN',\n",
    "                        'PatientID_from': f'PAT_{seq_idx}', 'PatientID_to': f'PAT_{seq_idx}',\n",
    "                        'patient_height': np.random.randint(150, 200), 'patient_weight': np.random.randint(50, 100),\n",
    "                        'patient_age': np.random.randint(20, 80), 'patient_gender': np.random.randint(0, 2)\n",
    "                    })\n",
    "            if not dummy_data: \n",
    "                 dummy_data.append({ 'Sequence': 0, 'Step': 1, 'SourceID': 'MRI_DUMMY_0', 'Predicted_Proportion': 1.0, \n",
    "                                     'Predicted_Increment': 10.0, 'Predicted_Cumulative': 10.0, \n",
    "                                     'GroundTruth_Increment': 10.0, 'GroundTruth_Cumulative': 10.0,\n",
    "                                     'timediff': 500, 'PTAB': -1000000, 'BodyGroup_from': 'CHEST', 'BodyGroup_to': 'ABDOMEN',\n",
    "                                     'PatientID_from': 'PAT_0', 'PatientID_to': 'PAT_0',\n",
    "                                     'patient_height': 175, 'patient_weight': 75, 'patient_age': 50, 'patient_gender': 1})\n",
    "            dummy_df = pd.DataFrame(dummy_data)\n",
    "            dummy_df.to_csv(transformer_predictions_file, index=False)\n",
    "            print(f\"Dummy '{transformer_predictions_file}' created with {len(dummy_df)} rows and {len(dummy_df['Sequence'].unique())} sequences.\")\n",
    "        \n",
    "        lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
    "            transformer_predictions_file, epochs=200, batch_size=32 ) \n",
    "        \n",
    "        if lstm_model is None or processed_lstm_data is None:\n",
    "            print(\"LSTM training failed or returned None. Exiting.\"); return\n",
    "\n",
    "        refined_results_df = generate_refined_predictions_with_lstm(lstm_model, processed_lstm_data)\n",
    "        if not refined_results_df.empty:\n",
    "            print(\"\\nSample of Refined Predictions (LSTM Total Time Approach - v19):\")\n",
    "            display_cols = [ 'Sequence', 'Step', 'SourceID', 'timediff', 'PTAB', 'BodyGroup_from', 'PatientID_from',\n",
    "                             'LSTM_Predicted_Increment', 'GroundTruth_Increment', \n",
    "                             'LSTM_Predicted_Cumulative', 'GroundTruth_Cumulative',\n",
    "                             'LSTM_Predicted_TotalTime', 'TotalTime_Difference']\n",
    "            actual_display_cols = [col for col in display_cols if col in refined_results_df.columns]\n",
    "            print(refined_results_df[actual_display_cols].head(20))\n",
    "            print(\"\\nGenerating visualizations for LSTM (total time approach - v19)...\")\n",
    "            visualize_lstm_results(refined_results_df, processed_lstm_data, lstm_model, lstm_history)\n",
    "        else: print(\"No refined predictions were generated by the LSTM flow.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM (total time) main function: {e}\"); import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for LSTM training...\n",
      "Processing data from: predictions_transformer_202513_with_details.csv\n",
      "\n",
      "Statistics for TARGET y_total_times_list (max GT_Cumulative per seq, 105 sequences):\n",
      "  Mean: 874.6571, Std Dev: 735.6474\n",
      "  Min: 1.0000, Max: 4420.0000\n",
      "  Number of zeros (<=1e-6): 0\n",
      "\n",
      "Num sequential features: 2, Max seq length: 56\n",
      "Num global features: 4\n",
      "\n",
      "Enhanced LSTM Model Summary (v18 - after sample call):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"total_time_lstm_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"total_time_lstm_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_lstm_v18          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">530,432</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_V (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate_features_v18        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                   │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_v18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_lstm_v18          │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m530,432\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W1 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_W2 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention_dense_V (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │           \u001b[38;5;34m513\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate_features_v18        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m516\u001b[0m)               │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)                   │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1_v18 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │        \u001b[38;5;34m66,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1_v18 (\u001b[38;5;33mDropout\u001b[0m)         │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2_v18 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2_v18 (\u001b[38;5;33mDropout\u001b[0m)         │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_v18 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,130,754</span> (4.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,130,754\u001b[0m (4.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,130,754</span> (4.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,130,754\u001b[0m (4.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manually split data: 84 train, 21 validation samples.\n",
      "Training target stats (orig scale): Mean=859.21, Std=770.96\n",
      "Validation target stats (orig scale): Mean=936.43, Std=568.84\n",
      "\n",
      "Starting LSTM model training (with scaled targets and global features)...\n",
      "Epoch 1/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 318ms/step - loss: 1.2881 - val_loss: 0.8600 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.4798 - val_loss: 0.8544 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.1131 - val_loss: 0.8536 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.0800 - val_loss: 0.8873 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 1.1602 - val_loss: 0.8761 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 1.1956 - val_loss: 0.8348 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.9415 - val_loss: 0.8301 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 1.1732 - val_loss: 0.8274 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.1323 - val_loss: 0.8463 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.9782 - val_loss: 0.8613 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.9760 - val_loss: 0.8514 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.1346 - val_loss: 0.8203 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.0516 - val_loss: 0.8025 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - loss: 1.2773 - val_loss: 0.7979 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.9557 - val_loss: 0.8108 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 1.0251 - val_loss: 0.8059 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 1.0424 - val_loss: 0.7825 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 1.2105 - val_loss: 0.8131 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.9620 - val_loss: 0.8468 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.0462 - val_loss: 0.8510 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 1.0134 - val_loss: 0.8418 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1.1312 - val_loss: 0.7992 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 1.1506 - val_loss: 0.7552 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 1.1108 - val_loss: 0.7499 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.9693 - val_loss: 0.7476 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.9297 - val_loss: 0.7545 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.8678 - val_loss: 0.8002 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.9790 - val_loss: 0.8616 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 0.9091 - val_loss: 0.8478 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.9683 - val_loss: 0.8162 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.9994 - val_loss: 0.7914 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.9061 - val_loss: 0.7740 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.9334 - val_loss: 0.7589 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.1554 - val_loss: 0.7581 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.1420 - val_loss: 0.7613 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.9615 - val_loss: 0.7418 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.9976 - val_loss: 0.7344 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 1.0712 - val_loss: 0.7379 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.8863 - val_loss: 0.7414 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.9669 - val_loss: 0.7412 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.0659 - val_loss: 0.7130 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8274 - val_loss: 0.7743 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.8594 - val_loss: 0.8094 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.8391 - val_loss: 0.8060 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.8625 - val_loss: 0.7733 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 1.0870 - val_loss: 0.7434 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.7860 - val_loss: 0.7322 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8921 - val_loss: 0.7477 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.8783 - val_loss: 0.7504 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.8542 - val_loss: 0.7597 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.0678 - val_loss: 0.7382 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.0462 - val_loss: 0.7201 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.9339 - val_loss: 0.7361 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1.0196 - val_loss: 0.8114 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.9741 - val_loss: 0.7710 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.9042\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.9036 - val_loss: 0.7259 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.8689 - val_loss: 0.7210 - learning_rate: 2.0000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.7825 - val_loss: 0.7202 - learning_rate: 2.0000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.9124 - val_loss: 0.7206 - learning_rate: 2.0000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.8837 - val_loss: 0.7244 - learning_rate: 2.0000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 1.0161 - val_loss: 0.7296 - learning_rate: 2.0000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.9428 - val_loss: 0.7342 - learning_rate: 2.0000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 0.9115 - val_loss: 0.7410 - learning_rate: 2.0000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 0.9711 - val_loss: 0.7456 - learning_rate: 2.0000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 1.1155 - val_loss: 0.7501 - learning_rate: 2.0000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.9354 - val_loss: 0.7579 - learning_rate: 2.0000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8865 - val_loss: 0.7586 - learning_rate: 2.0000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.8679 - val_loss: 0.7576 - learning_rate: 2.0000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.8359 - val_loss: 0.7470 - learning_rate: 2.0000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.8267 - val_loss: 0.7344 - learning_rate: 2.0000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.1527\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1.0980 - val_loss: 0.7212 - learning_rate: 2.0000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.0449 - val_loss: 0.7209 - learning_rate: 4.0000e-05\n",
      "Epoch 73/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 1.0162 - val_loss: 0.7207 - learning_rate: 4.0000e-05\n",
      "Epoch 74/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.9266 - val_loss: 0.7202 - learning_rate: 4.0000e-05\n",
      "Epoch 75/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.7562 - val_loss: 0.7190 - learning_rate: 4.0000e-05\n",
      "Epoch 76/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 1.0086 - val_loss: 0.7177 - learning_rate: 4.0000e-05\n",
      "Epoch 77/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.5936 - val_loss: 0.7166 - learning_rate: 4.0000e-05\n",
      "Epoch 78/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 0.8488 - val_loss: 0.7152 - learning_rate: 4.0000e-05\n",
      "Epoch 79/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.8991 - val_loss: 0.7141 - learning_rate: 4.0000e-05\n",
      "Epoch 80/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.9084 - val_loss: 0.7136 - learning_rate: 4.0000e-05\n",
      "Epoch 81/200\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 0.8364 - val_loss: 0.7144 - learning_rate: 4.0000e-05\n",
      "Epoch 81: early stopping\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "LSTM training finished.\n",
      "Generating refined predictions using LSTM's total time...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step\n",
      "Combined and refined predictions saved to predictions_lstm_refined_total_time_v18.csv\n",
      "\n",
      "Transformer MAE (Increments): 96.5457, LSTM-Refined MAE (Increments): 98.8926\n",
      "Improvement (Increments): -2.43%\n",
      "Transformer MAE (Cumulative): 206.8695, LSTM-Refined MAE (Cumulative): 349.0782\n",
      "Improvement (Cumulative): -68.74%\n",
      "\n",
      "LSTM MAE for Total Time (vs Max GT Cumulative): 483.6174\n",
      "\n",
      "Sample of Refined Predictions (LSTM Total Time Approach - v18):\n",
      "    Sequence  Step      SourceID  timediff     PTAB BodyGroup_from  \\\n",
      "0          0     1   MRI_MSR_104       0.0  -675700           KNEE   \n",
      "1          0     2     MRI_FRR_2      25.0  -675700           KNEE   \n",
      "2          0     3   MRI_FRR_257      30.0 -1403150           KNEE   \n",
      "3          0     4   MRI_FRR_264      38.0 -1403150           KNEE   \n",
      "4          0     5   MRI_FRR_264      41.0 -1403150           KNEE   \n",
      "5          0     6   MRI_FRR_264      46.0 -1403150           KNEE   \n",
      "6          0     7   MRI_FRR_264      58.0 -1403150           KNEE   \n",
      "7          0     8    MRI_CCS_11     104.0 -1403150           KNEE   \n",
      "8          0     9    MRI_CCS_11     105.0 -1403150           KNEE   \n",
      "9          0    10    MRI_CCS_11     183.0 -1403150           KNEE   \n",
      "10         0    11    MRI_CCS_11     184.0 -1403150           KNEE   \n",
      "11         0    12   MRI_FRR_257    1065.0     2350           KNEE   \n",
      "12         0    13   MRI_FRR_264    1066.0     2350           KNEE   \n",
      "13         0    14   MRI_FRR_264    1079.0     2350           KNEE   \n",
      "14         0    15    MRI_CCS_11    1106.0     2350           KNEE   \n",
      "15         0    16    MRI_CCS_11    1107.0     2350           KNEE   \n",
      "16         0    17    MRI_CCS_11    1111.0     2350           KNEE   \n",
      "17         0    18    MRI_CCS_11    1116.0     2350           KNEE   \n",
      "18         0    19  MRI_MPT_1005    1191.0     2350           KNEE   \n",
      "19         0    20    MRI_CCS_11    1217.0     2350           KNEE   \n",
      "\n",
      "                              PatientID_from  LSTM_Predicted_Increment  \\\n",
      "0   0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "1   0f95b3be187af4b017f05b3e4be07a6756732f58                129.754288   \n",
      "2   0f95b3be187af4b017f05b3e4be07a6756732f58                 35.387535   \n",
      "3   0f95b3be187af4b017f05b3e4be07a6756732f58                 35.387535   \n",
      "4   0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "5   0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "6   0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "7   0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "8   0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "9   0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "10  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "11  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "12  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "13  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "14  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "15  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "16  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "17  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "18  0f95b3be187af4b017f05b3e4be07a6756732f58                436.446228   \n",
      "19  0f95b3be187af4b017f05b3e4be07a6756732f58                 11.795846   \n",
      "\n",
      "    GroundTruth_Increment  LSTM_Predicted_Cumulative  GroundTruth_Cumulative  \\\n",
      "0                    25.0                  11.795846                    25.0   \n",
      "1                     5.0                 141.550140                    30.0   \n",
      "2                     8.0                 176.937683                    38.0   \n",
      "3                     3.0                 212.325226                    41.0   \n",
      "4                     5.0                 224.121078                    46.0   \n",
      "5                    12.0                 235.916931                    58.0   \n",
      "6                    46.0                 247.712784                   104.0   \n",
      "7                     1.0                 259.508636                   105.0   \n",
      "8                    78.0                 271.304474                   183.0   \n",
      "9                     1.0                 283.100311                   184.0   \n",
      "10                  881.0                 294.896149                  1065.0   \n",
      "11                    1.0                 306.691986                  1066.0   \n",
      "12                   13.0                 318.487823                  1079.0   \n",
      "13                   27.0                 330.283661                  1106.0   \n",
      "14                    1.0                 342.079498                  1107.0   \n",
      "15                    4.0                 353.875336                  1111.0   \n",
      "16                    5.0                 365.671173                  1116.0   \n",
      "17                   75.0                 377.467010                  1191.0   \n",
      "18                   26.0                 813.913208                  1217.0   \n",
      "19                   33.0                 825.709045                  1250.0   \n",
      "\n",
      "    LSTM_Predicted_TotalTime  TotalTime_Difference  \n",
      "0                        NaN                   NaN  \n",
      "1                        NaN                   NaN  \n",
      "2                        NaN                   NaN  \n",
      "3                        NaN                   NaN  \n",
      "4                        NaN                   NaN  \n",
      "5                        NaN                   NaN  \n",
      "6                        NaN                   NaN  \n",
      "7                        NaN                   NaN  \n",
      "8                        NaN                   NaN  \n",
      "9                        NaN                   NaN  \n",
      "10                       NaN                   NaN  \n",
      "11                       NaN                   NaN  \n",
      "12                       NaN                   NaN  \n",
      "13                       NaN                   NaN  \n",
      "14                       NaN                   NaN  \n",
      "15                       NaN                   NaN  \n",
      "16                       NaN                   NaN  \n",
      "17                       NaN                   NaN  \n",
      "18                       NaN                   NaN  \n",
      "19                       NaN                   NaN  \n",
      "\n",
      "Generating visualizations for LSTM (total time approach - v18)...\n",
      "Generating visualizations for LSTM results...\n",
      "Saved LSTM training loss plot.\n",
      "Saved cumulative time comparison plot.\n",
      "Saved increment comparison plot.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Saved total time prediction analysis plot.\n",
      "Visualizations for LSTM completed!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main_lstm_total_time_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
