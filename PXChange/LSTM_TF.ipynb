{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Assuming pad_sequences is available if needed, though direct use might change.\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Seaborn was commented out in original, keeping it that way\n",
    "#import test train split sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute increments and cumulative times from proportions and total time\n",
    "def calculate_times_from_proportions(proportions_per_step, total_time_for_sequence, mask_per_step):\n",
    "    \"\"\"\n",
    "    Calculates time increments and cumulative times from step-wise proportions and a total time.\n",
    "\n",
    "    Args:\n",
    "        proportions_per_step (tf.Tensor or np.ndarray): \n",
    "            Proportions for each step in sequences (batch_size, seq_len).\n",
    "        total_time_for_sequence (tf.Tensor or np.ndarray): \n",
    "            The total time for each sequence (batch_size, 1) or (batch_size,).\n",
    "        mask_per_step (tf.Tensor or np.ndarray): \n",
    "            Mask indicating valid steps (batch_size, seq_len).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (normalized_proportions, increments, cumulative_times)\n",
    "               all as tf.Tensor.\n",
    "    \"\"\"\n",
    "    proportions_tf = tf.cast(proportions_per_step, tf.float32)\n",
    "    total_time_tf = tf.cast(total_time_for_sequence, tf.float32)\n",
    "    mask_tf = tf.cast(mask_per_step, tf.float32)\n",
    "\n",
    "    # Ensure total_time_tf is (batch_size, 1) for broadcasting\n",
    "    if len(tf.shape(total_time_tf)) == 1:\n",
    "        total_time_tf = tf.expand_dims(total_time_tf, axis=-1)\n",
    "\n",
    "    # Apply mask to proportions\n",
    "    masked_proportions = proportions_tf * mask_tf\n",
    "\n",
    "    # Normalize proportions over valid steps\n",
    "    row_sums = tf.reduce_sum(masked_proportions, axis=1, keepdims=True)\n",
    "    # Prevent division by zero if a sequence is all padding or has zero proportions\n",
    "    row_sums = tf.where(tf.equal(row_sums, 0), tf.ones_like(row_sums), row_sums) \n",
    "    normalized_proportions = masked_proportions / row_sums\n",
    "\n",
    "    # Calculate increments\n",
    "    increments = normalized_proportions * total_time_tf # Broadcasting\n",
    "\n",
    "    # Calculate cumulative times\n",
    "    cumulative_times = tf.cumsum(increments, axis=1)\n",
    "    \n",
    "    # Ensure masked steps in final outputs are zero\n",
    "    increments *= mask_tf\n",
    "    cumulative_times *= mask_tf\n",
    "    normalized_proportions *= mask_tf\n",
    "\n",
    "    return normalized_proportions, increments, cumulative_times\n",
    "\n",
    "# %%\n",
    "class TotalTimeLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    LSTM model to predict only the total time of a sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=64, num_heads=4, dropout_rate=0.2):\n",
    "        super(TotalTimeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.lstm_layer = layers.LSTM(self.hidden_units, \n",
    "                                      return_sequences=True, \n",
    "                                      dropout=dropout_rate,\n",
    "                                      recurrent_dropout=dropout_rate,\n",
    "                                      name=\"lstm_1\")\n",
    "        \n",
    "        self.bi_lstm = layers.Bidirectional(\n",
    "            layers.LSTM(self.hidden_units, return_sequences=True, name=\"lstm_bidirectional_inner\"),\n",
    "            name=\"bidirectional_lstm_1\"\n",
    "        )\n",
    "        \n",
    "        mha_key_dim = (2 * self.hidden_units) // self.num_heads\n",
    "        if (2 * self.hidden_units) % self.num_heads != 0:\n",
    "            raise ValueError(f\"(2 * hidden_units) must be divisible by num_heads. \"\n",
    "                             f\"Got 2 * {self.hidden_units} and {self.num_heads}.\")\n",
    "\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads, key_dim=mha_key_dim, name=\"multi_head_attention_1\"\n",
    "        )\n",
    "        self.layer_norm_attn = layers.LayerNormalization(name=\"layer_norm_attention\")\n",
    "        self.global_avg_pool = layers.GlobalAveragePooling1D(name=\"global_avg_pooling_1d\")\n",
    "        self.total_time_head = layers.Dense(1, activation='linear', name=\"total_time_dense_output\") \n",
    "        \n",
    "    def call(self, inputs, training=False): \n",
    "        # Create a boolean mask from inputs. True for non-padded (valid), False for padded.\n",
    "        mask_bool = tf.reduce_any(tf.not_equal(inputs, 0.0), axis=-1)\n",
    "\n",
    "        lstm_out = self.lstm_layer(inputs, mask=mask_bool, training=training)\n",
    "        bi_lstm_out = self.bi_lstm(lstm_out, mask=mask_bool, training=training)\n",
    "        \n",
    "        # Attention mask for MHA: (batch_size, 1, 1, key_seq_len)\n",
    "        # True for allowed tokens, False for masked tokens.\n",
    "        mha_attention_mask = mask_bool[:, tf.newaxis, tf.newaxis, :]\n",
    "        attn_output = self.attention(query=bi_lstm_out, value=bi_lstm_out, key=bi_lstm_out, \n",
    "                                     attention_mask=mha_attention_mask, \n",
    "                                     training=training)\n",
    "        \n",
    "        x = self.layer_norm_attn(attn_output + bi_lstm_out) # Residual connection\n",
    "        sequence_encoding = self.global_avg_pool(x, mask=mask_bool)\n",
    "        total_time_pred = self.total_time_head(sequence_encoding)\n",
    "        # Optionally, apply ReLU here if total time must be non-negative and model struggles\n",
    "        # total_time_pred = tf.nn.relu(total_time_pred) \n",
    "        return total_time_pred\n",
    "\n",
    "# %%\n",
    "def process_input_data_for_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process the transformer predictions CSV to prepare data for LSTM training.\n",
    "    Target total time is now the max GroundTruth_Cumulative for each sequence.\n",
    "    \"\"\"\n",
    "    print(f\"Processing data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "    df = pd.read_csv(transformer_predictions_file)\n",
    "    \n",
    "    required_cols = ['Predicted_Proportion', 'GroundTruth_Cumulative', 'GroundTruth_Increment', 'Sequence', 'Step']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"CSV must contain '{col}' column.\")\n",
    "\n",
    "    sequences = df['Sequence'].unique()\n",
    "    \n",
    "    X_data_list = []\n",
    "    y_total_times_list = []\n",
    "    transformer_proportions_list = [] \n",
    "    ground_truth_increments_list = []\n",
    "    ground_truth_cumulative_list = [] # Store the full GT cumulative for reference\n",
    "    original_dfs_list = [] \n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_df = df[df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        if seq_df.empty:\n",
    "            original_dfs_list.append(seq_df) \n",
    "            continue\n",
    "        original_dfs_list.append(seq_df) \n",
    "\n",
    "        current_max_steps = seq_df['Step'].max()\n",
    "        if current_max_steps == 0: current_max_steps = 1 \n",
    "        \n",
    "        features = np.column_stack([\n",
    "            seq_df['Predicted_Proportion'].values,\n",
    "            seq_df['Step'].values / current_max_steps \n",
    "        ])\n",
    "        X_data_list.append(features)\n",
    "        \n",
    "        gt_cumulative_for_seq = seq_df['GroundTruth_Cumulative'].values\n",
    "        # **CRITICAL CHANGE HERE**: Use max cumulative time as the target total time\n",
    "        total_time_for_seq = np.max(gt_cumulative_for_seq) if len(gt_cumulative_for_seq) > 0 else 0.0\n",
    "        \n",
    "        y_total_times_list.append(total_time_for_seq)\n",
    "        \n",
    "        transformer_proportions_list.append(seq_df['Predicted_Proportion'].values)\n",
    "        ground_truth_increments_list.append(seq_df['GroundTruth_Increment'].values)\n",
    "        ground_truth_cumulative_list.append(gt_cumulative_for_seq) # Store original full cumulative path\n",
    "\n",
    "    if not X_data_list:\n",
    "        raise ValueError(\"No valid sequences processed. Check CSV content or processing logic.\")\n",
    "\n",
    "    y_total_times_array_unpadded = np.array(y_total_times_list)\n",
    "    print(f\"\\nStatistics for TARGET y_total_times_list (max GT_Cumulative per seq, {len(y_total_times_array_unpadded)} sequences):\")\n",
    "    print(f\"  Mean: {np.mean(y_total_times_array_unpadded):.4f}, Std Dev: {np.std(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Min: {np.min(y_total_times_array_unpadded):.4f}, Max: {np.max(y_total_times_array_unpadded):.4f}\")\n",
    "    print(f\"  Number of zeros (<=1e-6): {np.sum(y_total_times_array_unpadded <= 1e-6)}\")\n",
    "    print(f\"  Number non-positive (<=0): {np.sum(y_total_times_array_unpadded <= 0)}\\n\")\n",
    "\n",
    "\n",
    "    max_length = max(len(x) for x in X_data_list)\n",
    "    num_features = X_data_list[0].shape[1]\n",
    "\n",
    "    X_padded = np.zeros((len(X_data_list), max_length, num_features), dtype=np.float32)\n",
    "    masks_padded_float = np.zeros((len(X_data_list), max_length), dtype=np.float32) \n",
    "    transformer_proportions_padded = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "    # Storing the original GT increments and *full* GT cumulative for final CSV comparison\n",
    "    gt_increments_padded_original = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "    gt_cumulative_padded_original = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(X_data_list)):\n",
    "        seq_len = len(X_data_list[i])\n",
    "        if seq_len > 0:\n",
    "            X_padded[i, :seq_len, :] = X_data_list[i]\n",
    "            masks_padded_float[i, :seq_len] = 1.0 \n",
    "            transformer_proportions_padded[i, :seq_len] = transformer_proportions_list[i]\n",
    "            gt_increments_padded_original[i, :seq_len] = ground_truth_increments_list[i]\n",
    "            gt_cumulative_padded_original[i, :seq_len] = ground_truth_cumulative_list[i] # Store full path\n",
    "        \n",
    "    y_total_times_np = np.array(y_total_times_list, dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        'X_lstm_input': X_padded,\n",
    "        'y_lstm_target_total_times': y_total_times_np, # This is now max GT_Cumulative\n",
    "        'masks_for_calc': masks_padded_float, \n",
    "        'sequences_ids': sequences, \n",
    "        'original_dfs': original_dfs_list, \n",
    "        'transformer_proportions_padded': transformer_proportions_padded, \n",
    "        'gt_increments_padded_original': gt_increments_padded_original, # Original GT increments\n",
    "        'gt_cumulative_padded_original': gt_cumulative_padded_original, # Original GT cumulative path\n",
    "        'max_len': max_length,\n",
    "        'num_features': num_features\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def train_total_time_lstm(transformer_predictions_file, epochs=50, batch_size=32, val_split_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Train LSTM model to predict total_time with manual validation split and checks.\n",
    "    \"\"\"\n",
    "    print(\"Processing data for LSTM training...\")\n",
    "    data_for_lstm = process_input_data_for_lstm(transformer_predictions_file)\n",
    "    \n",
    "    print(f\"Number of features for LSTM input: {data_for_lstm['num_features']}\")\n",
    "    print(f\"Max sequence length for LSTM input: {data_for_lstm['max_len']}\")\n",
    "\n",
    "    lstm_model = TotalTimeLSTM(hidden_units=64, num_heads=4, dropout_rate=0.2) \n",
    "    \n",
    "    input_shape = (None, data_for_lstm['max_len'], data_for_lstm['num_features'])\n",
    "    # Build the model by making a call with some data, or using .build()\n",
    "    # Using .build() is cleaner here.\n",
    "    lstm_model.build(input_shape=input_shape) \n",
    "    \n",
    "    print(\"\\nLSTM Model Summary:\")\n",
    "    lstm_model.summary() \n",
    "\n",
    "    lstm_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001), \n",
    "        loss='mse' \n",
    "    )\n",
    "    \n",
    "    y_targets_all = data_for_lstm['y_lstm_target_total_times']\n",
    "    X_inputs_all = data_for_lstm['X_lstm_input']\n",
    "    \n",
    "    # Basic checks for NaN/Inf in data\n",
    "    if np.any(np.isnan(X_inputs_all)) or np.any(np.isinf(X_inputs_all)):\n",
    "        print(\"CRITICAL WARNING: NaN or Inf found in X_inputs_all. Training may fail or be unstable.\")\n",
    "    if np.any(np.isnan(y_targets_all)) or np.any(np.isinf(y_targets_all)):\n",
    "        print(\"CRITICAL WARNING: NaN or Inf found in y_targets_all. Training may fail or be unstable.\")\n",
    "    if len(y_targets_all) > 0 and np.all(np.abs(y_targets_all) <= 1e-6) : \n",
    "        print(\"CRITICAL WARNING: All target total times are near zero. Model will likely predict zero and not learn effectively.\")\n",
    "\n",
    "\n",
    "    if len(X_inputs_all) < 5: \n",
    "        print(\"Warning: Very few samples (<5), using all for training and no validation during fit.\")\n",
    "        X_train, y_train = X_inputs_all, y_targets_all\n",
    "        validation_data_for_fit = None\n",
    "    else:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_inputs_all, y_targets_all, test_size=val_split_ratio, random_state=42, shuffle=True\n",
    "        )\n",
    "        validation_data_for_fit = (X_val, y_val)\n",
    "        print(f\"\\nManually split data: {len(X_train)} train, {len(X_val)} validation samples.\")\n",
    "        print(\"Training target statistics (y_train):\")\n",
    "        print(f\"  Mean: {np.mean(y_train):.4f}, Std: {np.std(y_train):.4f}, Min: {np.min(y_train):.4f}, Max: {np.max(y_train):.4f}\")\n",
    "        print(f\"  Number of zeros (<=1e-6): {np.sum(np.abs(y_train) <= 1e-6)}\")\n",
    "        if len(y_val) > 0:\n",
    "            print(\"Validation target statistics (y_val):\")\n",
    "            print(f\"  Mean: {np.mean(y_val):.4f}, Std: {np.std(y_val):.4f}, Min: {np.min(y_val):.4f}, Max: {np.max(y_val):.4f}\")\n",
    "            print(f\"  Number of zeros (<=1e-6): {np.sum(np.abs(y_val) <= 1e-6)}\\n\")\n",
    "            if np.all(np.abs(y_val) <= 1e-6): \n",
    "                print(\"CRITICAL WARNING: All validation targets (y_val) are effectively zero. val_loss will likely be zero.\")\n",
    "        else:\n",
    "            print(\"No validation samples after split.\\n\")\n",
    "\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20, # Increased patience further\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Starting LSTM model training...\")\n",
    "    \n",
    "    history = lstm_model.fit(\n",
    "        X_train, \n",
    "        y_train,          \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data_for_fit, \n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM training finished.\")\n",
    "    return lstm_model, history, data_for_lstm\n",
    "\n",
    "# %%\n",
    "def generate_refined_predictions_with_lstm(lstm_model, processed_data):\n",
    "    \"\"\"\n",
    "    Generate refined time predictions using LSTM's total_time.\n",
    "    Output CSV changed to _v4.\n",
    "    \"\"\"\n",
    "    print(\"Generating refined predictions using LSTM's total time...\")\n",
    "    \n",
    "    X_input_for_prediction = processed_data['X_lstm_input']\n",
    "    lstm_predicted_total_times = lstm_model.predict(X_input_for_prediction) \n",
    "    lstm_predicted_total_times = np.squeeze(lstm_predicted_total_times)\n",
    "    # Ensure predicted total times are non-negative\n",
    "    lstm_predicted_total_times = np.maximum(0, lstm_predicted_total_times)\n",
    "\n",
    "\n",
    "    transformer_step_proportions = processed_data['transformer_proportions_padded'] \n",
    "    masks_for_calc = processed_data['masks_for_calc'] \n",
    "\n",
    "    _, lstm_refined_increments, lstm_refined_cumulative = calculate_times_from_proportions(\n",
    "        transformer_step_proportions,\n",
    "        lstm_predicted_total_times, \n",
    "        masks_for_calc \n",
    "    )\n",
    "\n",
    "    lstm_refined_increments_np = lstm_refined_increments.numpy()\n",
    "    lstm_refined_cumulative_np = lstm_refined_cumulative.numpy()\n",
    "\n",
    "    # Get original GT increments and cumulative for comparison in the final CSV\n",
    "    # These were stored with \"_original\" suffix in processed_data\n",
    "    gt_increments_original_padded = processed_data['gt_increments_padded_original']\n",
    "    gt_cumulative_original_padded = processed_data['gt_cumulative_padded_original']\n",
    "\n",
    "\n",
    "    results_list_df = []\n",
    "    original_dfs_from_processing = processed_data['original_dfs'] # These are the DFs as read from CSV\n",
    "\n",
    "    for i, seq_id in enumerate(processed_data['sequences_ids']):\n",
    "        if i >= len(original_dfs_from_processing):\n",
    "            print(f\"Warning: Index {i} out of bounds for original_dfs_from_processing. Skipping sequence ID {seq_id}.\")\n",
    "            continue\n",
    "        \n",
    "        # Use the original DataFrame for this sequence as the base\n",
    "        # This df already contains 'Predicted_Increment', 'Predicted_Cumulative' from the Transformer\n",
    "        # and 'GroundTruth_Increment', 'GroundTruth_Cumulative'\n",
    "        current_seq_df_base = original_dfs_from_processing[i].copy()\n",
    "        seq_len = len(current_seq_df_base)\n",
    "\n",
    "        if seq_len == 0:\n",
    "            if current_seq_df_base.empty: \n",
    "                 results_list_df.append(current_seq_df_base) \n",
    "            continue\n",
    "\n",
    "        if i >= len(lstm_predicted_total_times):\n",
    "            print(f\"Warning: Index {i} out of bounds for lstm_predicted_total_times. Skipping sequence ID {seq_id}.\")\n",
    "            continue\n",
    "\n",
    "        # Add LSTM predictions to this DataFrame\n",
    "        current_seq_df_base['LSTM_Predicted_TotalTime'] = lstm_predicted_total_times[i]\n",
    "        current_seq_df_base['LSTM_Predicted_Increment'] = lstm_refined_increments_np[i, :seq_len]\n",
    "        current_seq_df_base['LSTM_Predicted_Cumulative'] = lstm_refined_cumulative_np[i, :seq_len]\n",
    "        \n",
    "        # Ensure GroundTruth columns are present for MAE calculation\n",
    "        if 'GroundTruth_Increment' in current_seq_df_base.columns and 'Predicted_Increment' in current_seq_df_base.columns:\n",
    "            gt_increment = current_seq_df_base['GroundTruth_Increment'].fillna(0) \n",
    "            transformer_pred_increment = current_seq_df_base['Predicted_Increment'].fillna(0)\n",
    "            lstm_pred_increment = current_seq_df_base['LSTM_Predicted_Increment'].fillna(0)\n",
    "            \n",
    "            diff_transformer = np.abs(gt_increment - transformer_pred_increment)\n",
    "            diff_lstm = np.abs(gt_increment - lstm_pred_increment)\n",
    "            \n",
    "            current_seq_df_base['Increment_MAE_Transformer'] = diff_transformer\n",
    "            current_seq_df_base['Increment_MAE_LSTM'] = diff_lstm\n",
    "            current_seq_df_base['Increment_Improvement_Pct'] = np.where(\n",
    "                diff_transformer > 1e-6, (diff_transformer - diff_lstm) / diff_transformer * 100, 0 )\n",
    "        \n",
    "        if 'GroundTruth_Cumulative' in current_seq_df_base.columns and 'Predicted_Cumulative' in current_seq_df_base.columns:\n",
    "            gt_cumulative = current_seq_df_base['GroundTruth_Cumulative'].fillna(0)\n",
    "            transformer_pred_cumulative = current_seq_df_base['Predicted_Cumulative'].fillna(0)\n",
    "            lstm_pred_cumulative = current_seq_df_base['LSTM_Predicted_Cumulative'].fillna(0)\n",
    "\n",
    "            diff_transformer_cum = np.abs(gt_cumulative - transformer_pred_cumulative)\n",
    "            diff_lstm_cum = np.abs(gt_cumulative - lstm_pred_cumulative)\n",
    "\n",
    "            current_seq_df_base['Cumulative_MAE_Transformer'] = diff_transformer_cum\n",
    "            current_seq_df_base['Cumulative_MAE_LSTM'] = diff_lstm_cum\n",
    "            current_seq_df_base['Cumulative_Improvement_Pct'] = np.where(\n",
    "                diff_transformer_cum > 1e-6, (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100, 0 )\n",
    "        \n",
    "        results_list_df.append(current_seq_df_base)\n",
    "    \n",
    "    if not results_list_df:\n",
    "        print(\"Warning: No results to concatenate for the final CSV after processing sequences.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_results_df = pd.concat(results_list_df, ignore_index=True)\n",
    "    \n",
    "    output_filename = 'predictions_lstm_refined_total_time_v4.csv' # Changed filename\n",
    "    final_results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Combined and refined predictions saved to {output_filename}\")\n",
    "    \n",
    "    # Overall Performance Metrics\n",
    "    if not final_results_df.empty:\n",
    "        # MAE for Increments\n",
    "        if 'Increment_MAE_Transformer' in final_results_df.columns and 'Increment_MAE_LSTM' in final_results_df.columns:\n",
    "            # Filter out potential NaNs before mean calculation if any rows were skipped or had issues\n",
    "            valid_inc_mae_transformer = final_results_df['Increment_MAE_Transformer'].dropna()\n",
    "            valid_inc_mae_lstm = final_results_df['Increment_MAE_LSTM'].dropna()\n",
    "            if not valid_inc_mae_transformer.empty and not valid_inc_mae_lstm.empty:\n",
    "                avg_transformer_inc_mae = valid_inc_mae_transformer.mean()\n",
    "                avg_lstm_inc_mae = valid_inc_mae_lstm.mean()\n",
    "                print(\"\\n--- Mean Absolute Error for Increments ---\")\n",
    "                print(f\"Transformer MAE (Increments): {avg_transformer_inc_mae:.4f}\")\n",
    "                print(f\"LSTM-Refined MAE (Increments): {avg_lstm_inc_mae:.4f}\")\n",
    "                if avg_transformer_inc_mae > 1e-6: \n",
    "                    improvement_inc = (avg_transformer_inc_mae - avg_lstm_inc_mae) / avg_transformer_inc_mae * 100\n",
    "                    print(f\"Improvement (Increments): {improvement_inc:.2f}%\")\n",
    "\n",
    "        # MAE for Cumulative Times\n",
    "        if 'Cumulative_MAE_Transformer' in final_results_df.columns and 'Cumulative_MAE_LSTM' in final_results_df.columns:\n",
    "            valid_cum_mae_transformer = final_results_df['Cumulative_MAE_Transformer'].dropna()\n",
    "            valid_cum_mae_lstm = final_results_df['Cumulative_MAE_LSTM'].dropna()\n",
    "            if not valid_cum_mae_transformer.empty and not valid_cum_mae_lstm.empty:\n",
    "                avg_transformer_cum_mae = valid_cum_mae_transformer.mean()\n",
    "                avg_lstm_cum_mae = valid_cum_mae_lstm.mean()\n",
    "                print(\"\\n--- Mean Absolute Error for Cumulative Times ---\")\n",
    "                print(f\"Transformer MAE (Cumulative): {avg_transformer_cum_mae:.4f}\")\n",
    "                print(f\"LSTM-Refined MAE (Cumulative): {avg_lstm_cum_mae:.4f}\")\n",
    "                if avg_transformer_cum_mae > 1e-6: \n",
    "                    improvement_cum = (avg_transformer_cum_mae - avg_lstm_cum_mae) / avg_transformer_cum_mae * 100\n",
    "                    print(f\"Improvement (Cumulative): {improvement_cum:.2f}%\")\n",
    "\n",
    "    # LSTM Total Time Prediction Performance (against the targets it was trained on)\n",
    "    gt_total_times_for_lstm_training = processed_data['y_lstm_target_total_times'] # These are max GT_Cumulative\n",
    "    if len(lstm_predicted_total_times) == len(gt_total_times_for_lstm_training):\n",
    "        mae_total_time_lstm = np.mean(np.abs(gt_total_times_for_lstm_training - lstm_predicted_total_times))\n",
    "        print(\"\\n--- LSTM Total Time Prediction Performance (vs Max GT Cumulative) ---\")\n",
    "        print(f\"MAE for LSTM Predicted Total Time: {mae_total_time_lstm:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nWarning: Mismatch in lengths for GT total times (max GT cum) and LSTM predicted total times. Cannot compute overall MAE for total time.\")\n",
    "    return final_results_df\n",
    "\n",
    "# %%\n",
    "def visualize_lstm_results(results_df, processed_data, lstm_model, training_history):\n",
    "    if results_df.empty:\n",
    "        print(\"Results DataFrame is empty, skipping visualizations.\")\n",
    "        return\n",
    "    print(\"Generating visualizations for LSTM results...\")\n",
    "    plt.style.use('ggplot')\n",
    "    if training_history and hasattr(training_history, 'history'):\n",
    "        if 'loss' in training_history.history and 'val_loss' in training_history.history:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "            plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "            plt.title('LSTM Model Loss (Predicting Total Time)')\n",
    "            plt.xlabel('Epoch'); plt.ylabel('Mean Squared Error (Loss)'); plt.legend(); plt.tight_layout()\n",
    "            plt.savefig('lstm_total_time_training_loss_v4.png'); print(\"Saved LSTM training loss plot.\"); plt.close() # v4\n",
    "        else: print(\"Warning: Training history does not contain 'loss' or 'val_loss' keys.\")\n",
    "    else: print(\"Warning: No training history provided or history object is not as expected.\")\n",
    "\n",
    "    sample_sequence_ids = results_df['Sequence'].unique()\n",
    "    if len(sample_sequence_ids) == 0 : print(\"No sequences found in results_df for plotting.\"); return\n",
    "    sample_sequence_ids = sample_sequence_ids[:min(5, len(sample_sequence_ids))]\n",
    "    \n",
    "    if len(sample_sequence_ids) > 0:\n",
    "        num_plots = len(sample_sequence_ids)\n",
    "        fig_height = max(8, 3 * num_plots) \n",
    "        plt.figure(figsize=(15, fig_height))\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Cumulative'], 'o-', label='GT Cumul.', ms=4)\n",
    "            if 'Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Cumulative'], 's--', label='Transformer Cumul.', ms=4)\n",
    "            if 'LSTM_Predicted_Cumulative' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Cumulative'], '^-.', label='LSTM-Refined Cumul.', ms=4)\n",
    "            plt.title(f'Cumulative Times: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Cumulative Time'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_cumulative_time_comparison_v4.png'); print(\"Saved LSTM-refined cumulative time comparison plot.\"); plt.close() # v4\n",
    "\n",
    "        plt.figure(figsize=(15, fig_height))\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Increment'], 'o-', label='GT Incr.', ms=4)\n",
    "            if 'Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Increment'], 's--', label='Transformer Incr.', ms=4)\n",
    "            if 'LSTM_Predicted_Increment' in seq_data_plot.columns: plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Increment'], '^-.', label='LSTM-Refined Incr.', ms=4)\n",
    "            plt.title(f'Time Increments: Seq {seq_id}'); plt.xlabel('Step'); plt.ylabel('Time Increment'); plt.legend()\n",
    "        plt.tight_layout(); plt.savefig('lstm_refined_increment_comparison_v4.png'); print(\"Saved LSTM-refined increment comparison plot.\"); plt.close() # v4\n",
    "\n",
    "    # Target total times used for LSTM training (max GT cumulative)\n",
    "    gt_total_times_for_lstm_training = processed_data.get('y_lstm_target_total_times', np.array([]))\n",
    "    \n",
    "    if lstm_model is not None and 'X_lstm_input' in processed_data:\n",
    "        X_input_tensor = tf.convert_to_tensor(processed_data['X_lstm_input'], dtype=tf.float32)\n",
    "        lstm_pred_total_t_for_plot = lstm_model.predict(X_input_tensor).squeeze()\n",
    "        if lstm_pred_total_t_for_plot.ndim == 0: lstm_pred_total_t_for_plot = np.array([lstm_pred_total_t_for_plot])\n",
    "            \n",
    "        if gt_total_times_for_lstm_training.size > 0 and lstm_pred_total_t_for_plot.size > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1); \n",
    "            plt.hist(gt_total_times_for_lstm_training, bins=30, alpha=0.7, label='GT Total Times (Max Cumul.)')\n",
    "            plt.hist(lstm_pred_total_t_for_plot, bins=30, alpha=0.7, label='LSTM Pred Total Times')\n",
    "            plt.xlabel('Total Time'); plt.ylabel('Frequency'); plt.title('Distribution of Total Times'); plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2); \n",
    "            if len(gt_total_times_for_lstm_training) == len(lstm_pred_total_t_for_plot):\n",
    "                errors_total_time = gt_total_times_for_lstm_training - lstm_pred_total_t_for_plot\n",
    "                plt.hist(errors_total_time, bins=30, alpha=0.7, color='red')\n",
    "                plt.xlabel('Prediction Error (GT Max Cumul. - Pred)'); plt.ylabel('Frequency'); plt.title('LSTM Total Time Prediction Errors')\n",
    "                if errors_total_time.size > 0: \n",
    "                    mean_error_val = errors_total_time.mean()\n",
    "                    plt.axvline(mean_error_val, color='k', ls='--', lw=1, label=f'Mean Error: {mean_error_val:.2f}')\n",
    "                plt.legend()\n",
    "            else:\n",
    "                print(\"Warning: Mismatch length GT total times and predictions for error histogram.\")\n",
    "\n",
    "            plt.tight_layout(); plt.savefig('lstm_total_time_prediction_analysis_v4.png'); print(\"Saved LSTM total time prediction analysis plot.\"); plt.close() # v4\n",
    "        else: print(\"Warning: Not enough data for total time distribution plots (GT or Pred).\")\n",
    "    else: print(\"Warning: LSTM model or input data missing for total time prediction plot.\")\n",
    "    print(\"Visualizations for LSTM (total time approach) completed!\")\n",
    "\n",
    "# %%\n",
    "def main_lstm_total_time_flow():\n",
    "    try:\n",
    "        transformer_predictions_file = \"predictions_transformer_182625.csv\" \n",
    "        if not os.path.exists(transformer_predictions_file):\n",
    "            print(f\"Error: Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "            print(\"Attempting to create a DUMMY CSV for testing flow.\")\n",
    "            dummy_data = []\n",
    "            for seq_idx in range(150): # More dummy data for robust testing\n",
    "                num_steps = np.random.randint(5, 40) \n",
    "                steps = np.arange(1, num_steps + 1)\n",
    "                gt_increments = np.random.gamma(shape=2.5, scale=np.random.uniform(low=3.0, high=12.0), size=num_steps) + np.random.uniform(low=0.1, high=2.5)\n",
    "                gt_increments = np.maximum(gt_increments, 0.01) # Ensure positive\n",
    "                gt_cumulative = np.cumsum(gt_increments)\n",
    "                \n",
    "                raw_props = np.random.rand(num_steps) + 0.01 \n",
    "                pred_proportions = raw_props / raw_props.sum() \n",
    "                \n",
    "                # For dummy data, let Transformer's predicted total time be somewhat related to GT max cumulative\n",
    "                # This is what the LSTM will try to predict if this dummy CSV is used.\n",
    "                # The \"Predicted_Increment\" and \"Predicted_Cumulative\" in the dummy CSV will be based on this.\n",
    "                # The actual GT_Cumulative's max will be the LSTM's target.\n",
    "                # Let's make the dummy Transformer's prediction of total time (used for its props)\n",
    "                # also based on the max of GT_Cumulative to be consistent with what the LSTM should learn.\n",
    "                # This makes the dummy \"Predicted_Proportion\" more meaningful relative to the LSTM target.\n",
    "                \n",
    "                # The actual total time for this sequence (target for LSTM)\n",
    "                actual_sequence_total_time = gt_cumulative[-1] if num_steps > 0 else 0.1\n",
    "                actual_sequence_total_time = max(actual_sequence_total_time, 0.1) # Ensure positive\n",
    "\n",
    "                # Transformer's predicted increments/cumulative in the dummy CSV\n",
    "                # These are based on its own (potentially flawed) idea of total time, reflected by its proportions.\n",
    "                # For simplicity in dummy data, let's assume its proportions are decent and apply them to a\n",
    "                # slightly perturbed version of the actual_sequence_total_time.\n",
    "                dummy_transformer_effective_total_time = actual_sequence_total_time * np.random.uniform(0.7, 1.3)\n",
    "                dummy_transformer_effective_total_time = max(dummy_transformer_effective_total_time, 0.1)\n",
    "\n",
    "                pred_increments_from_transformer = pred_proportions * dummy_transformer_effective_total_time\n",
    "                pred_cumulative_from_transformer = np.cumsum(pred_increments_from_transformer)\n",
    "\n",
    "                for s_idx in range(num_steps):\n",
    "                    dummy_data.append({\n",
    "                        'Sequence': seq_idx, 'Step': steps[s_idx], 'SourceID': f'MRI_DUMMY_{s_idx%4 +1}',\n",
    "                        'Predicted_Proportion': pred_proportions[s_idx], \n",
    "                        'Predicted_Increment': pred_increments_from_transformer[s_idx], # From dummy Transformer\n",
    "                        'Predicted_Cumulative': pred_cumulative_from_transformer[s_idx], # From dummy Transformer\n",
    "                        'GroundTruth_Increment': gt_increments[s_idx], # Actual GT\n",
    "                        'GroundTruth_Cumulative': gt_cumulative[s_idx]  # Actual GT\n",
    "                    })\n",
    "            if not dummy_data: \n",
    "                 dummy_data.append({ 'Sequence': 0, 'Step': 1, 'SourceID': 'MRI_DUMMY_0', 'Predicted_Proportion': 1.0, \n",
    "                                     'Predicted_Increment': 10.0, 'Predicted_Cumulative': 10.0, \n",
    "                                     'GroundTruth_Increment': 10.0, 'GroundTruth_Cumulative': 10.0})\n",
    "            dummy_df = pd.DataFrame(dummy_data)\n",
    "            dummy_df.to_csv(transformer_predictions_file, index=False)\n",
    "            print(f\"Dummy '{transformer_predictions_file}' created with {len(dummy_df)} rows and {len(dummy_df['Sequence'].unique())} sequences.\")\n",
    "        \n",
    "        lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
    "            transformer_predictions_file, epochs=100, batch_size=16 ) \n",
    "        \n",
    "        if lstm_model is None or processed_lstm_data is None:\n",
    "            print(\"LSTM training failed or returned None. Exiting.\"); return\n",
    "\n",
    "        refined_results_df = generate_refined_predictions_with_lstm(lstm_model, processed_lstm_data)\n",
    "        if not refined_results_df.empty:\n",
    "            print(\"\\nSample of Refined Predictions (LSTM Total Time Approach):\")\n",
    "            display_cols = [ 'Sequence', 'Step', 'SourceID', \n",
    "                             'Predicted_Increment', 'LSTM_Predicted_Increment', 'GroundTruth_Increment', \n",
    "                             'Predicted_Cumulative', 'LSTM_Predicted_Cumulative', 'GroundTruth_Cumulative',\n",
    "                             'LSTM_Predicted_TotalTime', 'Increment_Improvement_Pct']\n",
    "            actual_display_cols = [col for col in display_cols if col in refined_results_df.columns]\n",
    "            print(refined_results_df[actual_display_cols].head(10))\n",
    "            print(\"\\nGenerating visualizations for LSTM (total time approach)...\")\n",
    "            visualize_lstm_results(refined_results_df, processed_lstm_data, lstm_model, lstm_history)\n",
    "        else: print(\"No refined predictions were generated by the LSTM flow.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM (total time) main function: {e}\"); import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for LSTM training...\n",
      "Processing data from: predictions_transformer_182625.csv\n",
      "\n",
      "Statistics for y_total_times_list (unpadded, 186 sequences):\n",
      "  Mean: 0.0000, Std Dev: 0.0000\n",
      "  Min: 0.0000, Max: 0.0000\n",
      "  Number of zeros (<=1e-6): 186\n",
      "  Number non-positive (<=0): 186\n",
      "\n",
      "Number of features for LSTM input: 2\n",
      "Max sequence length for LSTM input: 42\n",
      "\n",
      "LSTM Model Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'total_time_lstm_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"total_time_lstm_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"total_time_lstm_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_lstm_1            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ multi_head_attention_1          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_norm_attention            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pooling_1d           │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_lstm_1            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ multi_head_attention_1          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_norm_attention            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_avg_pooling_1d           │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ total_time_dense_output (\u001b[38;5;33mDense\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL WARNING: All target total times are near zero. Model will likely predict zero and not learn effectively.\n",
      "\n",
      "Manually split data: 148 train, 38 validation samples.\n",
      "Training target statistics (y_train):\n",
      "  Mean: 0.0000, Std: 0.0000, Min: 0.0000, Max: 0.0000\n",
      "  Number of zeros (<=1e-6): 148\n",
      "Validation target statistics (y_val):\n",
      "  Mean: 0.0000, Std: 0.0000, Min: 0.0000, Max: 0.0000\n",
      "  Number of zeros (<=1e-6): 38\n",
      "\n",
      "CRITICAL WARNING: All validation targets (y_val) are effectively zero. val_loss will likely be zero.\n",
      "Starting LSTM model training...\n",
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 90ms/step - loss: 0.9654 - val_loss: 0.1496\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.1066 - val_loss: 0.1212\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0825 - val_loss: 0.0271\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0369 - val_loss: 4.1180e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0169 - val_loss: 0.0012\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0070 - val_loss: 0.0027\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0044 - val_loss: 3.3923e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0028 - val_loss: 7.4967e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0022 - val_loss: 7.8833e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0017 - val_loss: 3.2900e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0018 - val_loss: 5.3877e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0018 - val_loss: 8.8285e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0016 - val_loss: 2.6047e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0013 - val_loss: 8.4508e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0014 - val_loss: 2.8779e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.8827e-04 - val_loss: 4.3747e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.5918e-04 - val_loss: 1.9609e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.3563e-04 - val_loss: 2.8569e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.4633e-04 - val_loss: 1.5407e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.1986e-04 - val_loss: 3.7759e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.4278e-04 - val_loss: 1.5575e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 6.2540e-04 - val_loss: 7.3563e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.2883e-04 - val_loss: 6.1768e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.4194e-04 - val_loss: 4.0107e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 5.8186e-04 - val_loss: 1.8177e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 6.4860e-04 - val_loss: 0.0012\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 6.0067e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 7.2353e-04 - val_loss: 1.8640e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.8328e-04 - val_loss: 5.0255e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 6.0207e-04 - val_loss: 1.6162e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 5.5226e-04 - val_loss: 2.5983e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.7068e-04 - val_loss: 4.1249e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 4.2168e-04 - val_loss: 1.1061e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.4223e-04 - val_loss: 8.4026e-05\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "LSTM training finished.\n",
      "Generating refined predictions using LSTM's total time...\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step\n",
      "Combined and refined predictions saved to predictions_lstm_refined_total_time_v3.csv\n",
      "\n",
      "--- Mean Absolute Error for Increments ---\n",
      "Transformer MAE (Increments): 46.1613\n",
      "LSTM-Refined MAE (Increments): 38.5159\n",
      "Improvement (Increments): 16.56%\n",
      "\n",
      "--- Mean Absolute Error for Cumulative Times ---\n",
      "Transformer MAE (Cumulative): 62.9617\n",
      "LSTM-Refined MAE (Cumulative): 237.7330\n",
      "Improvement (Cumulative): -277.58%\n",
      "\n",
      "--- LSTM Total Time Prediction Performance ---\n",
      "MAE for LSTM Predicted Total Time (vs GT Total Time): 0.0024\n",
      "\n",
      "Sample of Refined Predictions (LSTM Total Time Approach):\n",
      "   Sequence  Step     SourceID  Predicted_Increment  LSTM_Predicted_Increment  \\\n",
      "0         0     1  MRI_MSR_104             1.574359                  0.000011   \n",
      "1         0     2    MRI_FRR_2            23.615385                  0.000160   \n",
      "2         0     3  MRI_FRR_257             7.871795                  0.000053   \n",
      "3         0     4  MRI_FRR_264            17.317950                  0.000118   \n",
      "4         0     5  MRI_FRR_264            26.764105                  0.000182   \n",
      "5         0     6   MRI_CCS_11            45.656410                  0.000310   \n",
      "6         0     7   MRI_CCS_11             1.574359                  0.000011   \n",
      "7         0     8  MRI_FRR_257            39.358980                  0.000267   \n",
      "8         0     9  MRI_FRR_264             1.574359                  0.000011   \n",
      "9         0    10  MRI_FRR_264            20.466667                  0.000139   \n",
      "\n",
      "   GroundTruth_Increment  Predicted_Cumulative  LSTM_Predicted_Cumulative  \\\n",
      "0                   40.0              3.148718                   0.000011   \n",
      "1                    5.0             26.764103                   0.000171   \n",
      "2                    7.0             34.635900                   0.000224   \n",
      "3                   16.0             51.953850                   0.000342   \n",
      "4                    9.0             78.717960                   0.000524   \n",
      "5                    6.0            124.374370                   0.000834   \n",
      "6                  130.0            125.948720                   0.000845   \n",
      "7                    1.0            165.307710                   0.001112   \n",
      "8                   10.0            166.882060                   0.001122   \n",
      "9                    2.0            187.348720                   0.001261   \n",
      "\n",
      "   GroundTruth_Cumulative  LSTM_Predicted_TotalTime  Increment_Improvement_Pct  \n",
      "0                    40.0                  0.002074                  -4.097130  \n",
      "1                    45.0                  0.002074                  73.141358  \n",
      "2                    52.0                  0.002074                -702.934927  \n",
      "3                    68.0                  0.002074               -1113.997679  \n",
      "4                    77.0                  0.002074                  49.337058  \n",
      "5                    83.0                  0.002074                  84.870819  \n",
      "6                   213.0                  0.002074                  -1.225883  \n",
      "7                   214.0                  0.002074                  97.393745  \n",
      "8                   224.0                  0.002074                 -18.685208  \n",
      "9                   226.0                  0.002074                  89.170428  \n",
      "\n",
      "Generating visualizations for LSTM (total time approach)...\n",
      "Generating visualizations for LSTM results...\n",
      "Saved LSTM training loss plot.\n",
      "Saved LSTM-refined cumulative time comparison plot.\n",
      "Saved LSTM-refined increment comparison plot.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Saved LSTM total time prediction analysis plot.\n",
      "Visualizations for LSTM (total time approach) completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_lstm_total_time_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
