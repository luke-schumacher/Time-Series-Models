{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Assuming pad_sequences is available if needed, though direct use might change.\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Seaborn was commented out in original, keeping it that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute increments and cumulative times from proportions and total time\n",
    "def calculate_times_from_proportions(proportions_per_step, total_time_for_sequence, mask_per_step):\n",
    "    \"\"\"\n",
    "    Calculates time increments and cumulative times from step-wise proportions and a total time.\n",
    "\n",
    "    Args:\n",
    "        proportions_per_step (tf.Tensor or np.ndarray): \n",
    "            Proportions for each step in sequences (batch_size, seq_len).\n",
    "        total_time_for_sequence (tf.Tensor or np.ndarray): \n",
    "            The total time for each sequence (batch_size, 1) or (batch_size,).\n",
    "        mask_per_step (tf.Tensor or np.ndarray): \n",
    "            Mask indicating valid steps (batch_size, seq_len).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (normalized_proportions, increments, cumulative_times)\n",
    "               all as tf.Tensor.\n",
    "    \"\"\"\n",
    "    proportions_tf = tf.cast(proportions_per_step, tf.float32)\n",
    "    total_time_tf = tf.cast(total_time_for_sequence, tf.float32)\n",
    "    mask_tf = tf.cast(mask_per_step, tf.float32)\n",
    "\n",
    "    # Ensure total_time_tf is (batch_size, 1) for broadcasting\n",
    "    if len(tf.shape(total_time_tf)) == 1:\n",
    "        total_time_tf = tf.expand_dims(total_time_tf, axis=-1)\n",
    "\n",
    "    # Apply mask to proportions\n",
    "    masked_proportions = proportions_tf * mask_tf\n",
    "\n",
    "    # Normalize proportions over valid steps\n",
    "    row_sums = tf.reduce_sum(masked_proportions, axis=1, keepdims=True)\n",
    "    row_sums = tf.where(tf.equal(row_sums, 0), tf.ones_like(row_sums), row_sums) # Avoid division by zero\n",
    "    normalized_proportions = masked_proportions / row_sums\n",
    "\n",
    "    # Calculate increments\n",
    "    increments = normalized_proportions * total_time_tf # Broadcasting\n",
    "\n",
    "    # Calculate cumulative times\n",
    "    cumulative_times = tf.cumsum(increments, axis=1)\n",
    "    \n",
    "    # Ensure masked steps in final outputs are zero\n",
    "    increments *= mask_tf\n",
    "    cumulative_times *= mask_tf\n",
    "    normalized_proportions *= mask_tf\n",
    "\n",
    "    return normalized_proportions, increments, cumulative_times\n",
    "\n",
    "# %%\n",
    "class TotalTimeLSTM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    LSTM model to predict only the total time of a sequence,\n",
    "    using features that can include Transformer-predicted proportions.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_units=64, num_heads=4, dropout_rate=0.2):\n",
    "        super(TotalTimeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # LSTM for sequence processing\n",
    "        self.lstm_layer = layers.LSTM(self.hidden_units, \n",
    "                                      return_sequences=True, \n",
    "                                      dropout=dropout_rate,\n",
    "                                      recurrent_dropout=dropout_rate,\n",
    "                                      name=\"lstm_1\")\n",
    "        \n",
    "        # Bidirectional LSTM. Output dim: 2 * hidden_units\n",
    "        self.bi_lstm = layers.Bidirectional(\n",
    "            layers.LSTM(self.hidden_units, return_sequences=True, name=\"lstm_bidirectional_inner\"),\n",
    "            name=\"bidirectional_lstm_1\"\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        mha_key_dim = (2 * self.hidden_units) // self.num_heads\n",
    "        if (2 * self.hidden_units) % self.num_heads != 0:\n",
    "            # This check ensures that the output dimension of MHA can match BiLSTM output\n",
    "            raise ValueError(f\"(2 * hidden_units) must be divisible by num_heads for the intended MHA output dimension. \"\n",
    "                             f\"Got 2 * {self.hidden_units} (={2*self.hidden_units}) and num_heads={self.num_heads}.\")\n",
    "\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=self.num_heads, key_dim=mha_key_dim, name=\"multi_head_attention_1\"\n",
    "        )\n",
    "        self.layer_norm_attn = layers.LayerNormalization(name=\"layer_norm_attention\")\n",
    "\n",
    "        self.global_avg_pool = layers.GlobalAveragePooling1D(name=\"global_avg_pooling_1d\")\n",
    "        self.total_time_head = layers.Dense(1, activation='relu', name=\"total_time_dense_output\") \n",
    "        \n",
    "    def call(self, inputs, training=False): \n",
    "        # inputs shape: (batch_size, seq_len, num_features)\n",
    "        \n",
    "        # Create a boolean mask from inputs. Assumes padding is all zeros.\n",
    "        # True for non-padded, False for padded.\n",
    "        mask_bool = tf.reduce_any(tf.not_equal(inputs, 0.0), axis=-1)\n",
    "\n",
    "        lstm_out = self.lstm_layer(inputs, mask=mask_bool, training=training)\n",
    "        bi_lstm_out = self.bi_lstm(lstm_out, mask=mask_bool, training=training)\n",
    "        \n",
    "        # Prepare attention mask for MHA: (batch_size, 1, 1, seq_length)\n",
    "        # This will be broadcast to (batch_size, num_heads, query_seq_length, key_seq_length)\n",
    "        # True means keep, False means mask out.\n",
    "        # MHA expects False for positions to mask. So, if mask_bool is True for valid,\n",
    "        # we might need to invert it depending on MHA's interpretation or use it as is if it masks where mask is False.\n",
    "        # Keras MHA `attention_mask`: True for allowed, False for masked.\n",
    "        # Our `mask_bool` is True for allowed. So, it should be fine.\n",
    "        # The shape should be (B, T, S) or (B, N, T, S). For self-attention, T=S.\n",
    "        # Mask for MHA should indicate which tokens are padding.\n",
    "        # (B, S) -> (B, 1, 1, S) for key padding mask.\n",
    "        mha_attention_mask = mask_bool[:, tf.newaxis, tf.newaxis, :] # (batch, 1, 1, key_seq_len)\n",
    "\n",
    "        attn_output = self.attention(query=bi_lstm_out, value=bi_lstm_out, key=bi_lstm_out, \n",
    "                                     attention_mask=mha_attention_mask, \n",
    "                                     training=training)\n",
    "        \n",
    "        x = self.layer_norm_attn(attn_output + bi_lstm_out) # Residual connection\n",
    "        \n",
    "        sequence_encoding = self.global_avg_pool(x, mask=mask_bool)\n",
    "        \n",
    "        total_time_pred = self.total_time_head(sequence_encoding) \n",
    "        \n",
    "        return total_time_pred\n",
    "\n",
    "# %%\n",
    "def process_input_data_for_lstm(transformer_predictions_file):\n",
    "    \"\"\"\n",
    "    Process the transformer predictions CSV to prepare data for LSTM training.\n",
    "    \"\"\"\n",
    "    print(f\"Processing data from: {transformer_predictions_file}\")\n",
    "    if not os.path.exists(transformer_predictions_file):\n",
    "        raise FileNotFoundError(f\"Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "    df = pd.read_csv(transformer_predictions_file)\n",
    "    \n",
    "    if 'Predicted_Proportion' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'Predicted_Proportion' column from Transformer.\")\n",
    "    if 'GroundTruth_Cumulative' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'GroundTruth_Cumulative' column for target extraction.\")\n",
    "    if 'GroundTruth_Increment' not in df.columns:\n",
    "         raise ValueError(\"CSV must contain 'GroundTruth_Increment' column.\")\n",
    "\n",
    "\n",
    "    sequences = df['Sequence'].unique()\n",
    "    \n",
    "    X_data_list = []\n",
    "    y_total_times_list = []\n",
    "    masks_list = [] # This will store boolean masks (True for valid)\n",
    "    transformer_proportions_list = [] \n",
    "    ground_truth_increments_list = []\n",
    "    ground_truth_cumulative_list = []\n",
    "    original_dfs_list = [] \n",
    "\n",
    "    for seq_id in sequences:\n",
    "        seq_df = df[df['Sequence'] == seq_id].sort_values('Step').copy()\n",
    "        \n",
    "        if seq_df.empty:\n",
    "            print(f\"Warning: Sequence {seq_id} is empty. Skipping.\")\n",
    "            original_dfs_list.append(seq_df) # Append empty df to keep counts consistent if needed\n",
    "            continue\n",
    "        \n",
    "        original_dfs_list.append(seq_df) \n",
    "\n",
    "        current_max_steps = seq_df['Step'].max()\n",
    "        if current_max_steps == 0: current_max_steps = 1 \n",
    "        \n",
    "        features = np.column_stack([\n",
    "            seq_df['Predicted_Proportion'].values,\n",
    "            seq_df['Step'].values / current_max_steps \n",
    "        ])\n",
    "        \n",
    "        X_data_list.append(features)\n",
    "        \n",
    "        gt_cumulative_for_seq = seq_df['GroundTruth_Cumulative'].values\n",
    "        total_time_for_seq = gt_cumulative_for_seq[-1] if len(gt_cumulative_for_seq) > 0 else 0.0\n",
    "        y_total_times_list.append(total_time_for_seq)\n",
    "        \n",
    "        masks_list.append(np.ones(len(features), dtype=bool)) # Store boolean True for valid steps\n",
    "        \n",
    "        transformer_proportions_list.append(seq_df['Predicted_Proportion'].values)\n",
    "        ground_truth_increments_list.append(seq_df['GroundTruth_Increment'].values)\n",
    "        ground_truth_cumulative_list.append(gt_cumulative_for_seq)\n",
    "\n",
    "    if not X_data_list:\n",
    "        raise ValueError(\"No valid sequences processed. Check CSV content or processing logic.\")\n",
    "\n",
    "    max_length = max(len(x) for x in X_data_list)\n",
    "    num_features = X_data_list[0].shape[1]\n",
    "\n",
    "    X_padded = np.zeros((len(X_data_list), max_length, num_features), dtype=np.float32)\n",
    "    # masks_padded stores the float mask for calculate_times_from_proportions\n",
    "    masks_padded_float = np.zeros((len(X_data_list), max_length), dtype=np.float32) \n",
    "    transformer_proportions_padded = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "    gt_increments_padded = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "    gt_cumulative_padded = np.zeros((len(X_data_list), max_length), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(X_data_list)):\n",
    "        seq_len = len(X_data_list[i])\n",
    "        if seq_len > 0:\n",
    "            X_padded[i, :seq_len, :] = X_data_list[i]\n",
    "            masks_padded_float[i, :seq_len] = 1.0 # Float mask for helper\n",
    "            transformer_proportions_padded[i, :seq_len] = transformer_proportions_list[i]\n",
    "            gt_increments_padded[i, :seq_len] = ground_truth_increments_list[i]\n",
    "            gt_cumulative_padded[i, :seq_len] = ground_truth_cumulative_list[i]\n",
    "        \n",
    "    y_total_times_np = np.array(y_total_times_list, dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        'X_lstm_input': X_padded,\n",
    "        'y_lstm_target_total_times': y_total_times_np,\n",
    "        'masks_for_calc': masks_padded_float, # Float mask for calculate_times_from_proportions\n",
    "        'sequences_ids': sequences, \n",
    "        'original_dfs': original_dfs_list, \n",
    "        'transformer_proportions_padded': transformer_proportions_padded, \n",
    "        'gt_increments_padded': gt_increments_padded, \n",
    "        'gt_cumulative_padded': gt_cumulative_padded, \n",
    "        'max_len': max_length,\n",
    "        'num_features': num_features\n",
    "    }\n",
    "\n",
    "# %%\n",
    "def train_total_time_lstm(transformer_predictions_file, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train LSTM model to predict total_time.\n",
    "    \"\"\"\n",
    "    print(\"Processing data for LSTM training...\")\n",
    "    data_for_lstm = process_input_data_for_lstm(transformer_predictions_file)\n",
    "    \n",
    "    print(f\"Number of features for LSTM input: {data_for_lstm['num_features']}\")\n",
    "    print(f\"Max sequence length for LSTM input: {data_for_lstm['max_len']}\")\n",
    "\n",
    "    lstm_model = TotalTimeLSTM(hidden_units=64, num_heads=4, dropout_rate=0.2) \n",
    "    \n",
    "    # Explicitly build the model\n",
    "    # Batch dimension is None, seq_len is max_len, features is num_features\n",
    "    input_shape = (None, data_for_lstm['max_len'], data_for_lstm['num_features'])\n",
    "    lstm_model.build(input_shape=input_shape) \n",
    "    \n",
    "    print(\"LSTM Model Summary:\")\n",
    "    lstm_model.summary() \n",
    "\n",
    "    lstm_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001), \n",
    "        loss='mse' \n",
    "    )\n",
    "    \n",
    "    y_train_lstm = data_for_lstm['y_lstm_target_total_times']\n",
    "    X_train_lstm = data_for_lstm['X_lstm_input']\n",
    "    \n",
    "    # Check for NaN/inf in inputs and targets\n",
    "    if np.any(np.isnan(X_train_lstm)) or np.any(np.isinf(X_train_lstm)):\n",
    "        print(\"Warning: NaN or Inf found in X_train_lstm input.\")\n",
    "    if np.any(np.isnan(y_train_lstm)) or np.any(np.isinf(y_train_lstm)):\n",
    "        print(\"Warning: NaN or Inf found in y_train_lstm targets.\")\n",
    "\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Starting LSTM model training...\")\n",
    "    \n",
    "    history = lstm_model.fit(\n",
    "        X_train_lstm, \n",
    "        y_train_lstm,          \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2, \n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"LSTM training finished.\")\n",
    "    return lstm_model, history, data_for_lstm\n",
    "\n",
    "# %%\n",
    "def generate_refined_predictions_with_lstm(lstm_model, processed_data):\n",
    "    \"\"\"\n",
    "    Generate refined time predictions using LSTM's total_time.\n",
    "    \"\"\"\n",
    "    print(\"Generating refined predictions using LSTM's total time...\")\n",
    "    \n",
    "    X_input_for_prediction = processed_data['X_lstm_input']\n",
    "    lstm_predicted_total_times = lstm_model.predict(X_input_for_prediction) \n",
    "    lstm_predicted_total_times = np.squeeze(lstm_predicted_total_times) \n",
    "\n",
    "    transformer_step_proportions = processed_data['transformer_proportions_padded'] \n",
    "    # Use the float mask prepared for calculate_times_from_proportions\n",
    "    masks_for_calc = processed_data['masks_for_calc'] \n",
    "\n",
    "    _, lstm_refined_increments, lstm_refined_cumulative = calculate_times_from_proportions(\n",
    "        transformer_step_proportions,\n",
    "        lstm_predicted_total_times, \n",
    "        masks_for_calc # Pass the correct mask here\n",
    "    )\n",
    "\n",
    "    lstm_refined_increments_np = lstm_refined_increments.numpy()\n",
    "    lstm_refined_cumulative_np = lstm_refined_cumulative.numpy()\n",
    "\n",
    "    results_list_df = []\n",
    "    original_dfs = processed_data['original_dfs'] \n",
    "\n",
    "    for i, seq_id in enumerate(processed_data['sequences_ids']):\n",
    "        # Ensure index i is within bounds for original_dfs\n",
    "        if i >= len(original_dfs):\n",
    "            print(f\"Warning: Index {i} out of bounds for original_dfs. Skipping sequence ID {seq_id}.\")\n",
    "            continue\n",
    "        original_seq_df = original_dfs[i].copy() \n",
    "        seq_len = len(original_seq_df)\n",
    "\n",
    "        if seq_len == 0:\n",
    "            # This sequence was empty in the original data, skip adding LSTM predictions to it\n",
    "            # but it might be part of final_results_df if original_dfs included it.\n",
    "            # Best to ensure original_dfs only contains non-empty DFs if they are used this way.\n",
    "            # For now, if it's empty, we just append it as is or skip.\n",
    "            # If process_input_data_for_lstm filters them, this won't be an issue.\n",
    "            if original_seq_df.empty: # If it was truly empty and not just filtered later\n",
    "                 results_list_df.append(original_seq_df) # Add empty if it was there\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Ensure index i is within bounds for lstm_predicted_total_times\n",
    "        if i >= len(lstm_predicted_total_times):\n",
    "            print(f\"Warning: Index {i} out of bounds for lstm_predicted_total_times. Skipping sequence ID {seq_id}.\")\n",
    "            continue\n",
    "\n",
    "        original_seq_df['LSTM_Predicted_TotalTime'] = lstm_predicted_total_times[i]\n",
    "        original_seq_df['LSTM_Predicted_Increment'] = lstm_refined_increments_np[i, :seq_len]\n",
    "        original_seq_df['LSTM_Predicted_Cumulative'] = lstm_refined_cumulative_np[i, :seq_len]\n",
    "        \n",
    "        # MAE Calculation and Improvement Pct\n",
    "        if 'GroundTruth_Increment' in original_seq_df.columns and 'Predicted_Increment' in original_seq_df.columns:\n",
    "            gt_increment = original_seq_df['GroundTruth_Increment'].fillna(0) # Handle potential NaNs\n",
    "            transformer_pred_increment = original_seq_df['Predicted_Increment'].fillna(0)\n",
    "            lstm_pred_increment = original_seq_df['LSTM_Predicted_Increment'].fillna(0)\n",
    "            \n",
    "            diff_transformer = np.abs(gt_increment - transformer_pred_increment)\n",
    "            diff_lstm = np.abs(gt_increment - lstm_pred_increment)\n",
    "            \n",
    "            original_seq_df['Increment_MAE_Transformer'] = diff_transformer\n",
    "            original_seq_df['Increment_MAE_LSTM'] = diff_lstm\n",
    "            \n",
    "            original_seq_df['Increment_Improvement_Pct'] = np.where(\n",
    "                diff_transformer > 1e-6, \n",
    "                (diff_transformer - diff_lstm) / diff_transformer * 100,\n",
    "                0 \n",
    "            )\n",
    "        \n",
    "        if 'GroundTruth_Cumulative' in original_seq_df.columns and 'Predicted_Cumulative' in original_seq_df.columns:\n",
    "            gt_cumulative = original_seq_df['GroundTruth_Cumulative'].fillna(0)\n",
    "            transformer_pred_cumulative = original_seq_df['Predicted_Cumulative'].fillna(0)\n",
    "            lstm_pred_cumulative = original_seq_df['LSTM_Predicted_Cumulative'].fillna(0)\n",
    "\n",
    "            diff_transformer_cum = np.abs(gt_cumulative - transformer_pred_cumulative)\n",
    "            diff_lstm_cum = np.abs(gt_cumulative - lstm_pred_cumulative)\n",
    "\n",
    "            original_seq_df['Cumulative_MAE_Transformer'] = diff_transformer_cum\n",
    "            original_seq_df['Cumulative_MAE_LSTM'] = diff_lstm_cum\n",
    "\n",
    "            original_seq_df['Cumulative_Improvement_Pct'] = np.where(\n",
    "                diff_transformer_cum > 1e-6,\n",
    "                (diff_transformer_cum - diff_lstm_cum) / diff_transformer_cum * 100,\n",
    "                0\n",
    "            )\n",
    "        results_list_df.append(original_seq_df)\n",
    "    \n",
    "    if not results_list_df:\n",
    "        print(\"Warning: No results to concatenate for the final CSV after processing sequences.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_results_df = pd.concat(results_list_df, ignore_index=True)\n",
    "    \n",
    "    output_filename = 'predictions_lstm_refined_total_time_approach.csv'\n",
    "    final_results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Combined and refined predictions saved to {output_filename}\")\n",
    "    \n",
    "    # Overall Performance Metrics\n",
    "    if not final_results_df.empty:\n",
    "        if 'Increment_MAE_Transformer' in final_results_df.columns and 'Increment_MAE_LSTM' in final_results_df.columns:\n",
    "            avg_transformer_inc_mae = final_results_df['Increment_MAE_Transformer'].mean()\n",
    "            avg_lstm_inc_mae = final_results_df['Increment_MAE_LSTM'].mean()\n",
    "            print(\"\\n--- Mean Absolute Error for Increments ---\")\n",
    "            print(f\"Transformer MAE (Increments): {avg_transformer_inc_mae:.4f}\")\n",
    "            print(f\"LSTM-Refined MAE (Increments): {avg_lstm_inc_mae:.4f}\")\n",
    "            if avg_transformer_inc_mae > 1e-6: # Avoid division by zero\n",
    "                improvement_inc = (avg_transformer_inc_mae - avg_lstm_inc_mae) / avg_transformer_inc_mae * 100\n",
    "                print(f\"Improvement (Increments): {improvement_inc:.2f}%\")\n",
    "\n",
    "        if 'Cumulative_MAE_Transformer' in final_results_df.columns and 'Cumulative_MAE_LSTM' in final_results_df.columns:\n",
    "            avg_transformer_cum_mae = final_results_df['Cumulative_MAE_Transformer'].mean()\n",
    "            avg_lstm_cum_mae = final_results_df['Cumulative_MAE_LSTM'].mean()\n",
    "            print(\"\\n--- Mean Absolute Error for Cumulative Times ---\")\n",
    "            print(f\"Transformer MAE (Cumulative): {avg_transformer_cum_mae:.4f}\")\n",
    "            print(f\"LSTM-Refined MAE (Cumulative): {avg_lstm_cum_mae:.4f}\")\n",
    "            if avg_transformer_cum_mae > 1e-6: # Avoid division by zero\n",
    "                improvement_cum = (avg_transformer_cum_mae - avg_lstm_cum_mae) / avg_transformer_cum_mae * 100\n",
    "                print(f\"Improvement (Cumulative): {improvement_cum:.2f}%\")\n",
    "\n",
    "    gt_total_times_all_seqs = processed_data['y_lstm_target_total_times']\n",
    "    # Ensure lstm_predicted_total_times has the same length as gt_total_times_all_seqs\n",
    "    if len(lstm_predicted_total_times) == len(gt_total_times_all_seqs):\n",
    "        mae_total_time_lstm = np.mean(np.abs(gt_total_times_all_seqs - lstm_predicted_total_times))\n",
    "        print(\"\\n--- LSTM Total Time Prediction Performance ---\")\n",
    "        print(f\"MAE for LSTM Predicted Total Time (vs GT Total Time): {mae_total_time_lstm:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nWarning: Mismatch in lengths for GT total times and predicted total times. Cannot compute overall MAE for total time.\")\n",
    "\n",
    "\n",
    "    return final_results_df\n",
    "\n",
    "# %%\n",
    "def visualize_lstm_results(results_df, processed_data, lstm_model, training_history):\n",
    "    \"\"\"\n",
    "    Generate visualizations for the LSTM model that predicts total time.\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"Results DataFrame is empty, skipping visualizations.\")\n",
    "        return\n",
    "        \n",
    "    print(\"Generating visualizations for LSTM results...\")\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    if training_history and hasattr(training_history, 'history'):\n",
    "        if 'loss' in training_history.history and 'val_loss' in training_history.history:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "            plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "            plt.title('LSTM Model Loss (Predicting Total Time)')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Mean Squared Error (Loss)')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('lstm_total_time_training_loss.png')\n",
    "            print(\"Saved LSTM training loss plot.\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"Warning: Training history does not contain 'loss' or 'val_loss' keys.\")\n",
    "    else:\n",
    "        print(\"Warning: No training history provided or history object is not as expected.\")\n",
    "\n",
    "\n",
    "    sample_sequence_ids = results_df['Sequence'].unique()\n",
    "    if len(sample_sequence_ids) == 0 :\n",
    "        print(\"No sequences found in results_df for plotting.\")\n",
    "        return\n",
    "    sample_sequence_ids = sample_sequence_ids[:min(5, len(sample_sequence_ids))]\n",
    "    \n",
    "    if len(sample_sequence_ids) > 0:\n",
    "        num_plots = len(sample_sequence_ids)\n",
    "        fig_height_cumulative = max(8, 3 * num_plots) \n",
    "        plt.figure(figsize=(15, fig_height_cumulative))\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Cumulative'], 'o-', label='Ground Truth Cumul.', markersize=4)\n",
    "            if 'Predicted_Cumulative' in seq_data_plot.columns:\n",
    "                 plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Cumulative'], 's--', label='Transformer Pred. Cumul.', markersize=4)\n",
    "            if 'LSTM_Predicted_Cumulative' in seq_data_plot.columns:\n",
    "                plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Cumulative'], '^-.', label='LSTM-Refined Pred. Cumul.', markersize=4)\n",
    "            else:\n",
    "                print(f\"Warning: LSTM_Predicted_Cumulative not found for seq {seq_id}\")\n",
    "            plt.title(f'Cumulative Times: Sequence {seq_id}')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Cumulative Time')\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('lstm_refined_cumulative_time_comparison.png')\n",
    "        print(\"Saved LSTM-refined cumulative time comparison plot.\")\n",
    "        plt.close()\n",
    "\n",
    "        fig_height_increment = max(8, 3 * num_plots)\n",
    "        plt.figure(figsize=(15, fig_height_increment))\n",
    "        for i, seq_id in enumerate(sample_sequence_ids):\n",
    "            seq_data_plot = results_df[results_df['Sequence'] == seq_id].sort_values('Step')\n",
    "            if seq_data_plot.empty: continue\n",
    "            plt.subplot(num_plots, 1, i + 1)\n",
    "            plt.plot(seq_data_plot['Step'], seq_data_plot['GroundTruth_Increment'], 'o-', label='Ground Truth Incr.', markersize=4)\n",
    "            if 'Predicted_Increment' in seq_data_plot.columns:\n",
    "                 plt.plot(seq_data_plot['Step'], seq_data_plot['Predicted_Increment'], 's--', label='Transformer Pred. Incr.', markersize=4)\n",
    "            if 'LSTM_Predicted_Increment' in seq_data_plot.columns:\n",
    "                plt.plot(seq_data_plot['Step'], seq_data_plot['LSTM_Predicted_Increment'], '^-.', label='LSTM-Refined Pred. Incr.', markersize=4)\n",
    "            else:\n",
    "                 print(f\"Warning: LSTM_Predicted_Increment not found for seq {seq_id}\")\n",
    "            plt.title(f'Time Increments: Sequence {seq_id}')\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Time Increment')\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('lstm_refined_increment_comparison.png')\n",
    "        print(\"Saved LSTM-refined increment comparison plot.\")\n",
    "        plt.close()\n",
    "\n",
    "    gt_total_times = processed_data.get('y_lstm_target_total_times', np.array([]))\n",
    "    if lstm_model is not None and 'X_lstm_input' in processed_data:\n",
    "        X_input_tensor = tf.convert_to_tensor(processed_data['X_lstm_input'], dtype=tf.float32)\n",
    "        lstm_pred_total_t = lstm_model.predict(X_input_tensor).squeeze()\n",
    "\n",
    "        if gt_total_times.size > 0 and lstm_pred_total_t.size > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(gt_total_times, bins=30, alpha=0.7, label='Ground Truth Total Times')\n",
    "            plt.hist(lstm_pred_total_t, bins=30, alpha=0.7, label='LSTM Predicted Total Times')\n",
    "            plt.xlabel('Total Time')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Distribution of Total Times')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            errors_total_time = gt_total_times - lstm_pred_total_t\n",
    "            plt.hist(errors_total_time, bins=30, alpha=0.7, color='red')\n",
    "            plt.xlabel('Prediction Error (GT - Pred)')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Distribution of LSTM Total Time Prediction Errors')\n",
    "            if errors_total_time.size > 0:\n",
    "                mean_error_val = errors_total_time.mean()\n",
    "                plt.axvline(mean_error_val, color='k', linestyle='dashed', linewidth=1, label=f'Mean Error: {mean_error_val:.2f}')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('lstm_total_time_prediction_analysis.png')\n",
    "            print(\"Saved LSTM total time prediction analysis plot.\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"Warning: Not enough data for total time distribution plots.\")\n",
    "    else:\n",
    "        print(\"Warning: LSTM model or input data missing for total time prediction plot.\")\n",
    "    \n",
    "    print(\"Visualizations for LSTM (total time approach) completed!\")\n",
    "\n",
    "# %%\n",
    "def main_lstm_total_time_flow():\n",
    "    \"\"\"\n",
    "    Main function to run the LSTM model for total time prediction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        transformer_predictions_file = \"predictions_transformer_182625.csv\" \n",
    "        \n",
    "        if not os.path.exists(transformer_predictions_file):\n",
    "            print(f\"Error: Transformer predictions file not found: {transformer_predictions_file}\")\n",
    "            print(\"Attempting to create a DUMMY CSV for testing flow.\")\n",
    "            dummy_data = []\n",
    "            for seq_idx in range(20): # Increased dummy sequences\n",
    "                num_steps = np.random.randint(5, 25) # Varied sequence lengths\n",
    "                steps = np.arange(1, num_steps + 1)\n",
    "                # Ensure gt_increments are positive and have some variance\n",
    "                gt_increments = np.random.gamma(2, scale=5, size=num_steps) + 1 \n",
    "                gt_cumulative = np.cumsum(gt_increments)\n",
    "                \n",
    "                # Ensure raw_props are positive and sum to 1 for realistic proportions\n",
    "                raw_props = np.random.rand(num_steps) + 0.01 \n",
    "                pred_proportions = raw_props / raw_props.sum() \n",
    "                \n",
    "                # Transformer's predicted increments and cumulative (can be different from GT)\n",
    "                # For dummy, let's make them somewhat related but not identical to GT\n",
    "                dummy_transformer_total_time = gt_cumulative[-1] * np.random.uniform(0.8, 1.2) if num_steps > 0 else 0\n",
    "                pred_increments = pred_proportions * dummy_transformer_total_time\n",
    "                pred_cumulative = np.cumsum(pred_increments)\n",
    "\n",
    "                for s_idx in range(num_steps):\n",
    "                    dummy_data.append({\n",
    "                        'Sequence': seq_idx,\n",
    "                        'Step': steps[s_idx],\n",
    "                        'SourceID': f'MRI_DUMMY_{s_idx%3 +1}',\n",
    "                        'Predicted_Proportion': pred_proportions[s_idx],\n",
    "                        'Predicted_Increment': pred_increments[s_idx],\n",
    "                        'Predicted_Cumulative': pred_cumulative[s_idx],\n",
    "                        'GroundTruth_Increment': gt_increments[s_idx],\n",
    "                        'GroundTruth_Cumulative': gt_cumulative[s_idx]\n",
    "                    })\n",
    "            if not dummy_data:\n",
    "                 print(\"Critical Error: Failed to generate any dummy data steps.\")\n",
    "                 dummy_data.append({\n",
    "                        'Sequence': 0, 'Step': 1, 'SourceID': 'MRI_DUMMY_0',\n",
    "                        'Predicted_Proportion': 1.0, 'Predicted_Increment': 10.0,\n",
    "                        'Predicted_Cumulative': 10.0, 'GroundTruth_Increment': 10.0,\n",
    "                        'GroundTruth_Cumulative': 10.0\n",
    "                    }) # Fallback single row\n",
    "\n",
    "            dummy_df = pd.DataFrame(dummy_data)\n",
    "            dummy_df.to_csv(transformer_predictions_file, index=False)\n",
    "            print(f\"Dummy '{transformer_predictions_file}' created with {len(dummy_df)} rows and {len(dummy_df['Sequence'].unique())} sequences.\")\n",
    "        \n",
    "        lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
    "            transformer_predictions_file, epochs=50, batch_size=16 \n",
    "        )\n",
    "        \n",
    "        if lstm_model is None or processed_lstm_data is None:\n",
    "            print(\"LSTM training failed or returned None. Exiting.\")\n",
    "            return\n",
    "\n",
    "        refined_results_df = generate_refined_predictions_with_lstm(lstm_model, processed_lstm_data)\n",
    "        \n",
    "        if not refined_results_df.empty:\n",
    "            print(\"\\nSample of Refined Predictions (LSTM Total Time Approach):\")\n",
    "            display_cols = [\n",
    "                'Sequence', 'Step', 'SourceID', \n",
    "                'Predicted_Increment', # Transformer's original\n",
    "                'LSTM_Predicted_Increment', # LSTM-refined\n",
    "                'GroundTruth_Increment',\n",
    "                'Predicted_Cumulative', # Transformer's original\n",
    "                'LSTM_Predicted_Cumulative', # LSTM-refined\n",
    "                'GroundTruth_Cumulative',\n",
    "                'LSTM_Predicted_TotalTime',\n",
    "                'Increment_Improvement_Pct'\n",
    "            ]\n",
    "            actual_display_cols = [col for col in display_cols if col in refined_results_df.columns]\n",
    "            print(refined_results_df[actual_display_cols].head(10))\n",
    "        \n",
    "            print(\"\\nGenerating visualizations for LSTM (total time approach)...\")\n",
    "            visualize_lstm_results(refined_results_df, processed_lstm_data, lstm_model, lstm_history)\n",
    "        else:\n",
    "            print(\"No refined predictions were generated by the LSTM flow.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSTM (total time) main function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for LSTM training...\n",
      "Processing data from: predictions_transformer_182625.csv\n",
      "Number of features for LSTM input: 2\n",
      "Error in LSTM (total time) main function: object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_60648\\3529871220.py\", line 529, in main_lstm_total_time_flow\n",
      "    lstm_model, lstm_history, processed_lstm_data = train_total_time_lstm(\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Temp\\ipykernel_60648\\3529871220.py\", line 229, in train_total_time_lstm\n",
      "    lstm_model.summary() # Now this should work\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\summary_utils.py\", line 114, in format_layer_shape\n",
      "    if len(output_shapes) == 1:\n",
      "TypeError: object of type 'NoneType' has no len()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_lstm_total_time_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
