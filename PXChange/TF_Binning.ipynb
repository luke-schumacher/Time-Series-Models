{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef0a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dbc4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Encoding\n",
    "START_TOKEN = 13\n",
    "END_TOKEN = 14\n",
    "# Add a PAD token if not using mask_zero, but we are using mask_zero=END_TOKEN\n",
    "# So tokens 0-12 are source IDs, 13 is START, 14 is END (and PAD)\n",
    "ENCODING_LEGEND = {\n",
    "    'MRI_CCS_11': 1, 'MRI_EXU_95': 2, 'MRI_FRR_18': 3, 'MRI_FRR_257': 4,\n",
    "    'MRI_FRR_264': 5, 'MRI_FRR_2': 6, 'MRI_FRR_3': 7, 'MRI_FRR_34': 8, 'MRI_MPT_1005': 9,\n",
    "    'MRI_MSR_100': 10, 'MRI_MSR_104': 11, 'MRI_MSR_21': 12, 'MRI_MSR_24': 99,\n",
    "    'START': START_TOKEN, 'END': END_TOKEN\n",
    "}\n",
    "reverse_encoding = {v: k for k, v in ENCODING_LEGEND.items()}\n",
    "\n",
    "# Define valid source IDs for filtering (excluding START and END tokens)\n",
    "VALID_SOURCE_IDS = set([k for k in ENCODING_LEGEND.keys() if k not in ['START', 'END']])\n",
    "\n",
    "# Define the columns from the original data to keep in the final output\n",
    "COLUMNS_TO_KEEP = ['timediff', 'PTAB', 'BodyGroup_from', 'BodyGroup_to', 'PatientID_from', 'PatientID_to']\n",
    "\n",
    "# Binning parameters\n",
    "NUM_BINS = 250\n",
    "# Define bin edges from 0 to 1 (inclusive of 0, exclusive of 1 for all but the last bin)\n",
    "# The last bin will include 1.0\n",
    "BIN_EDGES = np.linspace(0.0, 1.0, NUM_BINS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c212b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the load_and_preprocess_data function (cell [36]) with this one.\n",
    "\n",
    "def load_and_preprocess_data(data_file):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses data from a CSV file, filtering out invalid sourceIDs.\n",
    "    Splits data into sequences based on 'MRI_MSR_104' (start) and 'MRI_MSR_100' (end).\n",
    "    Assigns a sequence number during loading and keeps specified additional columns.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {data_file}...\")\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "    all_sequences_tokens = []\n",
    "    all_sequences_times = []\n",
    "    all_sequences_sourceids = []\n",
    "    all_sequences_extra_data = [] # New: To store the extra columns\n",
    "\n",
    "    current_tokens = []\n",
    "    current_times = []\n",
    "    current_sourceids = []\n",
    "    current_extra_data = [] # New: For the current sequence\n",
    "\n",
    "    # Iterate through rows to build sequences\n",
    "    for idx, row in data.iterrows():\n",
    "        s_id = str(row['sourceID'])\n",
    "        t_diff = float(row['timediff'])\n",
    "\n",
    "        if s_id not in VALID_SOURCE_IDS:\n",
    "            continue\n",
    "\n",
    "        # New: Extract extra data for the current valid row\n",
    "        extra_data = {col: row.get(col) for col in COLUMNS_TO_KEEP}\n",
    "\n",
    "        if s_id == 'MRI_MSR_104':\n",
    "            if current_tokens:\n",
    "                token_seq = [START_TOKEN] + [int(ENCODING_LEGEND[x]) for x in current_tokens] + [END_TOKEN]\n",
    "                time_seq = [0.0] + current_times\n",
    "                all_sequences_tokens.append(token_seq)\n",
    "                all_sequences_times.append(time_seq)\n",
    "                all_sequences_sourceids.append(current_sourceids)\n",
    "                all_sequences_extra_data.append(current_extra_data) # New\n",
    "\n",
    "            current_tokens = [s_id]\n",
    "            current_times = [t_diff]\n",
    "            current_sourceids = [s_id]\n",
    "            current_extra_data = [extra_data] # New\n",
    "\n",
    "        elif s_id == 'MRI_MSR_100':\n",
    "             if current_tokens:\n",
    "                current_tokens.append(s_id)\n",
    "                current_times.append(t_diff)\n",
    "                current_sourceids.append(s_id)\n",
    "                current_extra_data.append(extra_data) # New\n",
    "\n",
    "                token_seq = [START_TOKEN] + [int(ENCODING_LEGEND[x]) for x in current_tokens] + [END_TOKEN]\n",
    "                time_seq = [0.0] + current_times\n",
    "                all_sequences_tokens.append(token_seq)\n",
    "                all_sequences_times.append(time_seq)\n",
    "                all_sequences_sourceids.append(current_sourceids)\n",
    "                all_sequences_extra_data.append(current_extra_data) # New\n",
    "\n",
    "                current_tokens, current_times, current_sourceids, current_extra_data = [], [], [], [] # New\n",
    "\n",
    "        elif current_tokens:\n",
    "            current_tokens.append(s_id)\n",
    "            current_times.append(t_diff)\n",
    "            current_sourceids.append(s_id)\n",
    "            current_extra_data.append(extra_data) # New\n",
    "\n",
    "    if current_tokens:\n",
    "         token_seq = [START_TOKEN] + [int(ENCODING_LEGEND[x]) for x in current_tokens] + [END_TOKEN]\n",
    "         time_seq = [0.0] + current_times\n",
    "         all_sequences_tokens.append(token_seq)\n",
    "         all_sequences_times.append(time_seq)\n",
    "         all_sequences_sourceids.append(current_sourceids)\n",
    "         all_sequences_extra_data.append(current_extra_data) # New\n",
    "\n",
    "    print(f\"Loaded {len(all_sequences_tokens)} sequences.\")\n",
    "    # New: Return the extra data list as well\n",
    "    return all_sequences_tokens, all_sequences_times, all_sequences_sourceids, all_sequences_extra_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ed1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_indices(proportions, bin_edges):\n",
    "    \"\"\"\n",
    "    Maps continuous proportions to discrete bin indices.\n",
    "    Handles the edge case for the maximum value (1.0).\n",
    "    \"\"\"\n",
    "    # Use np.digitize to find the bin index for each proportion\n",
    "    # digitize returns index i if bin_edges[i-1] <= x < bin_edges[i]\n",
    "    # For the last bin, we want to include the upper edge (1.0)\n",
    "    # np.digitize with right=False is default: bins[i-1] <= x < bins[i]\n",
    "    # To include the rightmost edge in the last bin, we can adjust values >= 1.0\n",
    "    proportions = np.clip(proportions, bin_edges[0], bin_edges[-1]) # Clip to [0, 1] range\n",
    "\n",
    "    # Use right=True to include the rightmost edge in the last bin\n",
    "    # bins[i-1] < x <= bins[i]\n",
    "    bin_indices = np.digitize(proportions, bin_edges, right=True) - 1 # -1 because bin_edges has N+1 edges for N bins\n",
    "\n",
    "    # Handle values exactly equal to the last edge (1.0) - np.digitize with right=True puts them in N+1 bin\n",
    "    # We want them in bin N-1 (0-indexed)\n",
    "    bin_indices[proportions == bin_edges[-1]] = len(bin_edges) - 2 # Index of the last bin (0-indexed)\n",
    "\n",
    "    # Ensure indices are within valid range [0, NUM_BINS - 1]\n",
    "    bin_indices = np.clip(bin_indices, 0, len(bin_edges) - 2)\n",
    "\n",
    "    return bin_indices\n",
    "\n",
    "def get_bin_centers(bin_indices, bin_edges):\n",
    "    \"\"\"\n",
    "    Returns the center value for a given array of bin indices.\n",
    "    \"\"\"\n",
    "    # Calculate bin centers as the midpoint of each bin\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    return bin_centers[bin_indices]\n",
    "\n",
    "\n",
    "def prepare_training_data(sequences_tokens, sequences_times, bin_edges):\n",
    "    \"\"\"\n",
    "    Prepares sequences for transformer training, including padding and masks.\n",
    "    Calculates target cumulative times, total times, and binned proportion targets.\n",
    "    \"\"\"\n",
    "    X_list, Y_list, masks_list, total_times_list, Y_binned_list = [], [], [], [], []\n",
    "\n",
    "    for tokens, times in zip(sequences_tokens, sequences_times):\n",
    "        # Ensure sequence has at least START and END tokens plus one event\n",
    "        if len(tokens) < 3:\n",
    "            # print(f\"Skipping short sequence with {len(tokens)} tokens.\") # Optional: uncomment for debugging\n",
    "            continue\n",
    "\n",
    "        # The last element in times should be the cumulative time of the last event\n",
    "        # which corresponds to the total time of the sequence.\n",
    "        total_time = times[-1]\n",
    "\n",
    "        # Input sequence X: START, Event1, Event2, ... EventN\n",
    "        # We predict the time *until* the event represented by the input token.\n",
    "        # So, the input sequence should be tokens[:-1] (START, Event1, ..., EventN-1)\n",
    "        x_seq = tokens[:-1]\n",
    "\n",
    "        # Target cumulative times Y: Time1, Time2, ... TimeN\n",
    "        # These are the cumulative times *at the end* of each step.\n",
    "        # These correspond to the time *at* the event represented by the token at the corresponding index in the input sequence.\n",
    "        # The first target time (times[1]) corresponds to the time of the first event (input token at index 1).\n",
    "        y_seq = times[1:]\n",
    "\n",
    "        # Calculate true time differences for proportion calculation\n",
    "        # time_diffs shape: (seq_len - 1) - corresponds to steps 1 to N\n",
    "        # These are the durations between events: duration[i] = time[i+1] - time[i]\n",
    "        time_diffs_unpadded = np.diff(times) # time_diffs[i] = times[i+1] - times[i]\n",
    "\n",
    "        # Calculate true proportions for the steps *after* the START token\n",
    "        # true_total is the last cumulative time\n",
    "        true_total = times[-1]\n",
    "        # Avoid division by zero\n",
    "        true_total_safe = true_total if true_total > 0 else 1.0\n",
    "        # true_props_unpadded shape: (seq_len - 1) - corresponds to steps 1 to N\n",
    "        # These are the proportions of the total time for each time difference.\n",
    "        true_props_unpadded = time_diffs_unpadded / true_total_safe\n",
    "\n",
    "        # Pad true_props to match input sequence length (X_list)\n",
    "        # The first position (corresponding to START token input at index 0) should have 0 proportion.\n",
    "        # The proportions for events 1 to N (indices 1 to N in input) are in true_props_unpadded.\n",
    "        true_props_padded = np.pad(true_props_unpadded, (1, 0), constant_values=0.0)\n",
    "\n",
    "        # Bin the true proportions\n",
    "        # Y_binned_seq shape: (seq_len) - corresponds to the input sequence length\n",
    "        y_binned_seq = get_bin_indices(true_props_padded, bin_edges)\n",
    "\n",
    "\n",
    "        # Mask: 1 for valid input tokens (not END_TOKEN), 0 otherwise\n",
    "        # The mask applies to the *input* sequence (X_list).\n",
    "        mask_seq = [1 if t != END_TOKEN else 0 for t in x_seq]\n",
    "\n",
    "        X_list.append(x_seq)\n",
    "        Y_list.append(y_seq) # Keep cumulative targets for CSV generation\n",
    "        masks_list.append(mask_seq)\n",
    "        total_times_list.append(total_time) # Keep total times for CSV generation\n",
    "        Y_binned_list.append(y_binned_seq) # Add binned targets\n",
    "\n",
    "\n",
    "    if not X_list:\n",
    "        print(\"No valid sequences found after preprocessing.\")\n",
    "        return np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "\n",
    "    # Determine max length based on the processed sequences\n",
    "    max_len = max(len(x) for x in X_list)\n",
    "    print(f\"Padding sequences to max length: {max_len}\")\n",
    "\n",
    "    # Pad sequences\n",
    "    # X_train: pad with END_TOKEN (mask_zero=True in embedding will ignore this)\n",
    "    X_train = pad_sequences(X_list, maxlen=max_len, padding='post', value=END_TOKEN)\n",
    "    # Y_cum_target: pad with 0.0\n",
    "    Y_cum_target = pad_sequences(Y_list, maxlen=max_len, padding='post', value=0.0)\n",
    "    # mask_train: pad with 0\n",
    "    mask_train = pad_sequences(masks_list, maxlen=max_len, padding='post', value=0)\n",
    "    # Y_binned_target: pad with a value that is within the valid bin range (e.g., 0)\n",
    "    # We will use the mask to ignore padded positions in the loss calculation\n",
    "    Y_binned_target = pad_sequences(Y_binned_list, maxlen=max_len, padding='post', value=0) # Changed padding value to 0\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.int32)\n",
    "    Y_cum_target = np.array(Y_cum_target, dtype=np.float32)\n",
    "    mask_train = np.array(mask_train, dtype=np.float32)\n",
    "    total_times = np.array(total_times_list, dtype=np.float32)\n",
    "    Y_binned_target = np.array(Y_binned_target, dtype=np.int32) # Binned targets are integers\n",
    "\n",
    "    print(f\"Prepared {X_train.shape[0]} sequences for training.\")\n",
    "    return X_train, Y_cum_target, mask_train, total_times, Y_binned_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0d10f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Transformer Components (unchanged)\n",
    "# ----------------------------\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_len=16384, use_embedding=True):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_embedding = use_embedding\n",
    "        if self.use_embedding:\n",
    "            # Set mask_zero to the actual padding value (END_TOKEN)\n",
    "            self.embedding = layers.Embedding(vocab_size, d_model, mask_zero=END_TOKEN)\n",
    "        else:\n",
    "            # If not using embedding, assume input is already dense (e.g., time features)\n",
    "            self.embedding = layers.Dense(d_model, activation=\"relu\")\n",
    "        self.max_len = max_len\n",
    "        # Ensure pos_encoding is created once and is large enough\n",
    "        self.pos_encoding = positional_encoding(self.max_len, d_model)\n",
    "\n",
    "    # Correct compute_mask signature to accept optional mask argument\n",
    "    def compute_mask(self, x, mask=None):\n",
    "         # If using embedding with mask_zero, the mask is computed based on mask_zero value\n",
    "         if self.use_embedding:\n",
    "              # Return a boolean mask indicating which elements are NOT the mask_zero value\n",
    "              return tf.math.not_equal(x, self.embedding.mask_zero)\n",
    "         # Otherwise, assume all steps are valid unless explicitly masked later\n",
    "         return None\n",
    "\n",
    "    def call(self, x):\n",
    "        # x is assumed to be token IDs if use_embedding is True, otherwise dense features\n",
    "        if self.use_embedding:\n",
    "            # The embedding layer itself computes and propagates the mask because mask_zero is set\n",
    "            x = self.embedding(x)\n",
    "        else:\n",
    "             # Apply dense layer if input is not token IDs\n",
    "             x = self.embedding(x)\n",
    "\n",
    "        # Scale the embedding output\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # Add positional encoding\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # Ensure positional encoding slice matches sequence length\n",
    "        x += self.pos_encoding[tf.newaxis, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model),\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        # Apply feed forward network with residual connection and layer normalization\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # MultiHeadAttention layer with causal mask\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        # Apply multi-head self-attention\n",
    "        # Keras automatically uses the mask attached to the input 'x'\n",
    "        attn_output = self.mha(query=x, key=x, value=x, use_causal_mask=True)\n",
    "        # Add residual connection and layer normalization\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttentionFeedForwardLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # Composes CausalSelfAttention and FeedForward layers\n",
    "        self.self_attention = CausalSelfAttention(num_heads=num_heads, d_model=d_model, dropout_rate=dropout_rate)\n",
    "        self.ffn = FeedForward(d_model, dff, dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Pass input through self-attention and then feed-forward network\n",
    "        # Mask from 'x' is propagated through these layers\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        # Positional embedding for the input tokens\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model, max_len=max_len)\n",
    "        # Stack of encoder layers\n",
    "        self.enc_layers = [SelfAttentionFeedForwardLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Apply positional embedding and dropout.\n",
    "        # The output 'x' from pos_embedding will carry the mask computed by PositionalEmbedding.compute_mask.\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through encoder layers. Keras will automatically propagate the mask\n",
    "        # through the layers that support masking (like MultiHeadAttention).\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x # The output tensor carries the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "513c2d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDiffTransformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Transformer model predicting proportions of total time for each sequence step.\n",
    "    This version predicts a probability distribution over bins for proportions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, num_bins, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        # Encoder processes the input sequence of tokens\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate, max_len)\n",
    "\n",
    "        # Head to predict the probability distribution over bins for proportions\n",
    "        # Output is NUM_BINS values per sequence step with softmax activation\n",
    "        self.proportion_head = layers.Dense(num_bins, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Pass input through the encoder\n",
    "        encoder_out = self.encoder(inputs) # encoder_out shape: (batch_size, seq_len, d_model)\n",
    "        # The mask from the embedding layer is propagated to encoder_out\n",
    "\n",
    "        # Predict probability distribution over bins for each step\n",
    "        # pred_bin_probs shape: (batch_size, seq_len, num_bins)\n",
    "        pred_bin_probs = self.proportion_head(encoder_out)\n",
    "\n",
    "        # Return the predicted bin probabilities\n",
    "        return pred_bin_probs # pred_bin_probs shape: (batch_size, seq_len, num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0b8175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the train_transformer function (cell [42]) with this one.\n",
    "\n",
    "def train_transformer(data_file, epochs=50, batch_size=32, num_bins=NUM_BINS, bin_edges=BIN_EDGES):\n",
    "    \"\"\"\n",
    "    Trains the TimeDiffTransformer model with proportion binning.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess data, now also gets sequences_extra_data\n",
    "        sequences_tokens, sequences_times, sequences_sourceids, sequences_extra_data = load_and_preprocess_data(data_file)\n",
    "\n",
    "        # Prepare data for training (this function does not need the extra data)\n",
    "        X_train, Y_cum_target, mask_train, total_times, Y_binned_target = prepare_training_data(\n",
    "            sequences_tokens, sequences_times, bin_edges\n",
    "        )\n",
    "\n",
    "        if X_train.shape[0] == 0:\n",
    "            print(\"No data available for training after preprocessing.\")\n",
    "            return None, None, None, None, None, None, None, None\n",
    "\n",
    "        # --- Model definition, optimizer, loss, and training loop remain the same ---\n",
    "        vocab_size = max(ENCODING_LEGEND.values()) + 1\n",
    "        max_seq_len = X_train.shape[1]\n",
    "        model = TimeDiffTransformer(\n",
    "            num_layers=3, d_model=64, num_heads=8, dff=128,\n",
    "            input_vocab_size=vocab_size, num_bins=num_bins,\n",
    "            dropout_rate=0.1, max_len=max_seq_len\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        proportion_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x, y_binned, mask):\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred_bin_probs = model(x)\n",
    "                mask_float = tf.cast(mask, tf.float32)\n",
    "                masked_props_loss = proportion_loss_fn(y_binned, pred_bin_probs, sample_weight=mask_float)\n",
    "                total_loss = masked_props_loss\n",
    "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            return total_loss, masked_props_loss\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_binned_target, mask_train)).batch(batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_epoch_loss, total_proportion_loss, num_batches = 0, 0, 0\n",
    "            for step, (batch_x, batch_y_binned, batch_mask) in enumerate(train_dataset):\n",
    "                loss, props_loss = train_step(batch_x, batch_y_binned, batch_mask)\n",
    "                total_epoch_loss += loss\n",
    "                total_proportion_loss += props_loss\n",
    "                num_batches += 1\n",
    "            avg_epoch_loss = total_epoch_loss / num_batches if num_batches > 0 else 0\n",
    "            avg_proportion_loss = total_proportion_loss / num_batches if num_batches > 0 else 0\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Total Loss: {avg_epoch_loss.numpy():.4f} - Proportion Loss: {avg_proportion_loss.numpy():.4f}\")\n",
    "        # --- End of unchanged training section ---\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "        # New: Return the extra data along with other results\n",
    "        return model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids, Y_binned_target, sequences_extra_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_transformer: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa84d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_differences(proportions, total_time, mask):\n",
    "    \"\"\"\n",
    "    Computes predicted increments and cumulative times from proportions and total time.\n",
    "    Applies masking to ignore padded steps.\n",
    "\n",
    "    Args:\n",
    "        proportions: Predicted proportions for each step (batch_size, seq_len).\n",
    "        total_time: The total time for each sequence (batch_size, 1).\n",
    "        mask: Mask indicating valid steps (batch_size, seq_len).\n",
    "\n",
    "    Returns:\n",
    "        proportions: Normalized proportions (batch_size, seq_len).\n",
    "        increments: Predicted time increments (batch_size, seq_len).\n",
    "        cumulative_times: Predicted cumulative times (batch_size, seq_len).\n",
    "    \"\"\"\n",
    "    # Apply mask to ensure only valid tokens contribute to calculations\n",
    "    proportions *= tf.cast(mask, tf.float32)\n",
    "\n",
    "    # Compute row-wise sum for normalization to handle variable-length sequences\n",
    "    # Sum across the sequence length dimension (axis=1)\n",
    "    row_sums = tf.reduce_sum(proportions, axis=1, keepdims=True)\n",
    "    # Prevent division by zero if a sequence is entirely masked (shouldn't happen with START token)\n",
    "    row_sums = tf.where(tf.equal(row_sums, 0), tf.ones_like(row_sums), row_sums)\n",
    "\n",
    "    # Normalize proportions so they sum to 1 over the valid (unmasked) steps\n",
    "    proportions /= row_sums\n",
    "\n",
    "    # Compute increments by multiplying normalized proportions by the total time\n",
    "    # total_time should have shape (batch_size, 1) for correct broadcasting\n",
    "    increments = proportions * total_time # Broadcasting total_time\n",
    "\n",
    "    # Compute cumulative times by summing increments along the sequence dimension\n",
    "    cumulative_times = tf.math.cumsum(increments, axis=1)\n",
    "\n",
    "    return proportions, increments, cumulative_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e14b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the generate_predictions_csv function (cell [43]) with this one.\n",
    "\n",
    "def generate_predictions_csv(model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids, sequences_extra_data, bin_edges):\n",
    "    \"\"\"\n",
    "    Generates predictions and saves to CSV, including additional original data columns.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model is None, cannot generate predictions.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"Generating predictions...\")\n",
    "\n",
    "    # --- Prediction and calculation logic remains the same ---\n",
    "    pred_bin_probs = model(X_train)\n",
    "    predicted_bin_indices = tf.argmax(pred_bin_probs, axis=-1, output_type=tf.int32)\n",
    "    predicted_proportions_continuous = get_bin_centers(predicted_bin_indices.numpy(), bin_edges)\n",
    "    total_times_tf = tf.constant(total_times, dtype=tf.float32)\n",
    "    total_times_expanded = tf.expand_dims(total_times_tf, axis=1)\n",
    "    proportions_pred_norm, increments_pred, cumulative_pred = compute_time_differences(\n",
    "        tf.constant(predicted_proportions_continuous, dtype=tf.float32),\n",
    "        total_times_expanded,\n",
    "        mask_train\n",
    "    )\n",
    "    proportions_pred_np = proportions_pred_norm.numpy()\n",
    "    increments_pred_np = increments_pred.numpy()\n",
    "    cumulative_pred_np = cumulative_pred.numpy()\n",
    "    X_train_np, Y_cum_target_np, mask_train_np = X_train, Y_cum_target, mask_train\n",
    "    gt_increments = np.zeros_like(Y_cum_target_np)\n",
    "    gt_increments[:, 0] = Y_cum_target_np[:, 0]\n",
    "    gt_increments[:, 1:] = Y_cum_target_np[:, 1:] - Y_cum_target_np[:, :-1]\n",
    "    gt_increments *= mask_train_np\n",
    "    # --- End of unchanged prediction logic ---\n",
    "\n",
    "    output_records = []\n",
    "    for seq_idx in range(X_train_np.shape[0]):\n",
    "        valid_mask = mask_train_np[seq_idx] == 1\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        safe_sourceids = sequences_sourceids[seq_idx] if seq_idx < len(sequences_sourceids) else []\n",
    "        safe_extra_data = sequences_extra_data[seq_idx] if seq_idx < len(sequences_extra_data) else [] # New\n",
    "        step_counter = 1\n",
    "\n",
    "        for i in range(len(valid_indices)):\n",
    "            valid_idx = valid_indices[i]\n",
    "            if valid_idx > 0:\n",
    "                source_id_index = valid_idx - 1\n",
    "                if source_id_index < len(safe_sourceids):\n",
    "                    source_id = safe_sourceids[source_id_index]\n",
    "                    \n",
    "                    # New: Get the extra data for this specific step\n",
    "                    extra_data_record = safe_extra_data[source_id_index] if source_id_index < len(safe_extra_data) else {}\n",
    "\n",
    "                    # Create the base record with predictions and ground truth\n",
    "                    record = {\n",
    "                        'Sequence': seq_idx,\n",
    "                        'Step': step_counter,\n",
    "                        'SourceID': source_id,\n",
    "                        'Predicted_Proportion': proportions_pred_np[seq_idx, valid_idx],\n",
    "                        'Predicted_Increment': increments_pred_np[seq_idx, valid_idx],\n",
    "                        'Predicted_Cumulative': cumulative_pred_np[seq_idx, valid_idx],\n",
    "                        'GroundTruth_Increment': gt_increments[seq_idx, valid_idx],\n",
    "                        'GroundTruth_Cumulative': Y_cum_target_np[seq_idx, valid_idx]\n",
    "                    }\n",
    "                    \n",
    "                    # New: Merge the extra data into the record\n",
    "                    record.update(extra_data_record)\n",
    "                    output_records.append(record)\n",
    "                    step_counter += 1\n",
    "\n",
    "    # New: Define final column order, including the kept columns\n",
    "    final_column_order = [\n",
    "        'Sequence', 'Step', 'SourceID', 'Predicted_Proportion',\n",
    "        'Predicted_Increment', 'Predicted_Cumulative',\n",
    "        'GroundTruth_Increment', 'GroundTruth_Cumulative'\n",
    "    ] + COLUMNS_TO_KEEP\n",
    "\n",
    "    if not output_records:\n",
    "        print(\"Warning: No valid prediction records generated.\")\n",
    "        predictions_df = pd.DataFrame(columns=final_column_order)\n",
    "    else:\n",
    "        predictions_df = pd.DataFrame(output_records)\n",
    "        # Reorder columns and ensure all are present\n",
    "        existing_cols = [col for col in final_column_order if col in predictions_df.columns]\n",
    "        predictions_df = predictions_df[existing_cols]\n",
    "\n",
    "    output_csv_path = 'predictions_transformer_182625_with_details.csv' # New output filename\n",
    "    try:\n",
    "        predictions_df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"Predictions saved successfully to {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions to CSV: {e}\")\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db1ca18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/182625/encoded_182625_condensed.csv...\n",
      "Loaded 186 sequences.\n",
      "Padding sequences to max length: 43\n",
      "Prepared 186 sequences for training.\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_3' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_3' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_3' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_3' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_4' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_4' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_4' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_4' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_5' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_5' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_5' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_5' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Total Loss: 4.5470 - Proportion Loss: 4.5470\n",
      "Epoch 2/50 - Total Loss: 3.8672 - Proportion Loss: 3.8672\n",
      "Epoch 3/50 - Total Loss: 3.5588 - Proportion Loss: 3.5588\n",
      "Epoch 4/50 - Total Loss: 3.3206 - Proportion Loss: 3.3206\n",
      "Epoch 5/50 - Total Loss: 3.1682 - Proportion Loss: 3.1682\n",
      "Epoch 6/50 - Total Loss: 3.0823 - Proportion Loss: 3.0823\n",
      "Epoch 7/50 - Total Loss: 3.0330 - Proportion Loss: 3.0330\n",
      "Epoch 8/50 - Total Loss: 3.0041 - Proportion Loss: 3.0041\n",
      "Epoch 9/50 - Total Loss: 2.9837 - Proportion Loss: 2.9837\n",
      "Epoch 10/50 - Total Loss: 2.9660 - Proportion Loss: 2.9660\n",
      "Epoch 11/50 - Total Loss: 2.9486 - Proportion Loss: 2.9486\n",
      "Epoch 12/50 - Total Loss: 2.9293 - Proportion Loss: 2.9293\n",
      "Epoch 13/50 - Total Loss: 2.9076 - Proportion Loss: 2.9076\n",
      "Epoch 14/50 - Total Loss: 2.8839 - Proportion Loss: 2.8839\n",
      "Epoch 15/50 - Total Loss: 2.8533 - Proportion Loss: 2.8533\n",
      "Epoch 16/50 - Total Loss: 2.8295 - Proportion Loss: 2.8295\n",
      "Epoch 17/50 - Total Loss: 2.8060 - Proportion Loss: 2.8060\n",
      "Epoch 18/50 - Total Loss: 2.7801 - Proportion Loss: 2.7801\n",
      "Epoch 19/50 - Total Loss: 2.7488 - Proportion Loss: 2.7488\n",
      "Epoch 20/50 - Total Loss: 2.7135 - Proportion Loss: 2.7135\n",
      "Epoch 21/50 - Total Loss: 2.7010 - Proportion Loss: 2.7010\n",
      "Epoch 22/50 - Total Loss: 2.6976 - Proportion Loss: 2.6976\n",
      "Epoch 23/50 - Total Loss: 2.6626 - Proportion Loss: 2.6626\n",
      "Epoch 24/50 - Total Loss: 2.6255 - Proportion Loss: 2.6255\n",
      "Epoch 25/50 - Total Loss: 2.5959 - Proportion Loss: 2.5959\n",
      "Epoch 26/50 - Total Loss: 2.5686 - Proportion Loss: 2.5686\n",
      "Epoch 27/50 - Total Loss: 2.5412 - Proportion Loss: 2.5412\n",
      "Epoch 28/50 - Total Loss: 2.5119 - Proportion Loss: 2.5119\n",
      "Epoch 29/50 - Total Loss: 2.4881 - Proportion Loss: 2.4881\n",
      "Epoch 30/50 - Total Loss: 2.4577 - Proportion Loss: 2.4577\n",
      "Epoch 31/50 - Total Loss: 2.4471 - Proportion Loss: 2.4471\n",
      "Epoch 32/50 - Total Loss: 2.4272 - Proportion Loss: 2.4272\n",
      "Epoch 33/50 - Total Loss: 2.3998 - Proportion Loss: 2.3998\n",
      "Epoch 34/50 - Total Loss: 2.3696 - Proportion Loss: 2.3696\n",
      "Epoch 35/50 - Total Loss: 2.3481 - Proportion Loss: 2.3481\n",
      "Epoch 36/50 - Total Loss: 2.3319 - Proportion Loss: 2.3319\n",
      "Epoch 37/50 - Total Loss: 2.3105 - Proportion Loss: 2.3105\n",
      "Epoch 38/50 - Total Loss: 2.2866 - Proportion Loss: 2.2866\n",
      "Epoch 39/50 - Total Loss: 2.2506 - Proportion Loss: 2.2506\n",
      "Epoch 40/50 - Total Loss: 2.2217 - Proportion Loss: 2.2217\n",
      "Epoch 41/50 - Total Loss: 2.1855 - Proportion Loss: 2.1855\n",
      "Epoch 42/50 - Total Loss: 2.1510 - Proportion Loss: 2.1510\n",
      "Epoch 43/50 - Total Loss: 2.1434 - Proportion Loss: 2.1434\n",
      "Epoch 44/50 - Total Loss: 2.1276 - Proportion Loss: 2.1276\n",
      "Epoch 45/50 - Total Loss: 2.1138 - Proportion Loss: 2.1138\n",
      "Epoch 46/50 - Total Loss: 2.1014 - Proportion Loss: 2.1014\n",
      "Epoch 47/50 - Total Loss: 2.0812 - Proportion Loss: 2.0812\n",
      "Epoch 48/50 - Total Loss: 2.1202 - Proportion Loss: 2.1202\n",
      "Epoch 49/50 - Total Loss: 2.0759 - Proportion Loss: 2.0759\n",
      "Epoch 50/50 - Total Loss: 2.0122 - Proportion Loss: 2.0122\n",
      "Training finished.\n",
      "Generating predictions...\n",
      "Predictions saved successfully to predictions_transformer_175651_with_details.csv\n",
      "\n",
      "Sample Predictions:\n",
      "   Sequence  Step     SourceID  Predicted_Cumulative  GroundTruth_Cumulative  \\\n",
      "0         0     1  MRI_MSR_104              3.430168                    40.0   \n",
      "1         0     2    MRI_FRR_2             29.156425                    45.0   \n",
      "2         0     3  MRI_FRR_257             34.301678                    52.0   \n",
      "3         0     4  MRI_FRR_264             53.167603                    68.0   \n",
      "4         0     5  MRI_FRR_264             92.614532                    77.0   \n",
      "5         0     6   MRI_CCS_11            111.480453                    83.0   \n",
      "6         0     7   MRI_CCS_11            116.625702                   213.0   \n",
      "7         0     8  MRI_FRR_257            128.631287                   214.0   \n",
      "8         0     9  MRI_FRR_264            130.346375                   224.0   \n",
      "9         0    10  MRI_FRR_264            166.363129                   226.0   \n",
      "\n",
      "   timediff     PTAB BodyGroup_from BodyGroup_to  \\\n",
      "0       0.0 -1182200       SHOULDER     SHOULDER   \n",
      "1      40.0 -1182200       SHOULDER     SHOULDER   \n",
      "2      45.0 -1181050       SHOULDER     SHOULDER   \n",
      "3      52.0 -1181050       SHOULDER     SHOULDER   \n",
      "4      68.0 -1181050       SHOULDER     SHOULDER   \n",
      "5      77.0 -1181050       SHOULDER     SHOULDER   \n",
      "6      83.0 -1181050       SHOULDER     SHOULDER   \n",
      "7     213.0      700       SHOULDER     SHOULDER   \n",
      "8     214.0      700       SHOULDER     SHOULDER   \n",
      "9     224.0      700       SHOULDER     SHOULDER   \n",
      "\n",
      "                             PatientID_from  \\\n",
      "0  222141929e178baebc35086b2e09016803a31622   \n",
      "1  222141929e178baebc35086b2e09016803a31622   \n",
      "2  222141929e178baebc35086b2e09016803a31622   \n",
      "3  222141929e178baebc35086b2e09016803a31622   \n",
      "4  222141929e178baebc35086b2e09016803a31622   \n",
      "5  222141929e178baebc35086b2e09016803a31622   \n",
      "6  222141929e178baebc35086b2e09016803a31622   \n",
      "7  222141929e178baebc35086b2e09016803a31622   \n",
      "8  222141929e178baebc35086b2e09016803a31622   \n",
      "9  222141929e178baebc35086b2e09016803a31622   \n",
      "\n",
      "                               PatientID_to  \n",
      "0  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "1  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "2  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "3  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "4  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "5  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "6  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "7  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "8  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n",
      "9  bab3a095d6a3db0905d1755af829c90031c7a3ce  \n"
     ]
    }
   ],
   "source": [
    "# Replace the main function call (cell [44]) with this one.\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the training and prediction process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_file = \"data/182625/encoded_182625_condensed.csv\"\n",
    "        if not os.path.exists(data_file):\n",
    "            print(f\"Error: Data file not found at {data_file}\")\n",
    "            return\n",
    "\n",
    "        # New: Unpack the extra data from the training result\n",
    "        result = train_transformer(data_file, epochs=50, num_bins=NUM_BINS, bin_edges=BIN_EDGES)\n",
    "\n",
    "        if result is None or result[0] is None:\n",
    "            print(\"Model training failed or no data was available. Exiting.\")\n",
    "            return\n",
    "\n",
    "        model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids, Y_binned_target, sequences_extra_data = result\n",
    "\n",
    "        # New: Pass the extra data to the prediction function\n",
    "        predictions_df = generate_predictions_csv(\n",
    "            model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids, sequences_extra_data, BIN_EDGES\n",
    "        )\n",
    "\n",
    "        if not predictions_df.empty:\n",
    "            print(\"\\nSample Predictions:\")\n",
    "            # Display a subset of columns for readability in the console\n",
    "            display_cols = ['Sequence', 'Step', 'SourceID', 'Predicted_Cumulative', 'GroundTruth_Cumulative'] + COLUMNS_TO_KEEP\n",
    "            print(predictions_df[display_cols].head(10))\n",
    "        else:\n",
    "            print(\"\\nNo predictions were generated.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
