{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Encoding\n",
    "START_TOKEN = 13\n",
    "END_TOKEN = 14\n",
    "ENCODING_LEGEND = {\n",
    "    'MRI_CCS_11': 1, 'MRI_EXU_95': 2, 'MRI_FRR_18': 3, 'MRI_FRR_257': 4,\n",
    "    'MRI_FRR_264': 5, 'MRI_FRR_3': 6, 'MRI_FRR_34': 7, 'MRI_MPT_1005': 8,\n",
    "    'MRI_MSR_100': 9, 'MRI_MSR_104': 10, 'MRI_MSR_21': 11, 'MRI_MSR_34': 12,\n",
    "    'START': START_TOKEN, 'END': END_TOKEN\n",
    "}\n",
    "reverse_encoding = {v: k for k, v in ENCODING_LEGEND.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_file):\n",
    "    data = pd.read_csv(data_file)\n",
    "    \n",
    "    # Split sequences based on \"SeqOrder\"\n",
    "    all_sequences_tokens = []\n",
    "    all_sequences_times = []\n",
    "    all_sequences_sourceids = []\n",
    "\n",
    "    current_tokens = []\n",
    "    current_times = []\n",
    "    current_sourceids = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        seq_order = row['SeqOrder']\n",
    "        s_id = row['sourceID']\n",
    "        t_diff = float(row['timediff'])\n",
    "        \n",
    "        if seq_order == 0 and current_tokens:\n",
    "            # Finalize previous sequence\n",
    "            token_seq = [START_TOKEN] + [int(ENCODING_LEGEND.get(str(x), x)) for x in current_tokens] + [END_TOKEN]\n",
    "            time_seq = [0.0] + current_times\n",
    "            \n",
    "            all_sequences_tokens.append(token_seq)\n",
    "            all_sequences_times.append(time_seq)\n",
    "            all_sequences_sourceids.append(current_sourceids)\n",
    "            \n",
    "            current_tokens = []\n",
    "            current_times = []\n",
    "            current_sourceids = []\n",
    "        \n",
    "        current_tokens.append(s_id)\n",
    "        current_times.append(t_diff)\n",
    "        current_sourceids.append(str(s_id))\n",
    "\n",
    "    # Add last sequence\n",
    "    if current_tokens:\n",
    "        token_seq = [START_TOKEN] + [int(ENCODING_LEGEND.get(str(x), x)) for x in current_tokens] + [END_TOKEN]\n",
    "        time_seq = [0.0] + current_times\n",
    "        \n",
    "        all_sequences_tokens.append(token_seq)\n",
    "        all_sequences_times.append(time_seq)\n",
    "        all_sequences_sourceids.append(current_sourceids)\n",
    "\n",
    "    return all_sequences_tokens, all_sequences_times, all_sequences_sourceids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(sequences_tokens, sequences_times):\n",
    "    X_list, Y_list, masks_list, total_times_list = [], [], [], []\n",
    "    \n",
    "    for tokens, times in zip(sequences_tokens, sequences_times):\n",
    "        total_time = times[-1]\n",
    "        \n",
    "        x_seq = tokens[:-1]    # input\n",
    "        y_seq = times[1:]      # target cumulative times\n",
    "        \n",
    "        # Mask: valid tokens are those not equal to the pad value\n",
    "        mask_seq = [1 if t != END_TOKEN else 0 for t in x_seq]\n",
    "        \n",
    "        X_list.append(x_seq)\n",
    "        Y_list.append(y_seq)\n",
    "        masks_list.append(mask_seq)\n",
    "        total_times_list.append(total_time)\n",
    "\n",
    "    max_len = max(len(x) for x in X_list)\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train = pad_sequences(X_list, maxlen=max_len, padding='post', value=END_TOKEN)\n",
    "    Y_cum_target = pad_sequences(Y_list, maxlen=max_len, padding='post', value=0.0)\n",
    "    mask_train = pad_sequences(masks_list, maxlen=max_len, padding='post', value=0)\n",
    "    \n",
    "    X_train = np.array(X_train, dtype=np.int32)\n",
    "    Y_cum_target = np.array(Y_cum_target, dtype=np.float32)\n",
    "    mask_train = np.array(mask_train, dtype=np.float32)\n",
    "    total_times = np.array(total_times_list, dtype=np.float32)\n",
    "    \n",
    "    return X_train, Y_cum_target, mask_train, total_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Transformer Components (unchanged)\n",
    "# ----------------------------\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_len=16384, use_embedding=True):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_embedding = use_embedding\n",
    "        if self.use_embedding:\n",
    "            self.embedding = layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        else:\n",
    "            self.embedding = layers.Dense(d_model, activation=\"relu\")\n",
    "        self.max_len = max_len\n",
    "        self.pos_encoding = positional_encoding(self.max_len, d_model)\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        if self.use_embedding:\n",
    "            return self.embedding.compute_mask(*args, **kwargs)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x += self.pos_encoding[tf.newaxis, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model),\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, key=x, value=x, use_causal_mask=True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttentionFeedForwardLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = CausalSelfAttention(num_heads=num_heads, d_model=d_model, dropout_rate=dropout_rate)\n",
    "        self.ffn = FeedForward(d_model, dff, dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n",
    "        self.enc_layers = [SelfAttentionFeedForwardLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [SelfAttentionFeedForwardLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "    \n",
    "    def call(self, x, context):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDiffTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate)\n",
    "        \n",
    "        # Modified heads to handle individual sequences\n",
    "        self.proportion_head = layers.Dense(1)  # Remove activation\n",
    "        self.total_time_head = layers.Dense(1, activation='relu')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        encoder_out = self.encoder(inputs)\n",
    "        \n",
    "        # Sequence-level attention for proportions\n",
    "        proportions = self.proportion_head(encoder_out)\n",
    "        proportions = tf.squeeze(proportions, axis=-1)\n",
    "        proportions = tf.nn.softmax(proportions, axis=1)  # Apply softmax explicitly\n",
    "        \n",
    "        # Total time prediction\n",
    "        seq_attention = tf.reduce_mean(encoder_out, axis=1)\n",
    "        total_time = self.total_time_head(seq_attention)\n",
    "        \n",
    "        return proportions, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_differences(proportions, total_time, mask):\n",
    "    # Ensure proportions and mask have compatible shapes\n",
    "    proportions = tf.reshape(proportions, tf.shape(mask))\n",
    "    \n",
    "    # Apply mask to ensure only valid tokens contribute\n",
    "    proportions *= tf.cast(mask, tf.float32)\n",
    "    \n",
    "    # Compute row-wise sum for normalization to handle variable-length sequences\n",
    "    row_sums = tf.reduce_sum(proportions, axis=1, keepdims=True)\n",
    "    row_sums = tf.where(row_sums == 0, tf.ones_like(row_sums), row_sums)\n",
    "    \n",
    "    # Normalize proportions\n",
    "    proportions /= row_sums\n",
    "    \n",
    "    # Compute increments (broadcasting total_time)\n",
    "    increments = proportions * tf.expand_dims(total_time, axis=1)\n",
    "    \n",
    "    # Compute cumulative times\n",
    "    cumulative_times = tf.math.cumsum(increments, axis=1)\n",
    "    \n",
    "    return proportions, increments, cumulative_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(data_file, epochs=50, batch_size=32):\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        sequences_tokens, sequences_times, sequences_sourceids = load_and_preprocess_data(data_file)\n",
    "        X_train, Y_cum_target, mask_train, total_times = prepare_training_data(sequences_tokens, sequences_times)\n",
    "        \n",
    "        # Model parameters\n",
    "        vocab_size = max(ENCODING_LEGEND.values()) + 1\n",
    "        max_seq_len = X_train.shape[1]\n",
    "        \n",
    "        model = TimeDiffTransformer(\n",
    "            num_layers=3, \n",
    "            d_model=64, \n",
    "            num_heads=8, \n",
    "            dff=128,\n",
    "            input_vocab_size=vocab_size, \n",
    "            dropout_rate=0.1, \n",
    "            max_len=max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Initial model call to build weights\n",
    "        _ = model(X_train)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step(x, y_cum, mask, total_time):\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred_props, pred_total = model(x)\n",
    "                \n",
    "                # Compute true proportions\n",
    "                time_diffs = y_cum[:, 1:] - y_cum[:, :-1]\n",
    "                true_total = total_time[:, tf.newaxis]\n",
    "                true_props = time_diffs / tf.where(true_total == 0, tf.ones_like(true_total), true_total)\n",
    "                \n",
    "                # Pad true_props to match prediction shape\n",
    "                true_props_padded = tf.pad(true_props, [[0, 0], [0, 1]], constant_values=0)\n",
    "                \n",
    "                # Compute masked losses\n",
    "                props_loss = tf.keras.losses.MeanSquaredError()(true_props_padded, pred_props)\n",
    "                total_time_loss = tf.keras.losses.MeanSquaredError()(total_time, pred_total)\n",
    "                \n",
    "                total_loss = props_loss + total_time_loss\n",
    "            \n",
    "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            return total_loss, props_loss, total_time_loss\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            loss, props_loss, total_loss = train_step(X_train, Y_cum_target, mask_train, total_times)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Total Loss: {loss.numpy():.4f}\")\n",
    "        \n",
    "        return model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_transformer: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_csv(model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids):\n",
    "    # Predict time differences\n",
    "    proportions_pred, increments_pred, cumulative_pred = compute_time_differences(\n",
    "        model(X_train)[0], total_times, mask_train\n",
    "    )\n",
    "    \n",
    "    # Convert TensorFlow tensors to NumPy arrays\n",
    "    proportions_pred = proportions_pred.numpy()\n",
    "    increments_pred = increments_pred.numpy()\n",
    "    cumulative_pred = cumulative_pred.numpy()\n",
    "    \n",
    "    # Ground truth computations\n",
    "    gt_increments = np.concatenate([Y_cum_target[:, 0:1], Y_cum_target[:, 1:] - Y_cum_target[:, :-1]], axis=1)\n",
    "    \n",
    "    # Collect predictions in a list of DataFrames\n",
    "    output_dataframes = []\n",
    "    \n",
    "    for seq_idx in range(X_train.shape[0]):\n",
    "        # Identify valid indices\n",
    "        valid_mask = mask_train[seq_idx] == 1\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "        \n",
    "        # Safety checks\n",
    "        if len(valid_indices) == 0:\n",
    "            print(f\"Warning: No valid indices for sequence {seq_idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure sourceids list is long enough\n",
    "        safe_sourceids = sequences_sourceids[seq_idx] if seq_idx < len(sequences_sourceids) else []\n",
    "        \n",
    "        # Prepare data with safe indexing\n",
    "        seq_df = pd.DataFrame({\n",
    "            'Sequence': [seq_idx] * len(valid_indices),\n",
    "            'Step': range(1, len(valid_indices) + 1),\n",
    "            'SourceID': [safe_sourceids[i] if i < len(safe_sourceids) else f'Unknown_{i}' for i in range(len(valid_indices))],\n",
    "            'Predicted_Proportion': proportions_pred[seq_idx][valid_indices],\n",
    "            'Predicted_Increment': increments_pred[seq_idx][valid_indices],\n",
    "            'Predicted_Cumulative': cumulative_pred[seq_idx][valid_indices],\n",
    "            'GroundTruth_Increment': gt_increments[seq_idx][valid_indices],\n",
    "            'GroundTruth_Cumulative': Y_cum_target[seq_idx][valid_indices]\n",
    "        })\n",
    "        \n",
    "        output_dataframes.append(seq_df)\n",
    "    \n",
    "    # Combine and save\n",
    "    if not output_dataframes:\n",
    "        print(\"No valid sequences found for predictions.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    predictions_df = pd.concat(output_dataframes, ignore_index=True)\n",
    "    predictions_df.to_csv('predictions_transformer.csv', index=False)\n",
    "    print(\"Predictions saved to predictions_transformer.csv\")\n",
    "    \n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_15' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_15' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_15' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_15' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_16' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_16' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_16' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_16' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_17' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_17' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_17' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_17' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Total Loss: 261922.9531\n",
      "Epoch 2/50 - Total Loss: 261922.9531\n",
      "Epoch 3/50 - Total Loss: 261087.2344\n",
      "Epoch 4/50 - Total Loss: 257413.1562\n",
      "Epoch 5/50 - Total Loss: 255475.0469\n",
      "Epoch 6/50 - Total Loss: 254331.1875\n",
      "Epoch 7/50 - Total Loss: 253649.4219\n",
      "Epoch 8/50 - Total Loss: 253306.2969\n",
      "Epoch 9/50 - Total Loss: 253087.5000\n",
      "Epoch 10/50 - Total Loss: 252934.1562\n",
      "Epoch 11/50 - Total Loss: 252820.9062\n",
      "Epoch 12/50 - Total Loss: 252732.1562\n",
      "Epoch 13/50 - Total Loss: 252661.0625\n",
      "Epoch 14/50 - Total Loss: 252597.5625\n",
      "Epoch 15/50 - Total Loss: 252536.5938\n",
      "Epoch 16/50 - Total Loss: 252473.8750\n",
      "Epoch 17/50 - Total Loss: 252406.7188\n",
      "Epoch 18/50 - Total Loss: 252335.5938\n",
      "Epoch 19/50 - Total Loss: 252263.4219\n",
      "Epoch 20/50 - Total Loss: 252192.3594\n",
      "Epoch 21/50 - Total Loss: 252123.3438\n",
      "Epoch 22/50 - Total Loss: 252056.3125\n",
      "Epoch 23/50 - Total Loss: 251990.6094\n",
      "Epoch 24/50 - Total Loss: 251925.6875\n",
      "Epoch 25/50 - Total Loss: 251861.0469\n",
      "Epoch 26/50 - Total Loss: 251796.3906\n",
      "Epoch 27/50 - Total Loss: 251731.5156\n",
      "Epoch 28/50 - Total Loss: 251666.3594\n",
      "Epoch 29/50 - Total Loss: 251600.8750\n",
      "Epoch 30/50 - Total Loss: 251535.0156\n",
      "Epoch 31/50 - Total Loss: 251468.6094\n",
      "Epoch 32/50 - Total Loss: 251401.5938\n",
      "Epoch 33/50 - Total Loss: 251334.2500\n",
      "Epoch 34/50 - Total Loss: 251266.5625\n",
      "Epoch 35/50 - Total Loss: 251198.7031\n",
      "Epoch 36/50 - Total Loss: 251130.5938\n",
      "Epoch 37/50 - Total Loss: 251062.3281\n",
      "Epoch 38/50 - Total Loss: 250993.9844\n",
      "Epoch 39/50 - Total Loss: 250925.5625\n",
      "Epoch 40/50 - Total Loss: 250856.8750\n",
      "Epoch 41/50 - Total Loss: 250787.9062\n",
      "Epoch 42/50 - Total Loss: 250718.5469\n",
      "Epoch 43/50 - Total Loss: 250648.8281\n",
      "Epoch 44/50 - Total Loss: 250578.8281\n",
      "Epoch 45/50 - Total Loss: 250508.7031\n",
      "Epoch 46/50 - Total Loss: 250438.4375\n",
      "Epoch 47/50 - Total Loss: 250367.9375\n",
      "Epoch 48/50 - Total Loss: 250297.2188\n",
      "Epoch 49/50 - Total Loss: 250226.0781\n",
      "Epoch 50/50 - Total Loss: 250154.5625\n",
      "Predictions saved to predictions_transformer.csv\n",
      "\n",
      "Sample Predictions:\n",
      "   Sequence  Step SourceID  Predicted_Proportion  Predicted_Increment   \n",
      "0         0     1     10.0              0.049973            15.341682  \\\n",
      "1         0     2      0.0              0.050101            15.380990   \n",
      "2         0     3      4.0              0.051360            15.767509   \n",
      "3         0     4      5.0              0.049961            15.337996   \n",
      "4         0     5      5.0              0.049819            15.294573   \n",
      "5         0     6      1.0              0.049873            15.310900   \n",
      "6         0     7      1.0              0.049161            15.092329   \n",
      "7         0     8      4.0              0.049285            15.130347   \n",
      "8         0     9      5.0              0.050465            15.492865   \n",
      "9         0    10      5.0              0.050275            15.434329   \n",
      "\n",
      "   Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \n",
      "0             15.341682                    0.0                     0.0  \n",
      "1             30.722672                   40.0                    40.0  \n",
      "2             46.490181                    5.0                    45.0  \n",
      "3             61.828178                    7.0                    52.0  \n",
      "4             77.122749                   16.0                    68.0  \n",
      "5             92.433647                    9.0                    77.0  \n",
      "6            107.525978                    6.0                    83.0  \n",
      "7            122.656326                  130.0                   213.0  \n",
      "8            138.149185                    1.0                   214.0  \n",
      "9            153.583511                   10.0                   224.0  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        data_file = \"encoded_182625.csv\"  # Replace with your actual file path\n",
    "        \n",
    "        # Train model and get predictions\n",
    "        result = train_transformer(data_file)\n",
    "        if result is None:\n",
    "            print(\"Training failed, no results returned.\")\n",
    "            return\n",
    "        \n",
    "        model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids = result\n",
    "        \n",
    "        predictions_df = generate_predictions_csv(\n",
    "            model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSample Predictions:\")\n",
    "        print(predictions_df.head(10))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
