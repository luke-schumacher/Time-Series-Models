{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Encoding\n",
    "START_TOKEN = 14\n",
    "END_TOKEN = 15\n",
    "ENCODING_LEGEND = {\n",
    "    'MRI_CCS_11': 1, 'MRI_EXU_95': 2, 'MRI_FRR_18': 3, 'MRI_FRR_257': 4,\n",
    "    'MRI_FRR_264': 5, 'MRI_FRR_2': 6, 'MRI_FRR_3': 7, 'MRI_FRR_34': 8, 'MRI_MPT_1005': 9,\n",
    "    'MRI_MSR_100': 10, 'MRI_MSR_104': 11, 'MRI_MSR_21': 12, 'MRI_MSR_34': 13,\n",
    "    'START': START_TOKEN, 'END': END_TOKEN\n",
    "}\n",
    "reverse_encoding = {v: k for k, v in ENCODING_LEGEND.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_file):\n",
    "    data = pd.read_csv(data_file)\n",
    "    \n",
    "    # Split sequences based on \"SeqOrder\"\n",
    "    all_sequences_tokens = []\n",
    "    all_sequences_times = []\n",
    "    all_sequences_sourceids = []\n",
    "\n",
    "    current_tokens = []\n",
    "    current_times = []\n",
    "    current_sourceids = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        seq_order = row['SeqOrder']\n",
    "        s_id = row['sourceID']\n",
    "        t_diff = float(row['timediff'])\n",
    "        \n",
    "        if seq_order == 0 and current_tokens:\n",
    "            # Finalize previous sequence\n",
    "            token_seq = [START_TOKEN] + [int(ENCODING_LEGEND.get(str(x), x)) for x in current_tokens] + [END_TOKEN]\n",
    "            time_seq = [0.0] + current_times\n",
    "            \n",
    "            all_sequences_tokens.append(token_seq)\n",
    "            all_sequences_times.append(time_seq)\n",
    "            all_sequences_sourceids.append(current_sourceids)\n",
    "            \n",
    "            current_tokens = []\n",
    "            current_times = []\n",
    "            current_sourceids = []\n",
    "        \n",
    "        current_tokens.append(s_id)\n",
    "        current_times.append(t_diff)\n",
    "        current_sourceids.append(str(s_id))\n",
    "\n",
    "    # Add last sequence\n",
    "    if current_tokens:\n",
    "        token_seq = [START_TOKEN] + [int(ENCODING_LEGEND.get(str(x), x)) for x in current_tokens] + [END_TOKEN]\n",
    "        time_seq = [0.0] + current_times\n",
    "        \n",
    "        all_sequences_tokens.append(token_seq)\n",
    "        all_sequences_times.append(time_seq)\n",
    "        all_sequences_sourceids.append(current_sourceids)\n",
    "\n",
    "    return all_sequences_tokens, all_sequences_times, all_sequences_sourceids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(sequences_tokens, sequences_times):\n",
    "    X_list, Y_list, masks_list, total_times_list = [], [], [], []\n",
    "    \n",
    "    for tokens, times in zip(sequences_tokens, sequences_times):\n",
    "        total_time = times[-1]\n",
    "        \n",
    "        x_seq = tokens[:-1]    # input\n",
    "        y_seq = times[1:]      # target cumulative times\n",
    "        \n",
    "        # Mask: valid tokens are those not equal to the pad value\n",
    "        mask_seq = [1 if t != END_TOKEN else 0 for t in x_seq]\n",
    "        \n",
    "        X_list.append(x_seq)\n",
    "        Y_list.append(y_seq)\n",
    "        masks_list.append(mask_seq)\n",
    "        total_times_list.append(total_time)\n",
    "\n",
    "    max_len = max(len(x) for x in X_list)\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train = pad_sequences(X_list, maxlen=max_len, padding='post', value=END_TOKEN)\n",
    "    Y_cum_target = pad_sequences(Y_list, maxlen=max_len, padding='post', value=0.0)\n",
    "    mask_train = pad_sequences(masks_list, maxlen=max_len, padding='post', value=0)\n",
    "    \n",
    "    X_train = np.array(X_train, dtype=np.int32)\n",
    "    Y_cum_target = np.array(Y_cum_target, dtype=np.float32)\n",
    "    mask_train = np.array(mask_train, dtype=np.float32)\n",
    "    total_times = np.array(total_times_list, dtype=np.float32)\n",
    "    \n",
    "    return X_train, Y_cum_target, mask_train, total_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Transformer Components (unchanged)\n",
    "# ----------------------------\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "    angle_rates = 1 / (10000 ** depths)\n",
    "    angle_rads = positions * angle_rates\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model, max_len=16384, use_embedding=True):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_embedding = use_embedding\n",
    "        if self.use_embedding:\n",
    "            self.embedding = layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        else:\n",
    "            self.embedding = layers.Dense(d_model, activation=\"relu\")\n",
    "        self.max_len = max_len\n",
    "        self.pos_encoding = positional_encoding(self.max_len, d_model)\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        if self.use_embedding:\n",
    "            return self.embedding.compute_mask(*args, **kwargs)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x += self.pos_encoding[tf.newaxis, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model),\n",
    "            layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class CausalSelfAttention(layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "        self.add = layers.Add()\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x, key=x, value=x, use_causal_mask=True)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttentionFeedForwardLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = CausalSelfAttention(num_heads=num_heads, d_model=d_model, dropout_rate=dropout_rate)\n",
    "        self.ffn = FeedForward(d_model, dff, dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n",
    "        self.enc_layers = [SelfAttentionFeedForwardLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.dec_layers = [SelfAttentionFeedForwardLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                           for _ in range(num_layers)]\n",
    "    \n",
    "    def call(self, x, context):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDiffTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate=0.1, max_len=16384):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate)\n",
    "        \n",
    "        # Modified heads to handle individual sequences\n",
    "        self.proportion_head = layers.Dense(1)  # Remove activation\n",
    "        self.total_time_head = layers.Dense(1, activation='relu')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        encoder_out = self.encoder(inputs)\n",
    "        \n",
    "        # Sequence-level attention for proportions\n",
    "        proportions = self.proportion_head(encoder_out)\n",
    "        proportions = tf.squeeze(proportions, axis=-1)\n",
    "        proportions = tf.nn.softmax(proportions, axis=1)  # Apply softmax explicitly\n",
    "        \n",
    "        # Total time prediction\n",
    "        seq_attention = tf.reduce_mean(encoder_out, axis=1)\n",
    "        total_time = self.total_time_head(seq_attention)\n",
    "        \n",
    "        return proportions, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_differences(proportions, total_time, mask):\n",
    "    # Ensure proportions and mask have compatible shapes\n",
    "    proportions = tf.reshape(proportions, tf.shape(mask))\n",
    "    \n",
    "    # Apply mask to ensure only valid tokens contribute\n",
    "    proportions *= tf.cast(mask, tf.float32)\n",
    "    \n",
    "    # Compute row-wise sum for normalization to handle variable-length sequences\n",
    "    row_sums = tf.reduce_sum(proportions, axis=1, keepdims=True)\n",
    "    row_sums = tf.where(row_sums == 0, tf.ones_like(row_sums), row_sums)\n",
    "    \n",
    "    # Normalize proportions\n",
    "    proportions /= row_sums\n",
    "    \n",
    "    # Compute increments (broadcasting total_time)\n",
    "    increments = proportions * tf.expand_dims(total_time, axis=1)\n",
    "    \n",
    "    # Compute cumulative times\n",
    "    cumulative_times = tf.math.cumsum(increments, axis=1)\n",
    "    \n",
    "    return proportions, increments, cumulative_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(data_file, epochs=50, batch_size=32):\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        sequences_tokens, sequences_times, sequences_sourceids = load_and_preprocess_data(data_file)\n",
    "        X_train, Y_cum_target, mask_train, total_times = prepare_training_data(sequences_tokens, sequences_times)\n",
    "        \n",
    "        # Model parameters\n",
    "        vocab_size = max(ENCODING_LEGEND.values()) + 1\n",
    "        max_seq_len = X_train.shape[1]\n",
    "        \n",
    "        model = TimeDiffTransformer(\n",
    "            num_layers=3, \n",
    "            d_model=64, \n",
    "            num_heads=8, \n",
    "            dff=128,\n",
    "            input_vocab_size=vocab_size, \n",
    "            dropout_rate=0.1, \n",
    "            max_len=max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Initial model call to build weights\n",
    "        _ = model(X_train)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step(x, y_cum, mask, total_time):\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred_props, pred_total = model(x)\n",
    "                \n",
    "                # Compute true proportions\n",
    "                time_diffs = y_cum[:, 1:] - y_cum[:, :-1]\n",
    "                true_total = total_time[:, tf.newaxis]\n",
    "                true_props = time_diffs / tf.where(true_total == 0, tf.ones_like(true_total), true_total)\n",
    "                \n",
    "                # Pad true_props to match prediction shape\n",
    "                true_props_padded = tf.pad(true_props, [[0, 0], [0, 1]], constant_values=0)\n",
    "                \n",
    "                # Compute masked losses\n",
    "                props_loss = tf.keras.losses.MeanSquaredError()(true_props_padded, pred_props)\n",
    "                total_time_loss = tf.keras.losses.MeanSquaredError()(total_time, pred_total)\n",
    "                \n",
    "                total_loss = props_loss + total_time_loss\n",
    "            \n",
    "            grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            return total_loss, props_loss, total_time_loss\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            loss, props_loss, total_loss = train_step(X_train, Y_cum_target, mask_train, total_times)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Total Loss: {loss.numpy():.4f}\")\n",
    "        \n",
    "        return model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_transformer: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_csv(model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids):\n",
    "    \"\"\"\n",
    "    Generates predictions using the trained model and saves them to a CSV file,\n",
    "    correctly aligning SourceIDs with sequence steps.\n",
    "\n",
    "    Args:\n",
    "        model: The trained TimeDiffTransformer model.\n",
    "        X_train: The input sequences (padded).\n",
    "        Y_cum_target: The target cumulative times (padded).\n",
    "        mask_train: The mask indicating valid sequence positions.\n",
    "        total_times: The true total time for each sequence.\n",
    "        sequences_sourceids: A list of lists, where each inner list contains\n",
    "                             the original source IDs for a sequence.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame containing predictions and ground truth.\n",
    "    \"\"\"\n",
    "    print(\"Generating predictions...\")\n",
    "    # Predict proportions and total time using the model\n",
    "    # Assuming model(X_train) returns (proportions, total_time_pred)\n",
    "    # We need the predicted proportions and the *true* total_times for computing increments\n",
    "    proportions_pred, _ = model(X_train) # We use true total_times below\n",
    "\n",
    "    # Compute predicted increments and cumulative times based on *true* total_times\n",
    "    # This aligns with how the loss might be calculated if focusing on proportions\n",
    "    proportions_pred_norm, increments_pred, cumulative_pred = compute_time_differences(\n",
    "        proportions_pred, tf.squeeze(total_times), mask_train # Use true total_times here\n",
    "    )\n",
    "\n",
    "    # Convert TensorFlow tensors to NumPy arrays for easier handling\n",
    "    proportions_pred_np = proportions_pred_norm.numpy()\n",
    "    increments_pred_np = increments_pred.numpy()\n",
    "    cumulative_pred_np = cumulative_pred.numpy()\n",
    "    X_train_np = X_train # Already numpy or can be converted if needed\n",
    "    Y_cum_target_np = Y_cum_target # Already numpy or can be converted\n",
    "    mask_train_np = mask_train # Already numpy or can be converted\n",
    "\n",
    "    # Compute ground truth increments for comparison\n",
    "    # Handle the first element carefully (it's the time of the first event relative to start)\n",
    "    gt_increments = np.zeros_like(Y_cum_target_np)\n",
    "    gt_increments[:, 0] = Y_cum_target_np[:, 0] # First increment is the first cumulative time\n",
    "    gt_increments[:, 1:] = Y_cum_target_np[:, 1:] - Y_cum_target_np[:, :-1]\n",
    "    # Apply mask to ground truth increments as well\n",
    "    gt_increments *= mask_train_np\n",
    "\n",
    "\n",
    "    # Collect predictions in a list of dictionaries for easy DataFrame creation\n",
    "    output_records = []\n",
    "\n",
    "    # Iterate through each sequence in the batch\n",
    "    for seq_idx in range(X_train_np.shape[0]):\n",
    "        # Find indices that are not padding AND not the END_TOKEN used for padding\n",
    "        # The mask already handles padding, but let's be explicit if needed.\n",
    "        # mask_train should be 1 for START, T1, T2, ... and 0 for END and PAD\n",
    "        valid_mask = mask_train_np[seq_idx] == 1\n",
    "        valid_indices = np.where(valid_mask)[0]\n",
    "\n",
    "        # Get the original source IDs for this sequence\n",
    "        safe_sourceids = sequences_sourceids[seq_idx] if seq_idx < len(sequences_sourceids) else []\n",
    "\n",
    "        step_counter = 1 # Initialize step counter for this sequence\n",
    "\n",
    "        # Iterate through the valid indices within this sequence\n",
    "        for i in range(len(valid_indices)):\n",
    "            valid_idx = valid_indices[i] # The actual index in the padded sequence\n",
    "\n",
    "            # Skip the START_TOKEN position (index 0 in the original sequence part)\n",
    "            # as it doesn't have a corresponding SourceID from the input data.\n",
    "            # Predictions/increments at this position might relate to the first step's timing.\n",
    "            if valid_idx == 0:\n",
    "                continue # Skip processing this row for the final CSV\n",
    "\n",
    "            # Calculate the index for the source ID list.\n",
    "            # Since we skipped valid_idx=0, the first real event corresponds to\n",
    "            # valid_idx=1, which should map to source ID at index 0.\n",
    "            source_id_index = valid_idx - 1\n",
    "\n",
    "            # Get the corresponding source ID safely\n",
    "            if source_id_index < len(safe_sourceids):\n",
    "                source_id = safe_sourceids[source_id_index]\n",
    "            else:\n",
    "                # This case indicates a potential mismatch or issue elsewhere\n",
    "                # if it occurs frequently.\n",
    "                source_id = f'Unknown_Mapping_Error_idx_{source_id_index}'\n",
    "                print(f\"Warning: Source ID index {source_id_index} out of bounds for sequence {seq_idx} with length {len(safe_sourceids)}\")\n",
    "\n",
    "\n",
    "            # Append record for this step\n",
    "            output_records.append({\n",
    "                'Sequence': seq_idx,\n",
    "                'Step': step_counter, # Use the dedicated counter\n",
    "                'SourceID': source_id,\n",
    "                'Predicted_Proportion': proportions_pred_np[seq_idx, valid_idx],\n",
    "                'Predicted_Increment': increments_pred_np[seq_idx, valid_idx],\n",
    "                'Predicted_Cumulative': cumulative_pred_np[seq_idx, valid_idx],\n",
    "                'GroundTruth_Increment': gt_increments[seq_idx, valid_idx],\n",
    "                'GroundTruth_Cumulative': Y_cum_target_np[seq_idx, valid_idx]\n",
    "            })\n",
    "\n",
    "            step_counter += 1 # Increment step counter only for actual events added\n",
    "\n",
    "    # Create DataFrame from the collected records\n",
    "    if not output_records:\n",
    "        print(\"Warning: No valid prediction records generated.\")\n",
    "        predictions_df = pd.DataFrame(columns=[\n",
    "            'Sequence', 'Step', 'SourceID', 'Predicted_Proportion',\n",
    "            'Predicted_Increment', 'Predicted_Cumulative',\n",
    "            'GroundTruth_Increment', 'GroundTruth_Cumulative'\n",
    "        ])\n",
    "    else:\n",
    "        predictions_df = pd.DataFrame(output_records)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    try:\n",
    "        predictions_df.to_csv('predictions_transformer_175974.csv', index=False)\n",
    "        print(\"Predictions saved successfully to predictions_transformer.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions to CSV: {e}\")\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_6' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_6' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_6' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_6' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_7' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_7' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_7' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_7' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'causal_self_attention_8' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'sequential_8' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'feed_forward_8' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lukis\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\layer.py:932: UserWarning: Layer 'self_attention_feed_forward_layer_8' (of type SelfAttentionFeedForwardLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Total Loss: 1371750.7500\n",
      "Epoch 2/50 - Total Loss: 1363133.6250\n",
      "Epoch 3/50 - Total Loss: 1359328.3750\n",
      "Epoch 4/50 - Total Loss: 1357353.6250\n",
      "Epoch 5/50 - Total Loss: 1356303.1250\n",
      "Epoch 6/50 - Total Loss: 1355811.7500\n",
      "Epoch 7/50 - Total Loss: 1355474.8750\n",
      "Epoch 8/50 - Total Loss: 1355177.7500\n",
      "Epoch 9/50 - Total Loss: 1354935.1250\n",
      "Epoch 10/50 - Total Loss: 1354763.7500\n",
      "Epoch 11/50 - Total Loss: 1354633.1250\n",
      "Epoch 12/50 - Total Loss: 1354513.8750\n",
      "Epoch 13/50 - Total Loss: 1354385.8750\n",
      "Epoch 14/50 - Total Loss: 1354252.8750\n",
      "Epoch 15/50 - Total Loss: 1354122.0000\n",
      "Epoch 16/50 - Total Loss: 1353991.8750\n",
      "Epoch 17/50 - Total Loss: 1353860.1250\n",
      "Epoch 18/50 - Total Loss: 1353725.6250\n",
      "Epoch 19/50 - Total Loss: 1353589.1250\n",
      "Epoch 20/50 - Total Loss: 1353453.1250\n",
      "Epoch 21/50 - Total Loss: 1353317.8750\n",
      "Epoch 22/50 - Total Loss: 1353182.7500\n",
      "Epoch 23/50 - Total Loss: 1353047.2500\n",
      "Epoch 24/50 - Total Loss: 1352911.5000\n",
      "Epoch 25/50 - Total Loss: 1352776.3750\n",
      "Epoch 26/50 - Total Loss: 1352642.0000\n",
      "Epoch 27/50 - Total Loss: 1352508.7500\n",
      "Epoch 28/50 - Total Loss: 1352376.0000\n",
      "Epoch 29/50 - Total Loss: 1352243.8750\n",
      "Epoch 30/50 - Total Loss: 1352111.8750\n",
      "Epoch 31/50 - Total Loss: 1351978.6250\n",
      "Epoch 32/50 - Total Loss: 1351843.8750\n",
      "Epoch 33/50 - Total Loss: 1351707.6250\n",
      "Epoch 34/50 - Total Loss: 1351570.0000\n",
      "Epoch 35/50 - Total Loss: 1351431.5000\n",
      "Epoch 36/50 - Total Loss: 1351292.5000\n",
      "Epoch 37/50 - Total Loss: 1351153.2500\n",
      "Epoch 38/50 - Total Loss: 1351013.1250\n",
      "Epoch 39/50 - Total Loss: 1350872.3750\n",
      "Epoch 40/50 - Total Loss: 1350730.6250\n",
      "Epoch 41/50 - Total Loss: 1350588.5000\n",
      "Epoch 42/50 - Total Loss: 1350446.0000\n",
      "Epoch 43/50 - Total Loss: 1350302.7500\n",
      "Epoch 44/50 - Total Loss: 1350158.8750\n",
      "Epoch 45/50 - Total Loss: 1350014.5000\n",
      "Epoch 46/50 - Total Loss: 1349869.3750\n",
      "Epoch 47/50 - Total Loss: 1349723.6250\n",
      "Epoch 48/50 - Total Loss: 1349577.0000\n",
      "Epoch 49/50 - Total Loss: 1349429.5000\n",
      "Epoch 50/50 - Total Loss: 1349281.2500\n",
      "Generating predictions...\n",
      "Predictions saved successfully to predictions_transformer.csv\n",
      "\n",
      "Sample Predictions:\n",
      "   Sequence  Step SourceID  Predicted_Proportion  Predicted_Increment  \\\n",
      "0         0     1     11.0              0.042931            11.548487   \n",
      "1         0     2      4.0              0.043498            11.700955   \n",
      "2         0     3     13.0              0.043867            11.800332   \n",
      "3         0     4      5.0              0.044410            11.946395   \n",
      "4         0     5      9.0              0.044190            11.887192   \n",
      "5         0     6      1.0              0.044028            11.843553   \n",
      "6         0     7      5.0              0.043857            11.797453   \n",
      "7         0     8      4.0              0.043377            11.668401   \n",
      "8         0     9      5.0              0.043838            11.792318   \n",
      "9         0    10      5.0              0.044006            11.837735   \n",
      "\n",
      "   Predicted_Cumulative  GroundTruth_Increment  GroundTruth_Cumulative  \n",
      "0             23.160145                   14.0                    14.0  \n",
      "1             34.861099                    1.0                    15.0  \n",
      "2             46.661430                    9.0                    24.0  \n",
      "3             58.607826                    9.0                    33.0  \n",
      "4             70.495018                   24.0                    57.0  \n",
      "5             82.338570                    6.0                    63.0  \n",
      "6             94.136024                  116.0                   179.0  \n",
      "7            105.804428                    2.0                   181.0  \n",
      "8            117.596748                   13.0                   194.0  \n",
      "9            129.434479                   16.0                   210.0  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        data_file = \"encoded_175974_condensed.csv\"  # Replace with your actual file path\n",
    "        \n",
    "        # Train model and get predictions\n",
    "        result = train_transformer(data_file)\n",
    "        if result is None:\n",
    "            print(\"Training failed, no results returned.\")\n",
    "            return\n",
    "        \n",
    "        model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids = result\n",
    "        \n",
    "        predictions_df = generate_predictions_csv(\n",
    "            model, X_train, Y_cum_target, mask_train, total_times, sequences_sourceids\n",
    "        )\n",
    "        \n",
    "        print(\"\\nSample Predictions:\")\n",
    "        print(predictions_df.head(10))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
